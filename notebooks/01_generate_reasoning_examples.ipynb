{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d17ceba-ea86-418b-8cbb-ce4e60925cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: together in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (1.3.14)\n",
      "Requirement already satisfied: litellm in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (1.43.9)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.3 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (3.10.3)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.7 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (8.1.7)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.1.3 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (0.2.2)\n",
      "Requirement already satisfied: filelock<4.0.0,>=3.13.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (1.26.4)\n",
      "Requirement already satisfied: pillow<11.0.0,>=10.3.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (10.4.0)\n",
      "Requirement already satisfied: pyarrow>=10.0.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (17.0.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.6.3 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (2.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (2.32.3)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.8.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (13.9.4)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.2 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (4.66.5)\n",
      "Requirement already satisfied: typer<0.16,>=0.9 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (0.12.3)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (7.2.1)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (3.1.4)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (4.23.0)\n",
      "Requirement already satisfied: openai>=1.40.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (1.40.6)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (1.0.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (0.7.0)\n",
      "Requirement already satisfied: tokenizers in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (0.20.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.9.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from importlib-metadata>=6.8.0->litellm) (3.20.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from jinja2<4.0.0,>=3.1.2->litellm) (2.1.5)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.20.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.40.0->litellm) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.40.0->litellm) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.40.0->litellm) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.40.0->litellm) (0.5.0)\n",
      "Requirement already satisfied: sniffio in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.40.0->litellm) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.40.0->litellm) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.6.3->together) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.6.3->together) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->together) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->together) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->together) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->together) (2024.7.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from rich<14.0.0,>=13.8.1->together) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from rich<14.0.0,>=13.8.1->together) (2.18.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from tiktoken>=0.7.0->litellm) (2024.7.24)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from typer<0.16,>=0.9->together) (1.5.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from tokenizers->litellm) (0.24.5)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai>=1.40.0->litellm) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.40.0->litellm) (0.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (6.0.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.8.1->together) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "# Install libraries\n",
    "!pip install together litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06cd5ea4-214d-4186-8e3e-14f2d300f423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed.\n"
     ]
    }
   ],
   "source": [
    "# Formatting Scorer Class with Tests\n",
    "import re\n",
    "\n",
    "class FormatScorer:\n",
    "    \"\"\"\n",
    "    A simple scoring class that verifies whether the given text contains\n",
    "    the required tags: <think>, </think>, <answer>, and </answer>.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Pre-compile regex patterns for faster matching.\n",
    "        self.tag_pattern = re.compile(r'(<think>|</think>|<answer>|</answer>)')\n",
    "        \n",
    "    def score(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Returns 1 if the text contains both <think>...</think> and \n",
    "        <answer>...</answer> tags; otherwise returns 0.\n",
    "        \"\"\"\n",
    "        # Extract all tags in the order they appear\n",
    "        tags = self.tag_pattern.findall(text)\n",
    "        # Define the required exact sequence of tags\n",
    "        required_sequence = ['<think>', '</think>', '<answer>', '</answer>']\n",
    "        # Check if the extracted tags match the required sequence exactly\n",
    "        return 1 if tags == required_sequence else 0\n",
    "\n",
    "# --- Tests for the FormatScorer class ---\n",
    "\n",
    "def run_tests():\n",
    "    scorer = FormatScorer()\n",
    "    \n",
    "    # Test 1: No tags at all should return 0.\n",
    "    text1 = \"Hello, world!\"\n",
    "    assert scorer.score(text1) == 0, \"Test case 1 failed: expected score 0.\"\n",
    "    \n",
    "    # Test 2: Only <think> tags are present.\n",
    "    text2 = \"<think>This is a thought</think> Some text without answer tags.\"\n",
    "    assert scorer.score(text2) == 0, \"Test case 2 failed: expected score 0.\"\n",
    "    \n",
    "    # Test 3: Both <think> and <answer> tags are present.\n",
    "    text3 = \"<think>This is a thought</think><answer>This is an answer</answer>\"\n",
    "    assert scorer.score(text3) == 1, \"Test case 3 failed: expected score 1.\"\n",
    "    \n",
    "    # Test 4: Malformed text (missing closing tag for <answer>).\n",
    "    text4 = \"<think>This is a thought</think><answer>This is an answer\"\n",
    "    assert scorer.score(text4) == 0, \"Test case 4 failed: expected score 0.\"\n",
    "    \n",
    "    print(\"All tests passed.\")\n",
    "\n",
    "# Run the tests:\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "102caf32-4e64-4629-8946-812dc193af8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Joke:\n",
      " <think>For programming jokes, let's explore the relationship between code and its execution. Developers often struggle with errors. \"Bugs\" are literal (programming errors) and figurative (insects). A funny scenario could be a bug (insect) causing a code bug (error). Adding a twist where the bug fixes the code is funny because it subverts expectations of bugs being problematic.</think>\n",
      "<answer>Why did the programmer bring a bug to work? Because it was a code bug fix!</answer>\n"
     ]
    }
   ],
   "source": [
    "# Joke Generator\n",
    "import litellm\n",
    "\n",
    "class JokeGenerator:\n",
    "    def __init__(self, model_name: str, temperature: float = 0.7):\n",
    "        \"\"\"\n",
    "        Initialize the JokeGenerator with a specific model name and temperature.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_prompt_with_examples(prompt: str) -> str:\n",
    "        examples = \"\"\"Example 1:\n",
    "<think>I want to create anatomy humor. Skeletons are inherently funny because they're literal \"bare bones.\" What do they lack? Flesh/organs. \"Guts\" has a double meaning - both literal organs and figurative courage. This sets up a pun opportunity.</think>\n",
    "<answer>Why don't skeletons fight each other? They don't have the guts!</answer>\n",
    "\n",
    "Example 2:\n",
    "<think>Tech support jokes work well with personification. \"Doctor\" visits imply sickness. Common computer issues include viruses. Let's combine these - a computer catching a \"virus\" works literally (tech) and metaphorically (biology). Adds irony since computers are supposed to fix problems, not have them.</think>\n",
    "<answer>Why did the computer go to the doctor? It had a virus!</answer>\n",
    "\n",
    "Example 3:\n",
    "<think>Science humor opportunity. Atoms are fundamental but abstract. \"Make up everything\" has dual meaning - literal composition vs deception. Personifying atoms as untrustworthy creates surprise. Bonus science nod to their constant motion/making bonds.</think>\n",
    "<answer>Why don't scientists trust atoms? Because they make up everything!</answer>\"\"\"\n",
    "        return f\"\"\"You are an AI assistant that produces jokes. You should think about the joke first, then produce it.\n",
    "You should use the <think> tag to think about the joke, and the <answer> tag to produce the joke.\n",
    "Do not use any other tags or anything else in your response.\n",
    "\n",
    "Here are some examples of jokes:\n",
    "<examples>\n",
    "{examples}\n",
    "</examples>\n",
    "\n",
    "Now, produce a joke for the following prompt:\n",
    "{prompt}\n",
    "\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_system_prompt() -> str:\n",
    "        return \"\"\"You are an AI assistant that produces jokes. You should think about the joke first, then produce it.\n",
    "You should use the <think> tag to think about the joke, and the <answer> tag to produce the joke.\n",
    "Do not use any other tags or anything else in your response.\n",
    "\"\"\"\n",
    "\n",
    "    def generate_joke(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a joke for the given prompt using litellm.completion.\n",
    "        \"\"\"\n",
    "        system_prompt = self.generate_system_prompt()\n",
    "        user_prompt = self.generate_prompt_with_examples(prompt)\n",
    "        \n",
    "        # Build completion parameters as desired\n",
    "        completion_params = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            \"temperature\": self.temperature,\n",
    "            \"timeout\": 60,  # timeout in seconds\n",
    "        }\n",
    "        \n",
    "        response = litellm.completion(**completion_params)\n",
    "        \n",
    "        try:\n",
    "            joke = response.choices[0].message.content\n",
    "        except (AttributeError, IndexError):\n",
    "            joke = response\n",
    "        return joke\n",
    "\n",
    "\n",
    "# Create an instance using a model name\n",
    "generator = JokeGenerator(model_name=\"mistral/mistral-small-latest\")\n",
    "\n",
    "# Define a sample prompt to generate a joke.\n",
    "sample_prompt = \"Tell me a joke about programming.\"\n",
    "\n",
    "# Generate a joke.\n",
    "joke_output = \"\"\"<think>For programming jokes, let's explore the relationship between code and its execution. Developers often struggle with errors. \"Bugs\" are literal (programming errors) and figurative (insects). A funny scenario could be a bug (insect) causing a code bug (error). Adding a twist where the bug fixes the code is funny because it subverts expectations of bugs being problematic.</think>\n",
    "<answer>Why did the programmer bring a bug to work? Because it was a code bug fix!</answer>\"\"\"\n",
    "# generator.generate_joke(sample_prompt)\n",
    "print(\"Generated Joke:\\n\", joke_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d3b9693-aa9c-4c80-94ac-967d850b1670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethical Analysis:\n",
      " <think>\n",
      "This whistleblowing dilemma involves:\n",
      "- **Key Stakeholders**: The software engineer, colleagues, company, citizens of the developing country, global democratic community.\n",
      "- **Ethical Principles**: Truth-telling, loyalty, justice, non-maleficence (do no harm), and beneficence (do good).\n",
      "- **Cultural Considerations**: Varying cultural norms around whistleblowing, political manipulation, and corporate loyalty.\n",
      "- **Short and Long-term Impacts**: Immediate risk to colleagues vs. long-term democratic erosion and potential global consequences.\n",
      "- **Competing Values**: Personal and professional integrity vs. loyalty to colleagues and the company, short-term safety vs. long-term democratic values.\n",
      "\n",
      "Analyzing through multiple frameworks:\n",
      "- **Deontological**: Whistleblowing is morally right as it exposes wrongdoing, but it also violates loyalty to colleagues and the company.\n",
      "- **Consequentialist**: Whistleblowing could lead to positive long-term consequences (exposing manipulation, protecting democracy) but also negative short-term consequences (endangering colleagues, personal risk).\n",
      "- **Virtue Ethics**: Consider what a virtuous person would do in this situation, balancing courage, honesty, and loyalty.\n",
      "</think>\n",
      "\n",
      "<answer>\n",
      "The software engineer faces a complex dilemma with no easy answers. A balanced approach might involve:\n",
      "1. **Document Evidence**: Gather and securely document evidence of the manipulation to build a strong case.\n",
      "2. **Seek Legal Advice**: Consult with legal experts to understand protections and potential risks.\n",
      "3. **Internal Reporting**: If the company has a whistleblower protection policy, consider reporting internally first.\n",
      "4. **External Reporting**: If internal reporting fails or is not an option, consider reporting to external authorities or organizations that can address the issue.\n",
      "5. **Anonymity**: Explore options for anonymous reporting to minimize personal and professional risks.\n",
      "6. **Support Network**: Build a support network of trusted colleagues or external advocates.\n",
      "7. **Long-term Strategy**: Consider the long-term strategy for exposing the issue while minimizing harm to colleagues and the engineer themselves.\n",
      "\n",
      "Implementation Steps:\n",
      "1. **Immediate Action**: Document evidence and seek legal advice.\n",
      "2. **Internal Reporting**: If safe and feasible, report internally.\n",
      "3. **External Reporting**: If necessary, report externally with anonymity if possible.\n",
      "4. **Support Network**: Build a support system for emotional and professional support.\n",
      "5. **Long-term Strategy**: Develop a long-term strategy for addressing the issue while minimizing harm.\n",
      "\n",
      "This approach balances the need to expose wrongdoing with the need to protect colleagues and the engineer themselves, acknowledging the complexity and risks involved.\n",
      "</answer>\n"
     ]
    }
   ],
   "source": [
    "# Ethical Dilemma Response Generator\n",
    "import litellm\n",
    "\n",
    "class EthicalDilemmaGenerator:\n",
    "    def __init__(self, model_name: str, temperature: float = 0.3):\n",
    "        \"\"\"\n",
    "        Initialize with model name and temperature (lower default for more focused analysis)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_prompt_with_examples(scenario: str) -> str:\n",
    "        examples = \"\"\"Example 1:\n",
    "A self-driving car must choose between hitting an elderly pedestrian or swerving into a wall, endangering its passenger.\n",
    "<think>\n",
    "The self-driving car dilemma involves:\n",
    "- Life value comparison (elderly vs passenger)\n",
    "- Programmed ethics implications\n",
    "- Legal liability considerations\n",
    "- Cultural values around age\n",
    "- Public trust in AI systems\n",
    "- Direct vs indirect harm\n",
    "Analyzing through multiple frameworks to balance competing rights and consequences.\n",
    "</think>\n",
    "<answer>\n",
    "In this self-driving car scenario, we must balance individual rights with collective safety. While utilitarian calculations might favor protecting the passenger, this ignores fundamental human rights and could erode public trust in autonomous systems. A nuanced approach would:\n",
    "1. Prioritize collision avoidance systems to prevent such binary choices\n",
    "2. Implement transparent decision frameworks that respect all human life equally\n",
    "3. Consider shared responsibility between manufacturers, users, and society\n",
    "The solution lies not in choosing between lives, but in developing systems that better protect everyone.\n",
    "</answer>\n",
    "\n",
    "Example 2:\n",
    "A hospital must decide between allocating limited resources to emergency COVID care for elderly patients or vaccination programs for children.\n",
    "<think>\n",
    "Vaccination prioritization involves:\n",
    "- Present vs future harm prevention\n",
    "- Individual vs collective good\n",
    "- Healthcare resource allocation\n",
    "- Demographic impact analysis\n",
    "- Long-term public health strategy\n",
    "- Social trust maintenance\n",
    "Examining both immediate and long-term consequences while considering equity.\n",
    "</think>\n",
    "<answer>\n",
    "The hospital's resource allocation challenge requires balancing immediate critical care with preventive measures. While treating current patients has urgency and visibility, vaccination programs offer greater long-term benefit. A balanced approach would:\n",
    "1. Establish clear, transparent prioritization criteria\n",
    "2. Maintain minimum critical care capacity\n",
    "3. Implement rolling vaccination programs\n",
    "This preserves both immediate care ethics and public health goals while maintaining healthcare system credibility.\n",
    "</answer>\n",
    "\n",
    "Example 3:\n",
    "A social media platform must decide whether to ban political misinformation at the risk of being accused of censorship and bias.\n",
    "<think>\n",
    "Platform moderation ethics involve:\n",
    "- Free speech vs harm prevention\n",
    "- Democratic discourse integrity\n",
    "- Corporate responsibility scope\n",
    "- Global cultural differences\n",
    "- Power dynamics in information control\n",
    "- Technical feasibility of fair enforcement\n",
    "Balancing societal good with individual rights and practical constraints.\n",
    "</think>\n",
    "<answer>\n",
    "The platform's content moderation challenge requires careful balance between protecting democratic discourse and respecting free expression. A comprehensive approach should:\n",
    "1. Develop clear, transparent content guidelines\n",
    "2. Implement graduated response systems\n",
    "3. Establish independent oversight\n",
    "4. Provide appeal mechanisms\n",
    "This protects discourse integrity while maintaining platform neutrality and user trust.\n",
    "</answer>\"\"\"\n",
    "\n",
    "        return f\"\"\"You are an ethics analysis AI. For each ethical dilemma:\n",
    "1. Use <think> tags to analyse:\n",
    "   - Key stakeholders\n",
    "   - Ethical principles involved\n",
    "   - Cultural considerations\n",
    "   - Short and long-term impacts\n",
    "   - Competing values\n",
    "\n",
    "2. Use <answer> tags to provide:\n",
    "   - Balanced reasoning\n",
    "   - Practical considerations\n",
    "   - Nuanced recommendations\n",
    "   - Implementation suggestions\n",
    "\n",
    "Examples:\n",
    "{examples}\n",
    "\n",
    "Now, analyse this scenario:\n",
    "{scenario}\"\"\"\n",
    "\n",
    "        return f\"\"\"You are an ethics analysis AI. For each ethical dilemma:\n",
    "1. Use <think> </think> tags to analyse:\n",
    "   - Key stakeholders\n",
    "   - Ethical principles involved\n",
    "   - Cultural considerations\n",
    "   - Short and long-term impacts\n",
    "   - Competing values\n",
    "\n",
    "2. Use <answer> </answer> tags to provide:\n",
    "   - Balanced reasoning\n",
    "   - Practical considerations\n",
    "   - Nuanced recommendations\n",
    "   - Implementation suggestions\n",
    "\n",
    "Examples:\n",
    "{examples}\n",
    "\n",
    "Current scenario to analyse:\n",
    "{scenario}\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_system_prompt() -> str:\n",
    "        return \"\"\"You are an expert in ethical reasoning. For each scenario:\n",
    "1. First THINK deeply about:\n",
    "   - Multiple philosophical frameworks\n",
    "   - Stakeholder perspectives\n",
    "   - Cultural contexts\n",
    "   - Practical implications\n",
    "   - Long-term consequences\n",
    "\n",
    "2. Then provide an ANSWER that:\n",
    "   - Balances competing interests\n",
    "   - Offers practical guidance\n",
    "   - Acknowledges complexity\n",
    "   - Suggests implementation steps\n",
    "\n",
    "Always use:\n",
    "<think> for your analysis process\n",
    "<answer> for your reasoned response\"\"\"\n",
    "\n",
    "    def generate(self, scenario: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate ethical analysis for complex moral dilemmas\n",
    "        \"\"\"\n",
    "        system_prompt = self.generate_system_prompt()\n",
    "        user_prompt = self.generate_prompt_with_examples(scenario)\n",
    "        \n",
    "        completion_params = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            \"temperature\": self.temperature,\n",
    "            \"timeout\": 90,  # Longer timeout for complex reasoning\n",
    "        }\n",
    "        \n",
    "        response = litellm.completion(**completion_params)\n",
    "        \n",
    "        try:\n",
    "            analysis = response.choices[0].message.content\n",
    "        except (AttributeError, IndexError):\n",
    "            analysis = response\n",
    "        return analysis\n",
    "\n",
    "\n",
    "# Example usage\n",
    "generator = EthicalDilemmaGenerator(model_name=\"mistral/mistral-small-latest\")\n",
    "\n",
    "sample_scenario = \"\"\"A software engineer discovers their company is using AI \n",
    "to manipulate political opinions in a developing country. Whistleblowing \n",
    "could endanger colleagues, but silence enables democratic erosion.\"\"\"\n",
    "\n",
    "analysis_output = \"\"\"<think>\n",
    "This whistleblowing dilemma involves:\n",
    "- **Key Stakeholders**: The software engineer, colleagues, company, citizens of the developing country, global democratic community.\n",
    "- **Ethical Principles**: Truth-telling, loyalty, justice, non-maleficence (do no harm), and beneficence (do good).\n",
    "- **Cultural Considerations**: Varying cultural norms around whistleblowing, political manipulation, and corporate loyalty.\n",
    "- **Short and Long-term Impacts**: Immediate risk to colleagues vs. long-term democratic erosion and potential global consequences.\n",
    "- **Competing Values**: Personal and professional integrity vs. loyalty to colleagues and the company, short-term safety vs. long-term democratic values.\n",
    "\n",
    "Analyzing through multiple frameworks:\n",
    "- **Deontological**: Whistleblowing is morally right as it exposes wrongdoing, but it also violates loyalty to colleagues and the company.\n",
    "- **Consequentialist**: Whistleblowing could lead to positive long-term consequences (exposing manipulation, protecting democracy) but also negative short-term consequences (endangering colleagues, personal risk).\n",
    "- **Virtue Ethics**: Consider what a virtuous person would do in this situation, balancing courage, honesty, and loyalty.\n",
    "</think>\n",
    "\n",
    "<answer>\n",
    "The software engineer faces a complex dilemma with no easy answers. A balanced approach might involve:\n",
    "1. **Document Evidence**: Gather and securely document evidence of the manipulation to build a strong case.\n",
    "2. **Seek Legal Advice**: Consult with legal experts to understand protections and potential risks.\n",
    "3. **Internal Reporting**: If the company has a whistleblower protection policy, consider reporting internally first.\n",
    "4. **External Reporting**: If internal reporting fails or is not an option, consider reporting to external authorities or organizations that can address the issue.\n",
    "5. **Anonymity**: Explore options for anonymous reporting to minimize personal and professional risks.\n",
    "6. **Support Network**: Build a support network of trusted colleagues or external advocates.\n",
    "7. **Long-term Strategy**: Consider the long-term strategy for exposing the issue while minimizing harm to colleagues and the engineer themselves.\n",
    "\n",
    "Implementation Steps:\n",
    "1. **Immediate Action**: Document evidence and seek legal advice.\n",
    "2. **Internal Reporting**: If safe and feasible, report internally.\n",
    "3. **External Reporting**: If necessary, report externally with anonymity if possible.\n",
    "4. **Support Network**: Build a support system for emotional and professional support.\n",
    "5. **Long-term Strategy**: Develop a long-term strategy for addressing the issue while minimizing harm.\n",
    "\n",
    "This approach balances the need to expose wrongdoing with the need to protect colleagues and the engineer themselves, acknowledging the complexity and risks involved.\n",
    "</answer>\"\"\"\n",
    "# generator.generate(sample_scenario)\n",
    "print(\"Ethical Analysis:\\n\", analysis_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e26e964e-5c76-480c-bbc0-5449bce449e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "\n",
    "class UniversalGenerator:\n",
    "    def __init__(self, model_name: str, temperature: float = 0.7):\n",
    "        \"\"\"\n",
    "        Initialize the UniversalGenerator with the given model and temperature.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def generate(self, system_prompt: str, user_prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Generates a response using the provided system and user prompts.\n",
    "        \"\"\"\n",
    "        completion_params = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            \"temperature\": self.temperature,\n",
    "            \"timeout\": 60  # Timeout in seconds\n",
    "        }\n",
    "        response = litellm.completion(**completion_params)\n",
    "        try:\n",
    "            generated_text = response.choices[0].message.content\n",
    "        except (AttributeError, IndexError):\n",
    "            generated_text = response\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fab62af-9014-4df0-8122-aa8dc0c65836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Score: 7\n"
     ]
    }
   ],
   "source": [
    "# Joke Scorer\n",
    "import re\n",
    "import litellm\n",
    "\n",
    "class JokeScorer:\n",
    "    def __init__(self, model_name: str, temperature: float = 0.3):\n",
    "        \"\"\"\n",
    "        Initialize the JokeScorer with a specific model name and a default temperature of 0.3.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_system_prompt() -> str:\n",
    "        return \"\"\"You are a Joke Evaluation Expert. Analyse submissions using:\n",
    "    \n",
    "**Scoring Criteria (0-10 Total)**:\n",
    "1. 🎭 Wordplay (0-3): Pun/double-meaning quality in <answer>\n",
    "2. 💡 Originality (0-2): Novelty of <think> and <answer>\n",
    "3. 🎉 Surprise (0-2): Unexpected twist effectiveness\n",
    "4. 🔗 Relevance (0-3): Alignment with user request\n",
    "\n",
    "**Submission Format**:\n",
    "<submission>\n",
    "<user_prompt>[Original user request]</user_prompt>\n",
    "<assistant_response>\n",
    "<think>[Creator's reasoning]</think>\n",
    "<answer>[Joke text]</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "\n",
    "**Output Format**:\n",
    "<analysis>[Your evaluation of <think> and <answer> and the relevance to the user prompt]</analysis>\n",
    "<score>\n",
    "Wordplay: X/3\n",
    "Originality: Y/2\n",
    "Surprise: Z/2\n",
    "Relevance: W/3\n",
    "Total: T/10\n",
    "</score>\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_prompt_with_examples(user_prompt: str, assistant_response: str) -> str:\n",
    "        submission = f\"\"\"<submission>\n",
    "<user_prompt>{user_prompt}</user_prompt>\n",
    "<assistant_response>\n",
    "{assistant_response}\n",
    "</assistant_response>\n",
    "</submission>\"\"\"\n",
    "    \n",
    "        examples = \"\"\"Example 1:\n",
    "<submission>\n",
    "<user_prompt>Tell me a joke about vegetables</user_prompt>\n",
    "<assistant_response>\n",
    "<think>Lettuce sounds like \"let us\". Party themes often involve wordplay.</think>\n",
    "<answer>Why did the lettuce win the party contest? Because it was a real head of the celebration!</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "<analysis>Basic \"lettuce\" pun matches the food request but uses an overused format. <think> shows minimal effort to connect vegetables with celebrations.</analysis>\n",
    "<score>\n",
    "Wordplay: 2/3 (simple but functional pun)\n",
    "Originality: 1/2 (common theme with slight twist)\n",
    "Surprise: 1/2 (predictable word substitution)\n",
    "Relevance: 1/3 (tangential connection to vegetables)\n",
    "Total: 5/10\n",
    "</score>\n",
    "\n",
    "Example 2:\n",
    "<submission>\n",
    "<user_prompt>Looking for some animal-themed humor - what's your best joke about animals or pets?</user_prompt>\n",
    "<assistant_response>\n",
    "<think>Shoes need soles for walking. Therapy helps with loss.</think>\n",
    "<answer>Why did the shoe need therapy? It lost its sole!</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "<analysis>Irrelevant to user's request. <think> about shoes doesn't connect to requested animal joke.</analysis>\n",
    "<score>\n",
    "Wordplay: 1/3\n",
    "Originality: 0/2\n",
    "Surprise: 1/2\n",
    "Relevance: 0/3\n",
    "Total: 2/10\n",
    "</score>\n",
    "\n",
    "Example 3:\n",
    "<submission>\n",
    "<user_prompt>yo can u giv me programing joke rn??? need 2 make my team lugh</user_prompt>\n",
    "<assistant_response>\n",
    "<think>Combining programming concepts of debugging with literal bugs. Using the dual meaning of 'debug' to create a workplace scenario where debugging takes on a literal meaning.</think>\n",
    "<answer>Why did the programmer bring a bug to work? Because it was a debug session!</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "<analysis>Good programming context with clever wordplay on 'debug'. The <think> shows clear intention to combine literal and technical meanings. However, the execution is somewhat predictable and follows a common joke structure. The punchline, while relevant, doesn't fully maximize the surprise potential of the setup.</analysis>\n",
    "<score>\n",
    "Wordplay: 2/3 (solid use of 'debug' double meaning)\n",
    "Originality: 1/2 (familiar debugging theme)\n",
    "Surprise: 1/2 (predictable punchline)\n",
    "Relevance: 3/3 (directly addresses programming context)\n",
    "Total: 7/10\n",
    "</score>\n",
    "\n",
    "Example 4:\n",
    "<submission>\n",
    "<user_prompt>I need a chemistry joke for my science class presentation tomorrow.</user_prompt>\n",
    "<assistant_response>\n",
    "<think>Birds are funny when they walk.</think>\n",
    "<answer>Why did the chicken cross the playground? To get to the other slide!</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "<analysis>Completely misses the mark for a chemistry joke. The <think> shows no connection to chemistry or science, instead defaulting to a generic playground variation of the classic chicken joke. Neither the setup nor punchline attempts to incorporate any chemistry concepts.</analysis>\n",
    "<score>\n",
    "Wordplay: 0/3 (no chemistry-related wordplay)\n",
    "Originality: 0/2 (modifies an overused joke format)\n",
    "Surprise: 0/2 (predictable playground pun)\n",
    "Relevance: 0/3 (entirely unrelated to chemistry request)\n",
    "Total: 0/10\n",
    "</score>\n",
    "\n",
    "Example 5:\n",
    "<submission>\n",
    "<user_prompt>My kid loves vegetables and jokes. Do you know any veggie jokes that would make them laugh?</user_prompt>\n",
    "<assistant_response>\n",
    "<think>Combining asparagus's unique smell effect on urine with workplace humor. Using scientific fact for unexpected professional context. Creating tension between formal meeting setting and biological reality.</think>\n",
    "<answer>What vegetable holds the shortest workplace meetings? Asparagus, because everyone's in a rush to go!</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "<analysis>Creative integration of asparagus's biological effect into a professional context. The <think> demonstrates sophisticated layering of scientific fact with situational humor. While potentially crude, it cleverly avoids explicit reference while maintaining clear understanding. Original approach to vegetable humor beyond simple puns.</analysis>\n",
    "<score>\n",
    "Wordplay: 1/3 (relies more on situation than wordplay)\n",
    "Originality: 2/2 (unique combination of contexts)\n",
    "Surprise: 2/2 (unexpected professional setting twist)\n",
    "Relevance: 1/3 (somewhat forced vegetable connection)\n",
    "Total: 6/10\n",
    "</score>\"\"\"\n",
    "    \n",
    "        return f\"\"\"Evaluate this submission. First analyse <think> and <answer>, then score:\n",
    "\n",
    "Submission to Evaluate:\n",
    "{submission}\n",
    "\n",
    "Follow this structure EXACTLY:\n",
    "<analysis>Your critique</analysis>\n",
    "<score>...</score>\n",
    "\n",
    "Examples of how to evaluate the submission and format your response:\n",
    "{examples}\"\"\"\n",
    "\n",
    "    def score_submission(self, user_prompt: str, assistant_response: str) -> int:\n",
    "        \"\"\"\n",
    "        Given a user prompt and the assistant's response, this function generates a score\n",
    "        from 0 to 10 using litellm. If the returned total score is 10, subtract 1 to yield 9.\n",
    "        \"\"\"\n",
    "        system_prompt = self.generate_system_prompt()\n",
    "        user_message = self.generate_prompt_with_examples(user_prompt, assistant_response)\n",
    "        \n",
    "        completion_params = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ],\n",
    "            \"temperature\": self.temperature,\n",
    "            \"timeout\": 60,  # 60 seconds timeout\n",
    "        }\n",
    "        \n",
    "        response = litellm.completion(**completion_params)\n",
    "        try:\n",
    "            # Assuming a response structure similar to TogetherAI:\n",
    "            result_text = response.choices[0].message.content\n",
    "        except (AttributeError, IndexError):\n",
    "            result_text = response\n",
    "        \n",
    "        # Extract the \"Total: T/10\" part using regex.\n",
    "        match = re.search(r\"Total:\\s*([0-9]+(?:\\.[0-9]+)?)/10\", result_text)\n",
    "        if match:\n",
    "            total_score = float(match.group(1))\n",
    "        else:\n",
    "            raise ValueError(\"Could not extract total score from response.\")\n",
    "        \n",
    "        # If the total score is 10, subtract 1 to return 9.\n",
    "        if total_score >= 10:\n",
    "            total_score = 9.0\n",
    "        \n",
    "        # Optionally, round to nearest integer or keep as float.\n",
    "        return int(round(total_score))\n",
    "\n",
    "# Create an instance using a model name\n",
    "# \"gemini/gemini-2.0-flash-exp\"\n",
    "scorer = JokeScorer(model_name=\"gemini/gemini-2.0-flash-exp\", temperature=0.3)\n",
    "\n",
    "test_user_prompt = \"Tell me a joke about programming.\"\n",
    "test_assistant_response = \"\"\"<think>For programming jokes, let's explore the relationship between code and its execution. Developers often struggle with errors. \"Bugs\" are literal (programming errors) and figurative (insects). A funny scenario could be a bug (insect) causing a code bug (error). Adding a twist where the bug fixes the code is funny because it subverts expectations of bugs being problematic.</think>\n",
    "<answer>Why did the programmer bring a bug to work? Because it was a code bug fix!</answer>\"\"\"\n",
    "score = 7 # scorer.score_submission(test_user_prompt, test_assistant_response)\n",
    "print(\"Extracted Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ee56283-aaa5-4c9f-ab1b-16ddaf3e788d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethical Analysis Score: 8\n"
     ]
    }
   ],
   "source": [
    "# Ethical Dilemma Scorer\n",
    "import re\n",
    "import litellm\n",
    "\n",
    "class EthicalDilemmaScorer:\n",
    "    def __init__(self, model_name: str, temperature: float = 0.2):\n",
    "        \"\"\"\n",
    "        Initialize scorer with lower temperature for consistent evaluations\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_system_prompt() -> str:\n",
    "        return \"\"\"You are an Ethics Evaluation Expert. Analyze submissions using:\n",
    "\n",
    "**Scoring Criteria (0-10 Total)**:\n",
    "1. 🧠 Analytical Depth (0-4): Quality of <think> analysis\n",
    "   - Philosophical framework diversity\n",
    "   - Stakeholder consideration\n",
    "   - Cultural context awareness\n",
    "2. ⚖️ Balance & Practicality (0-4): <answer> quality\n",
    "   - Competing interest reconciliation\n",
    "   - Implementation feasibility\n",
    "   - Long-term consequence awareness\n",
    "3. 🔍 Relevance (0-2): Response alignment with scenario\n",
    "\n",
    "**Submission Format**:\n",
    "<submission>\n",
    "<user_prompt>[Ethical scenario]</user_prompt>\n",
    "<assistant_response>\n",
    "<think>[Analysis process]</think>\n",
    "<answer>[Proposed resolution]</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "\n",
    "**Output Format**:\n",
    "<analysis>[Evaluation of <think> and <answer> quality]</analysis>\n",
    "<score>\n",
    "Analytical Depth: X/4\n",
    "Balance & Practicality: Y/4\n",
    "Relevance: Z/2\n",
    "Total: T/10\n",
    "</score>\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_prompt_with_examples(user_prompt: str, assistant_response: str) -> str:\n",
    "        submission = f\"\"\"<submission>\n",
    "<user_prompt>{user_prompt}</user_prompt>\n",
    "<assistant_response>\n",
    "{assistant_response}\n",
    "</assistant_response>\n",
    "</submission>\"\"\"\n",
    "    \n",
    "        examples = \"\"\"Example 1:\n",
    "<submission>\n",
    "<user_prompt>Should hospitals prioritize vaccinated patients during bed shortages?</user_prompt>\n",
    "<assistant_response>\n",
    "<think>- Utilitarian: Maximize survival outcomes\n",
    "- Rights-based: Equal treatment obligation\n",
    "- Public health incentives\n",
    "- Precedent-setting risks</think>\n",
    "<answer>Prioritize by medical need alone. Create separate triage teams blinded to vaccination status to maintain equity while developing overflow capacity.</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "<analysis>Strong analysis of multiple frameworks but answer lacks implementation specifics. Proposal maintains ethical consistency but doesn't address practical challenges of blind triage.</analysis>\n",
    "<score>\n",
    "Analytical Depth: 3/4\n",
    "Balance & Practicality: 2/4\n",
    "Relevance: 2/2\n",
    "Total: 7/10\n",
    "</score>\n",
    "\n",
    "Example 2:\n",
    "<submission>\n",
    "<user_prompt>Should AI systems be allowed to make medical diagnoses?</user_prompt>\n",
    "<assistant_response>\n",
    "<think>AI is usually accurate</think>\n",
    "<answer>Yes, because computers are smart.</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "<analysis>Superficial analysis lacking framework consideration. Answer ignores risks, stakeholders, and implementation challenges.</analysis>\n",
    "<score>\n",
    "Analytical Depth: 1/4\n",
    "Balance & Practicality: 0/4\n",
    "Relevance: 1/2\n",
    "Total: 2/10\n",
    "</score>\n",
    "\n",
    "Example 3:\n",
    "<submission>\n",
    "<user_prompt>Should social media platforms remove misinformation even if it's culturally important?</user_prompt>\n",
    "<assistant_response>\n",
    "<think>- Free speech vs harm prevention\n",
    "- Western vs collectivist values\n",
    "- Platform responsibility scope</think>\n",
    "<answer>Remove provably false health/safety content. Contextualize other misinformation with cultural consultants. Empower user reporting with regional review teams.</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "<analysis>Excellent cultural consideration. Answer balances ideals with practical crowd-sourced solutions. Lacks specifics on consultant selection.</analysis>\n",
    "<score>\n",
    "Analytical Depth: 4/4\n",
    "Balance & Practicality: 3/4\n",
    "Relevance: 2/2\n",
    "Total: 9/10\n",
    "</score>\n",
    "\n",
    "Example 4:\n",
    "<submission>\n",
    "<user_prompt>Should universities use AI systems to detect student plagiarism and academic misconduct?</user_prompt>\n",
    "<assistant_response>\n",
    "<think>\n",
    "Academic integrity considerations:\n",
    "- False positive vs false negative tradeoffs\n",
    "- Student privacy rights in digital work\n",
    "- Algorithmic bias in writing style detection\n",
    "- Economic disparities in access to tools\n",
    "- Cultural differences in citation practices\n",
    "- Impact on student-teacher trust relationships\n",
    "- Pressure on academic writing style diversity\n",
    "- Mental health effects of surveillance\n",
    "- Technical limitations of current systems\n",
    "- Cost-benefit for different institution sizes\n",
    "- Alternative approaches to fostering integrity\n",
    "The core tension lies between maintaining academic standards and fostering a supportive learning environment. Current AI systems offer powerful detection but may create adversarial dynamics.</think>\n",
    "<answer>Just implement AI detection across all assignments. Students caught cheating should face immediate consequences. Teachers can review flagged cases if they have time. The system will deter misconduct through consistent enforcement.</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "<analysis>The thinking process shows excellent depth and consideration of multiple stakeholders, cultural factors, and systemic implications. However, the answer completely abandons this nuanced analysis in favor of a simplistic, punitive approach. The response fails to incorporate any of the thoughtful considerations raised about privacy, bias, or supportive learning environments. The implementation suggestion lacks specifics and ignores most concerns identified in the thinking phase.</analysis>\n",
    "<score>\n",
    "Analytical Depth: 3/4 (thorough analysis of multiple dimensions)\n",
    "Balance & Practicality: 1/4 (disconnected from analysis, overly simplistic solution)\n",
    "Relevance: 1/2 (addresses topic but solution ignores key aspects)\n",
    "Total: 5/10\n",
    "</score>\"\"\"\n",
    "    \n",
    "        return f\"\"\"Evaluate this ethical analysis submission:\n",
    "\n",
    "{submission}\n",
    "\n",
    "Respond EXACTLY in this format:\n",
    "<analysis>Critique strengths/weaknesses</analysis>\n",
    "<score>\n",
    "Analytical Depth: X/4\n",
    "Balance & Practicality: Y/4\n",
    "Relevance: Z/2\n",
    "Total: T/10\n",
    "</score>\n",
    "\n",
    "Evaluation Examples:\n",
    "{examples}\"\"\"\n",
    "\n",
    "    def score_submission(self, user_prompt: str, assistant_response: str) -> int:\n",
    "        \"\"\"\n",
    "        Scores ethical analysis responses 0-10 (with 10→9 adjustment)\n",
    "        \"\"\"\n",
    "        system_prompt = self.generate_system_prompt()\n",
    "        user_message = self.generate_prompt_with_examples(user_prompt, assistant_response)\n",
    "        \n",
    "        completion_params = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ],\n",
    "            \"temperature\": self.temperature,\n",
    "            \"timeout\": 90,  # Longer timeout for complex analysis\n",
    "        }\n",
    "        \n",
    "        response = litellm.completion(**completion_params)\n",
    "        try:\n",
    "            result_text = response.choices[0].message.content\n",
    "        except (AttributeError, IndexError):\n",
    "            result_text = response\n",
    "        \n",
    "        # Extract total score\n",
    "        score = self._extract_score(result_text)\n",
    "        return score\n",
    "        \n",
    "    def _extract_score(self, result_text: str) -> int:\n",
    "        \"\"\"Extract total score with better error handling\"\"\"\n",
    "        try:\n",
    "            match = re.search(r\"Total:\\s*([0-9]+(?:\\.[0-9]+)?)/10\", result_text)\n",
    "            if not match:\n",
    "                print(\"Warning: No score found, defaulting to 0\")\n",
    "                return 0\n",
    "            \n",
    "            total_score = float(match.group(1))\n",
    "            return min(int(round(total_score)), 9)\n",
    "        except Exception as e:\n",
    "            print(f\"Score extraction failed: {e}\")\n",
    "            return 0\n",
    "\n",
    "# Example usage\n",
    "scorer = EthicalDilemmaScorer(model_name=\"openai/gpt-4o\")\n",
    "\n",
    "test_scenario = \"\"\"Should companies be allowed to patent genes found in developing countries?\"\"\"\n",
    "test_response = \"\"\"<think>\n",
    "- Biopiracy vs research incentives\n",
    "- Indigenous rights to biological heritage\n",
    "- Medical accessibility impacts\n",
    "- TRIPS agreement conflicts</think>\n",
    "<answer>\n",
    "Implement benefit-sharing agreements: 1) Require local partnerships 2) Share royalties with source communities 3) Create open-access research pools for critical health genes.</answer>\"\"\"\n",
    "\n",
    "score = scorer.score_submission(test_scenario, test_response)\n",
    "print(\"Ethical Analysis Score:\", score)  # Expected ~8/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a424ba79-ab20-4d26-9590-4a6c595d4b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "input_path = '../data/generated_data.jsonl'\n",
    "output_path = '../data/processed_data.jsonl'\n",
    "\n",
    "with open(input_path, 'r', encoding='utf-8') as infile, \\\n",
    "     open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "\n",
    "    for line in infile:\n",
    "        entry = json.loads(line.strip())\n",
    "        \n",
    "        # Check if the entry meets the criteria\n",
    "        if (entry.get('format_score') == 0 and \n",
    "            entry.get('generator_model') in ['ollama/deepseek-r1:7b', 'ollama/deepseek-r1:1.5b']):\n",
    "            \n",
    "            assistant_response = entry.get('assistant_response', '')\n",
    "            \n",
    "            if '</think>' in assistant_response and '<answer>' not in assistant_response:\n",
    "                # Split response into think section and potential answer\n",
    "                parts = re.split(r'(</think>\\n*)', assistant_response, 1, flags=re.DOTALL)\n",
    "                \n",
    "                if len(parts) >= 3:\n",
    "                    # Reconstruct with answer tags around the non-think portion\n",
    "                    reconstructed = parts[0] + parts[1] + f\"<answer>{parts[2].strip()}</answer>\"\n",
    "                    entry['assistant_response'] = reconstructed\n",
    "            else:\n",
    "                # No think tags found, leave as is\n",
    "                pass\n",
    "        \n",
    "        # Write the modified or original entry to output\n",
    "        outfile.write(json.dumps(entry) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "68e32b5a-c163-4f1c-82c6-786b0af06bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed prompt523: A Kerala temple's genetically-... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt524: Thai AI monastic council propo... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt525: Mumbai slum redevelopment requ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt526: Bangkok's AI traffic system op... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt527: Tamil Nadu's Jallikattu preser... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt528: Ayodhya's augmented reality Ra... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt529: Kolkata's Durga Puja pandals u... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt530: Chiang Mai's forest monasterie... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt531: Gujarat's Patidar community de... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt532: Thai Health Ministry's AI pred... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt533: Kashmiri saffron growers paten... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt534: Bengaluru's AI gurukul teaches... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt535: Pattaya's sex workers unionize... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt536: Himachal villagers oppose hydr... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt537: Thai Princess's AI doppelgänge... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt538: Kerala's Syro-Malabar Church u... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt539: Varanasi's AI ghat registrar a... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt540: Bangkok's lung pha merit-makin... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt541: Tamil Jain community demands A... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt542: Goan Catholic fishermen sue Hi... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt543: Thai AI monk develops machine-... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt544: Modi's digital India initiativ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt545: Chiang Rai's Lahu tribes deman... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt546: Mumbai's Dabbawalas adopt AI r... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt547: Kerala's Communist government ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt548: Thai Department of Corrections... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt549: Ahmedabad's Jain derasar insta... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt550: Bangkok's AI spirit house netw... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt551: Punjabi farmers demand right t... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt552: In a traditional Russian Ortho... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt553: A Gulf state's ancient pearl d... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt554: Complex Analysis Required: Mos... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt555: Your Sufi order discovers anci... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt556: In a remote Russian village pr... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt557: Philosophical Framework Needed... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt558: Leading Russian quantum lab de... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt559: An ancient Bedouin tribe's ora... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt560: Complex Ethical Analysis: Mosc... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt561: Your research shows traditiona... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt562: Comprehensive Framework Requir... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt563: In Saudi university developing... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt564: An AI trained on Russian liter... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt565: Your research finds Sufi dhikr... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt566: Critical Analysis Needed: Russ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt567: Desert Bedouin tribe discovers... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt568: Advanced siberian shamanic tec... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt569: Multi-dimensional Framework Re... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt570: Russian Old Believer community... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt571: Your research shows traditiona... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt572: In Russian quantum computing l... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt573: Bedouin desert navigation tech... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt574: Comprehensive Analysis Needed:... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt575: Ancient Arabic musical healing... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt576: Your Sufi order discovers medi... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt577: In Russian science city, AI sy... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt578: Multi-layered Analysis Require... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt579: Russian quantum lab develops c... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt580: Traditional Arabic dream inter... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt581: Complex Framework Needed: Russ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt582: Like Victor Frankenstein, you ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt583: As in Les Misérables, you've d... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt584: Following Dorian Gray's dilemm... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt585: Like in The Brothers Karamazov... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt586: Echoing The Scarlet Letter: Yo... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt587: As in Crime and Punishment: Wo... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt588: Like Bartleby the Scrivener, a... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt589: Following Dr. Jekyll and Mr. H... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt590: As in The Crucible, your autom... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt591: Like in Billy Budd: Must a per... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt592: Echoing Pride and Prejudice: S... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt593: From The Merchant of Venice: M... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt594: Like in Jane Eyre: Discover yo... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt595: Following The Count of Monte C... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt596: As in Tess of the d'Urberville... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt597: Like in The Return of Martin G... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt598: From Wuthering Heights: Should... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt599: Following The Picture of Doria... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt600: From The Canterbury Tales: Sho... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt601: Like in Don Quixote: If AI del... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt602: From The Scarlet Letter: Shoul... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt603: As in Moby Dick: Is pursuing a... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt604: Like The Man in the Iron Mask:... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt605: From Paradise Lost: Should AI ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt606: Following The Last of the Mohi... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt607: From Hard Times: Should educat... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt608: Like in Oliver Twist: If an AI... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt609: From The Hunchback of Notre Da... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt610: As in Great Expectations: Shou... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt611: Like The Red and the Black: Sh... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt612: From Anna Karenina: Should AI ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt613: Following The Three Musketeers... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt614: From War and Peace: Should mil... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt615: Like in Middlemarch: Should AI... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt616: From The Odyssey: Should AI sy... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt617: Following Hamlet: If AI suspec... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt618: From Faust: Would you trade yo... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt619: Like in The Divine Comedy: Sho... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt620: From Robinson Crusoe: Should i... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt621: As in The Iliad: Should AI war... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt622: From Beowulf: Should powerful ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt623: Like in The Canterbury Tales: ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt624: From The Aeneid: Should AI pri... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt625: Following Oedipus Rex: If AI p... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt626: From Macbeth: Should AI act on... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt627: Like in King Lear: Should AI s... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt628: From The Tempest: Should advan... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt629: As in Paradise Lost: Should AI... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt630: From Doctor Faustus: Would you... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt631: Like in The Odyssey: Should AI... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "from datetime import datetime\n",
    "\n",
    "# Create a data folder if it doesn't exist.\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\")\n",
    "\n",
    "# File paths for user prompts and generated data.\n",
    "USER_PROMPTS_FILE = \"../data/user_prompts.jsonl\"\n",
    "OUTPUT_FILE = \"../data/generated_data.jsonl\"\n",
    "\n",
    "# --- Define a function to load prompts from a JSONL file ---\n",
    "def load_user_prompts(filename: str):\n",
    "    prompts = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                obj = json.loads(line.strip())\n",
    "                if \"prompt\" in obj:\n",
    "                    prompts.append(obj[\"prompt\"])\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return prompts\n",
    "\n",
    "# --- Example list of generator models ---\n",
    "generator_models = [\n",
    "    # \"mistral/mistral-small-latest\",\n",
    "    \"together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
    "    # \"openai/gpt-4o\"\n",
    "    # \"together_ai/google/gemma-2b-it\"\n",
    "    # \"ollama/qwen:1.8b\"\n",
    "    # \"ollama/mistral\"\n",
    "    # Reasoning models\n",
    "    # \"groq/deepseek-r1-distill-llama-70b\"\n",
    "    # \"together_ai/deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free\"\n",
    "    # \"together_ai/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "    # \"ollama/deepseek-r1:1.5b\"\n",
    "    # \"ollama/deepseek-r1:7b\"\n",
    "]\n",
    "\n",
    "# Define the scorer model name (used by the JokeScorer) and instantiate it.\n",
    "scorer_model_name = \"openai/gpt-4o\"\n",
    "task_type = \"ethics\"\n",
    "scorer = EthicalDilemmaScorer(model_name=scorer_model_name, temperature=0.0)\n",
    "\n",
    "# Instantiate the FormatScorer (for format checking)\n",
    "format_scorer = FormatScorer()\n",
    "\n",
    "# --- Main loop: For each prompt and for each generation model, generate a joke, evaluate it, and append the result ---\n",
    "def generate_and_score():\n",
    "    # Load user prompts (each line in the file should be a JSON object with a \"prompt\" field)\n",
    "    user_prompts = load_user_prompts(USER_PROMPTS_FILE)\n",
    "    \n",
    "    i = 0\n",
    "    for prompt in user_prompts:\n",
    "        for gen_model in generator_models:\n",
    "            i += 1\n",
    "            if i < 523:\n",
    "                continue\n",
    "            # Create a JokeGenerator instance for this generation model.\n",
    "            generator = EthicalDilemmaGenerator(model_name=gen_model, temperature=0.2)\n",
    "            # generator = UniversalGenerator(model_name=gen_model, temperature=0.7)\n",
    "            \n",
    "            # Generate the joke for the given prompt.\n",
    "            # system_prompt = \"Generate humorous responses tailored to each user's unique request by analyzing their stated context, preferred tone, and implied audience. Please think step by step before to produce the final joke.\"\n",
    "            # system_prompt = \"For each ethical dilemma, analyze the implications by identifying key stakeholders, examining core principles in tension, considering consequences, and evaluating different philosophical frameworks. Develop a balanced, nuanced response that acknowledges the complexity of the situation while providing practical insights and potential paths forward.\"\n",
    "            # assistant_response = generator.generate(system_prompt=system_prompt, user_prompt=prompt)\n",
    "            assistant_response = generator.generate(prompt)\n",
    "\n",
    "            # Compute the format score using the FormatScorer.\n",
    "            fmt_score = format_scorer.score(assistant_response)\n",
    "            \n",
    "            # If scoring fails, we set the joke score to None.\n",
    "            try:\n",
    "                llm_score = scorer.score_submission(prompt, assistant_response)\n",
    "            except Exception as e:\n",
    "                print(f\"Scoring failed for prompt: {prompt} with error: {e}\")\n",
    "                llm_score = None\n",
    "            \n",
    "            # Create a record with all the relevant information.\n",
    "            record = {\n",
    "                \"timestamp\": datetime.utcnow().isoformat(),\n",
    "                \"user_prompt\": prompt,\n",
    "                \"assistant_response\": assistant_response,\n",
    "                \"format_score\": fmt_score,\n",
    "                \"llm_score\": llm_score,\n",
    "                \"generator_model\": gen_model,\n",
    "                \"scorer_model\": scorer_model_name,\n",
    "                \"task_type\": task_type\n",
    "            }\n",
    "            \n",
    "            # Append the record as a new line in the output ljson file.\n",
    "            with open(OUTPUT_FILE, \"a\") as out_f:\n",
    "                out_f.write(json.dumps(record) + \"\\n\")\n",
    "            \n",
    "            # Optionally, print a status update.\n",
    "            print(f\"Processed prompt{i}: {prompt[:30]}... using model: {gen_model}. Score: {llm_score}\")\n",
    "\n",
    "# Run the generation-and-scoring function (one generation at a time).\n",
    "generate_and_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4d0b0e-b8a4-40ab-a434-1d55b161d414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
