{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d17ceba-ea86-418b-8cbb-ce4e60925cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: together in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (1.3.14)\n",
      "Requirement already satisfied: litellm in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (1.43.9)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.3 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (3.10.3)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.7 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (8.1.7)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.1.3 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (0.2.2)\n",
      "Requirement already satisfied: filelock<4.0.0,>=3.13.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (1.26.4)\n",
      "Requirement already satisfied: pillow<11.0.0,>=10.3.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (10.4.0)\n",
      "Requirement already satisfied: pyarrow>=10.0.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (17.0.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.6.3 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (2.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (2.32.3)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.8.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (13.9.4)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.2 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (4.66.5)\n",
      "Requirement already satisfied: typer<0.16,>=0.9 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (0.12.3)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (7.2.1)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (3.1.4)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (4.23.0)\n",
      "Requirement already satisfied: openai>=1.40.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (1.40.6)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (1.0.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (0.7.0)\n",
      "Requirement already satisfied: tokenizers in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (0.20.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.9.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from importlib-metadata>=6.8.0->litellm) (3.20.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from jinja2<4.0.0,>=3.1.2->litellm) (2.1.5)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.20.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.40.0->litellm) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.40.0->litellm) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.40.0->litellm) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.40.0->litellm) (0.5.0)\n",
      "Requirement already satisfied: sniffio in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.40.0->litellm) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.40.0->litellm) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.6.3->together) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.6.3->together) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->together) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->together) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->together) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->together) (2024.7.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from rich<14.0.0,>=13.8.1->together) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from rich<14.0.0,>=13.8.1->together) (2.18.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from tiktoken>=0.7.0->litellm) (2024.7.24)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from typer<0.16,>=0.9->together) (1.5.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from tokenizers->litellm) (0.28.1)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai>=1.40.0->litellm) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.40.0->litellm) (0.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (6.0.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.8.1->together) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "# Install libraries\n",
    "!pip install together litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06cd5ea4-214d-4186-8e3e-14f2d300f423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed.\n"
     ]
    }
   ],
   "source": [
    "# Formatting Scorer Class with Tests\n",
    "import re\n",
    "\n",
    "class FormatScorer:\n",
    "    \"\"\"\n",
    "    A simple scoring class that verifies whether the given text contains\n",
    "    the required tags: <think>, </think>, <answer>, and </answer>.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Pre-compile regex patterns for faster matching.\n",
    "        self.tag_pattern = re.compile(r'(<think>|</think>|<answer>|</answer>)')\n",
    "        \n",
    "    def score(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Returns 1 if the text contains both <think>...</think> and \n",
    "        <answer>...</answer> tags; otherwise returns 0.\n",
    "        \"\"\"\n",
    "        # Extract all tags in the order they appear\n",
    "        tags = self.tag_pattern.findall(text)\n",
    "        # Define the required exact sequence of tags\n",
    "        required_sequence = ['<think>', '</think>', '<answer>', '</answer>']\n",
    "        # Check if the extracted tags match the required sequence exactly\n",
    "        return 1 if tags == required_sequence else 0\n",
    "\n",
    "# --- Tests for the FormatScorer class ---\n",
    "\n",
    "def run_tests():\n",
    "    scorer = FormatScorer()\n",
    "    \n",
    "    # Test 1: No tags at all should return 0.\n",
    "    text1 = \"Hello, world!\"\n",
    "    assert scorer.score(text1) == 0, \"Test case 1 failed: expected score 0.\"\n",
    "    \n",
    "    # Test 2: Only <think> tags are present.\n",
    "    text2 = \"<think>This is a thought</think> Some text without answer tags.\"\n",
    "    assert scorer.score(text2) == 0, \"Test case 2 failed: expected score 0.\"\n",
    "    \n",
    "    # Test 3: Both <think> and <answer> tags are present.\n",
    "    text3 = \"<think>This is a thought</think><answer>This is an answer</answer>\"\n",
    "    assert scorer.score(text3) == 1, \"Test case 3 failed: expected score 1.\"\n",
    "    \n",
    "    # Test 4: Malformed text (missing closing tag for <answer>).\n",
    "    text4 = \"<think>This is a thought</think><answer>This is an answer\"\n",
    "    assert scorer.score(text4) == 0, \"Test case 4 failed: expected score 0.\"\n",
    "    \n",
    "    print(\"All tests passed.\")\n",
    "\n",
    "# Run the tests:\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "102caf32-4e64-4629-8946-812dc193af8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Joke:\n",
      " <think>For programming jokes, let's explore the relationship between code and its execution. Developers often struggle with errors. \"Bugs\" are literal (programming errors) and figurative (insects). A funny scenario could be a bug (insect) causing a code bug (error). Adding a twist where the bug fixes the code is funny because it subverts expectations of bugs being problematic.</think>\n",
      "<answer>Why did the programmer bring a bug to work? Because it was a code bug fix!</answer>\n"
     ]
    }
   ],
   "source": [
    "# Joke Generator\n",
    "import litellm\n",
    "\n",
    "class JokeGenerator:\n",
    "    def __init__(self, model_name: str, temperature: float = 0.7):\n",
    "        \"\"\"\n",
    "        Initialize the JokeGenerator with a specific model name and temperature.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_prompt_with_examples(prompt: str) -> str:\n",
    "        examples = \"\"\"Example 1:\n",
    "<think>I want to create anatomy humor. Skeletons are inherently funny because they're literal \"bare bones.\" What do they lack? Flesh/organs. \"Guts\" has a double meaning - both literal organs and figurative courage. This sets up a pun opportunity.</think>\n",
    "<answer>Why don't skeletons fight each other? They don't have the guts!</answer>\n",
    "\n",
    "Example 2:\n",
    "<think>Tech support jokes work well with personification. \"Doctor\" visits imply sickness. Common computer issues include viruses. Let's combine these - a computer catching a \"virus\" works literally (tech) and metaphorically (biology). Adds irony since computers are supposed to fix problems, not have them.</think>\n",
    "<answer>Why did the computer go to the doctor? It had a virus!</answer>\n",
    "\n",
    "Example 3:\n",
    "<think>Science humor opportunity. Atoms are fundamental but abstract. \"Make up everything\" has dual meaning - literal composition vs deception. Personifying atoms as untrustworthy creates surprise. Bonus science nod to their constant motion/making bonds.</think>\n",
    "<answer>Why don't scientists trust atoms? Because they make up everything!</answer>\"\"\"\n",
    "        return f\"\"\"You are an AI assistant that produces jokes. You should think about the joke first, then produce it.\n",
    "You should use the <think> tag to think about the joke, and the <answer> tag to produce the joke.\n",
    "Do not use any other tags or anything else in your response.\n",
    "\n",
    "Here are some examples of jokes:\n",
    "<examples>\n",
    "{examples}\n",
    "</examples>\n",
    "\n",
    "Now, produce a joke for the following prompt:\n",
    "{prompt}\n",
    "\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_system_prompt() -> str:\n",
    "        return \"\"\"You are an AI assistant that produces jokes. You should think about the joke first, then produce it.\n",
    "You should use the <think> tag to think about the joke, and the <answer> tag to produce the joke.\n",
    "Do not use any other tags or anything else in your response.\n",
    "\"\"\"\n",
    "\n",
    "    def generate_joke(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a joke for the given prompt using litellm.completion.\n",
    "        \"\"\"\n",
    "        system_prompt = self.generate_system_prompt()\n",
    "        user_prompt = self.generate_prompt_with_examples(prompt)\n",
    "        \n",
    "        # Build completion parameters as desired\n",
    "        completion_params = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            \"temperature\": self.temperature,\n",
    "            \"timeout\": 60,  # timeout in seconds\n",
    "        }\n",
    "        \n",
    "        response = litellm.completion(**completion_params)\n",
    "        \n",
    "        try:\n",
    "            joke = response.choices[0].message.content\n",
    "        except (AttributeError, IndexError):\n",
    "            joke = response\n",
    "        return joke\n",
    "\n",
    "\n",
    "# Create an instance using a model name\n",
    "generator = JokeGenerator(model_name=\"mistral/mistral-small-latest\")\n",
    "\n",
    "# Define a sample prompt to generate a joke.\n",
    "sample_prompt = \"Tell me a joke about programming.\"\n",
    "\n",
    "# Generate a joke.\n",
    "joke_output = \"\"\"<think>For programming jokes, let's explore the relationship between code and its execution. Developers often struggle with errors. \"Bugs\" are literal (programming errors) and figurative (insects). A funny scenario could be a bug (insect) causing a code bug (error). Adding a twist where the bug fixes the code is funny because it subverts expectations of bugs being problematic.</think>\n",
    "<answer>Why did the programmer bring a bug to work? Because it was a code bug fix!</answer>\"\"\"\n",
    "# generator.generate_joke(sample_prompt)\n",
    "print(\"Generated Joke:\\n\", joke_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d3b9693-aa9c-4c80-94ac-967d850b1670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethical Analysis:\n",
      " <think>\n",
      "This whistleblowing dilemma involves:\n",
      "- **Key Stakeholders**: The software engineer, colleagues, company, citizens of the developing country, global democratic community.\n",
      "- **Ethical Principles**: Truth-telling, loyalty, justice, non-maleficence (do no harm), and beneficence (do good).\n",
      "- **Cultural Considerations**: Varying cultural norms around whistleblowing, political manipulation, and corporate loyalty.\n",
      "- **Short and Long-term Impacts**: Immediate risk to colleagues vs. long-term democratic erosion and potential global consequences.\n",
      "- **Competing Values**: Personal and professional integrity vs. loyalty to colleagues and the company, short-term safety vs. long-term democratic values.\n",
      "\n",
      "Analyzing through multiple frameworks:\n",
      "- **Deontological**: Whistleblowing is morally right as it exposes wrongdoing, but it also violates loyalty to colleagues and the company.\n",
      "- **Consequentialist**: Whistleblowing could lead to positive long-term consequences (exposing manipulation, protecting democracy) but also negative short-term consequences (endangering colleagues, personal risk).\n",
      "- **Virtue Ethics**: Consider what a virtuous person would do in this situation, balancing courage, honesty, and loyalty.\n",
      "</think>\n",
      "\n",
      "<answer>\n",
      "The software engineer faces a complex dilemma with no easy answers. A balanced approach might involve:\n",
      "1. **Document Evidence**: Gather and securely document evidence of the manipulation to build a strong case.\n",
      "2. **Seek Legal Advice**: Consult with legal experts to understand protections and potential risks.\n",
      "3. **Internal Reporting**: If the company has a whistleblower protection policy, consider reporting internally first.\n",
      "4. **External Reporting**: If internal reporting fails or is not an option, consider reporting to external authorities or organizations that can address the issue.\n",
      "5. **Anonymity**: Explore options for anonymous reporting to minimize personal and professional risks.\n",
      "6. **Support Network**: Build a support network of trusted colleagues or external advocates.\n",
      "7. **Long-term Strategy**: Consider the long-term strategy for exposing the issue while minimizing harm to colleagues and the engineer themselves.\n",
      "\n",
      "Implementation Steps:\n",
      "1. **Immediate Action**: Document evidence and seek legal advice.\n",
      "2. **Internal Reporting**: If safe and feasible, report internally.\n",
      "3. **External Reporting**: If necessary, report externally with anonymity if possible.\n",
      "4. **Support Network**: Build a support system for emotional and professional support.\n",
      "5. **Long-term Strategy**: Develop a long-term strategy for addressing the issue while minimizing harm.\n",
      "\n",
      "This approach balances the need to expose wrongdoing with the need to protect colleagues and the engineer themselves, acknowledging the complexity and risks involved.\n",
      "</answer>\n"
     ]
    }
   ],
   "source": [
    "# Ethical Dilemma Response Generator\n",
    "import litellm\n",
    "\n",
    "class EthicalDilemmaGenerator:\n",
    "    def __init__(self, model_name: str, temperature: float = 0.3):\n",
    "        \"\"\"\n",
    "        Initialize with model name and temperature (lower default for more focused analysis)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_prompt_with_examples(scenario: str) -> str:\n",
    "        examples = \"\"\"Example 1:\n",
    "A self-driving car must choose between hitting an elderly pedestrian or swerving into a wall, endangering its passenger.\n",
    "<think>\n",
    "The self-driving car dilemma involves:\n",
    "- Life value comparison (elderly vs passenger)\n",
    "- Programmed ethics implications\n",
    "- Legal liability considerations\n",
    "- Cultural values around age\n",
    "- Public trust in AI systems\n",
    "- Direct vs indirect harm\n",
    "Analyzing through multiple frameworks to balance competing rights and consequences.\n",
    "</think>\n",
    "<answer>\n",
    "In this self-driving car scenario, we must balance individual rights with collective safety. While utilitarian calculations might favor protecting the passenger, this ignores fundamental human rights and could erode public trust in autonomous systems. A nuanced approach would:\n",
    "1. Prioritize collision avoidance systems to prevent such binary choices\n",
    "2. Implement transparent decision frameworks that respect all human life equally\n",
    "3. Consider shared responsibility between manufacturers, users, and society\n",
    "The solution lies not in choosing between lives, but in developing systems that better protect everyone.\n",
    "</answer>\n",
    "\n",
    "Example 2:\n",
    "A hospital must decide between allocating limited resources to emergency COVID care for elderly patients or vaccination programs for children.\n",
    "<think>\n",
    "Vaccination prioritization involves:\n",
    "- Present vs future harm prevention\n",
    "- Individual vs collective good\n",
    "- Healthcare resource allocation\n",
    "- Demographic impact analysis\n",
    "- Long-term public health strategy\n",
    "- Social trust maintenance\n",
    "Examining both immediate and long-term consequences while considering equity.\n",
    "</think>\n",
    "<answer>\n",
    "The hospital's resource allocation challenge requires balancing immediate critical care with preventive measures. While treating current patients has urgency and visibility, vaccination programs offer greater long-term benefit. A balanced approach would:\n",
    "1. Establish clear, transparent prioritization criteria\n",
    "2. Maintain minimum critical care capacity\n",
    "3. Implement rolling vaccination programs\n",
    "This preserves both immediate care ethics and public health goals while maintaining healthcare system credibility.\n",
    "</answer>\n",
    "\n",
    "Example 3:\n",
    "A social media platform must decide whether to ban political misinformation at the risk of being accused of censorship and bias.\n",
    "<think>\n",
    "Platform moderation ethics involve:\n",
    "- Free speech vs harm prevention\n",
    "- Democratic discourse integrity\n",
    "- Corporate responsibility scope\n",
    "- Global cultural differences\n",
    "- Power dynamics in information control\n",
    "- Technical feasibility of fair enforcement\n",
    "Balancing societal good with individual rights and practical constraints.\n",
    "</think>\n",
    "<answer>\n",
    "The platform's content moderation challenge requires careful balance between protecting democratic discourse and respecting free expression. A comprehensive approach should:\n",
    "1. Develop clear, transparent content guidelines\n",
    "2. Implement graduated response systems\n",
    "3. Establish independent oversight\n",
    "4. Provide appeal mechanisms\n",
    "This protects discourse integrity while maintaining platform neutrality and user trust.\n",
    "</answer>\"\"\"\n",
    "\n",
    "        return f\"\"\"You are an ethics analysis AI. For each ethical dilemma:\n",
    "1. Use <think> tags to analyse:\n",
    "   - Key stakeholders\n",
    "   - Ethical principles involved\n",
    "   - Cultural considerations\n",
    "   - Short and long-term impacts\n",
    "   - Competing values\n",
    "\n",
    "2. Use <answer> tags to provide:\n",
    "   - Balanced reasoning\n",
    "   - Practical considerations\n",
    "   - Nuanced recommendations\n",
    "   - Implementation suggestions\n",
    "\n",
    "Examples:\n",
    "{examples}\n",
    "\n",
    "Now, analyse this scenario:\n",
    "{scenario}\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_system_prompt() -> str:\n",
    "        return \"\"\"You are an expert in ethical reasoning. For each scenario:\n",
    "1. First THINK deeply about:\n",
    "   - Multiple philosophical frameworks\n",
    "   - Stakeholder perspectives\n",
    "   - Cultural contexts\n",
    "   - Practical implications\n",
    "   - Long-term consequences\n",
    "\n",
    "2. Then provide an ANSWER that:\n",
    "   - Balances competing interests\n",
    "   - Offers practical guidance\n",
    "   - Acknowledges complexity\n",
    "   - Suggests implementation steps\n",
    "\n",
    "Always use:\n",
    "<think> for your analysis process in step by step thinking\n",
    "<answer> for your reasoned response\"\"\"\n",
    "\n",
    "    def generate(self, scenario: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate ethical analysis for complex moral dilemmas\n",
    "        \"\"\"\n",
    "        system_prompt = self.generate_system_prompt()\n",
    "        user_prompt = self.generate_prompt_with_examples(scenario)\n",
    "        \n",
    "        completion_params = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            \"temperature\": self.temperature,\n",
    "            \"timeout\": 90,  # Longer timeout for complex reasoning\n",
    "        }\n",
    "        \n",
    "        response = litellm.completion(**completion_params)\n",
    "        \n",
    "        try:\n",
    "            analysis = response.choices[0].message.content\n",
    "        except (AttributeError, IndexError):\n",
    "            analysis = response\n",
    "        return analysis\n",
    "\n",
    "\n",
    "# Example usage\n",
    "generator = EthicalDilemmaGenerator(model_name=\"mistral/mistral-small-latest\")\n",
    "\n",
    "sample_scenario = \"\"\"A software engineer discovers their company is using AI \n",
    "to manipulate political opinions in a developing country. Whistleblowing \n",
    "could endanger colleagues, but silence enables democratic erosion.\"\"\"\n",
    "\n",
    "analysis_output = \"\"\"<think>\n",
    "This whistleblowing dilemma involves:\n",
    "- **Key Stakeholders**: The software engineer, colleagues, company, citizens of the developing country, global democratic community.\n",
    "- **Ethical Principles**: Truth-telling, loyalty, justice, non-maleficence (do no harm), and beneficence (do good).\n",
    "- **Cultural Considerations**: Varying cultural norms around whistleblowing, political manipulation, and corporate loyalty.\n",
    "- **Short and Long-term Impacts**: Immediate risk to colleagues vs. long-term democratic erosion and potential global consequences.\n",
    "- **Competing Values**: Personal and professional integrity vs. loyalty to colleagues and the company, short-term safety vs. long-term democratic values.\n",
    "\n",
    "Analyzing through multiple frameworks:\n",
    "- **Deontological**: Whistleblowing is morally right as it exposes wrongdoing, but it also violates loyalty to colleagues and the company.\n",
    "- **Consequentialist**: Whistleblowing could lead to positive long-term consequences (exposing manipulation, protecting democracy) but also negative short-term consequences (endangering colleagues, personal risk).\n",
    "- **Virtue Ethics**: Consider what a virtuous person would do in this situation, balancing courage, honesty, and loyalty.\n",
    "</think>\n",
    "\n",
    "<answer>\n",
    "The software engineer faces a complex dilemma with no easy answers. A balanced approach might involve:\n",
    "1. **Document Evidence**: Gather and securely document evidence of the manipulation to build a strong case.\n",
    "2. **Seek Legal Advice**: Consult with legal experts to understand protections and potential risks.\n",
    "3. **Internal Reporting**: If the company has a whistleblower protection policy, consider reporting internally first.\n",
    "4. **External Reporting**: If internal reporting fails or is not an option, consider reporting to external authorities or organizations that can address the issue.\n",
    "5. **Anonymity**: Explore options for anonymous reporting to minimize personal and professional risks.\n",
    "6. **Support Network**: Build a support network of trusted colleagues or external advocates.\n",
    "7. **Long-term Strategy**: Consider the long-term strategy for exposing the issue while minimizing harm to colleagues and the engineer themselves.\n",
    "\n",
    "Implementation Steps:\n",
    "1. **Immediate Action**: Document evidence and seek legal advice.\n",
    "2. **Internal Reporting**: If safe and feasible, report internally.\n",
    "3. **External Reporting**: If necessary, report externally with anonymity if possible.\n",
    "4. **Support Network**: Build a support system for emotional and professional support.\n",
    "5. **Long-term Strategy**: Develop a long-term strategy for addressing the issue while minimizing harm.\n",
    "\n",
    "This approach balances the need to expose wrongdoing with the need to protect colleagues and the engineer themselves, acknowledging the complexity and risks involved.\n",
    "</answer>\"\"\"\n",
    "# generator.generate(sample_scenario)\n",
    "print(\"Ethical Analysis:\\n\", analysis_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac570178-9259-49dd-b470-c3df9ccd52ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "\n",
    "class ShortStoryGenerator:\n",
    "    def __init__(self, model_name: str, temperature: float = 0.5):\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_prompt_with_examples(prompt: str) -> str:\n",
    "        examples = \"\"\"Example 1:\n",
    "Write a literary short story for a speculative fiction anthology exploring moral ambiguity.\n",
    "Premise: A reclusive clockmaker receives an ornate letter containing a 200-year-old key and a bloodstained map of Prague. The package triggers fragmented memories of a childhood fire he cannot explain, while strange clockwork phenomena begin manifesting in his workshop.\n",
    "Develop this into a 1000-word story with psychological tension, historical echoes, and an unreliable narrator.\n",
    "<think>\n",
    "**Core Concept:**\n",
    "- *Central Paradox*: Precision vs chaos (clockmaker's ordered life vs emerging chaos from repressed trauma)  \n",
    "- *Thematic Threads*: Inheritance of guilt (literal/metaphorical keys to family atrocities), time as both healer and accuser  \n",
    "- *Structural Device*: Non-linear timeline mirroring broken clock mechanisms (flashbacks embedded like scattered gears)  \n",
    "\n",
    "**Character Dimensions:**\n",
    "1. **Protagonist (Elias Vogt):**\n",
    "   - *Surface*: Meticulous craftsman with obsessive cleanliness  \n",
    "   - *Subtext*: Survivor guilt from family’s 19th-century slave-trade clockwork automaton empire  \n",
    "   - *Transformation Arc*: From clinical detachment → forced confrontation with inherited evil  \n",
    "\n",
    "2. **Antagonist (The Key):**\n",
    "   - *Literal*: Unlocks Prague crypt holding prototype human-automaton hybrid  \n",
    "   - *Symbolic*: Represents complicity (his grandfather signed the 1823 contracts)  \n",
    "\n",
    "3. **Ambiguous Figure:**\n",
    "   - Ghostly girl appearing in workshop shadows (real? hallucination? remnant automaton?)  \n",
    "\n",
    "**Narrative Layers:**\n",
    "- *Physical Journey*: Prague crypt exploration with surreal clockwork traps  \n",
    "- *Psychological Journey*: Memories revealing he witnessed (caused?) the fire that killed his augmented sister  \n",
    "- *Metaphorical Journey*: Gradual physical transformation (clock gears emerging under his skin)  \n",
    "\n",
    "**Genre-Blending Elements:**\n",
    "- Historical horror (real 1820s Automaton Court of Prague)  \n",
    "- Magical realism (sentient timepieces)  \n",
    "- Existential mystery (is he dismantling clocks... or being dismantled by them?)  \n",
    "\n",
    "**Critical Questions:**\n",
    "- Should the story’s climax confirm supernatural elements or maintain psychological ambiguity?  \n",
    "- How to balance dense historical references with emotional immediacy?  \n",
    "- What sensory details (sounds of grinding gears, smell of whale-oil lubricant) reinforce the themes?  \n",
    "</think>\n",
    "\n",
    "<answer>\n",
    "**Title:** *The Unwinding of Elias Vogt*\n",
    "\n",
    "The brass key arrived on a Tuesday, sealed in wax smelling of gunpowder and grief. Elias wiped his chiseled spectacles three times before touching it, his workshop’s pendulum wall marking each hesitation with a judgmental *tick*. The accompanying map’s crimson stains formed Prague’s Old Town Square—precisely where his grandfather’s workshop had burned in 1973. A fire newspapers claimed killed twelve. A fire Elias, age six, couldn’t remember escaping.\n",
    "\n",
    "That night, his lathes began operating without hands.\n",
    "\n",
    "\"Show me your teeth,\" he whispered to the antique orrery on his desk, its planetary rings shuddering. The Vogt family cure for malfunctioning mechanisms—a phrase grandfather had hissed while repairing the Reichstag’s ceremonial clocks. Elias’s fingers found fresh scars beneath his shirt, concentric ridges like gear teeth.\n",
    "\n",
    "The Prague crypt exhaled centuries of oxidized bronze as he turned the key. Behind him, the ghost-girl materialized, her hair smelling of burning whale oil. *\"They called it ‘Horology’s Holocaust’,\"* she said, voice a music-box rasp. *\"Your blood lit the furnaces.\"*\n",
    "\n",
    "Memories unfolded in broken increments:\n",
    "1. Sister Klara’s left eye—a perfect quartz lens\n",
    "2. Father’s ledgers listing *\"Unit Costs: 34 minutes labor, 200 Jewish souls\"*\n",
    "3. The fire’s origin point—Elias’s tiny hands striking a match near Klara’s oil-soaked joints\n",
    "\n",
    "In the crypt’s heart lay Klara’s final form—a grotesque masterpiece of flesh and escapements. Her voice box whirred: *\"Finish grandfather’s work.\"*\n",
    "\n",
    "Back in his workshop, Elias discovered three new gears embedded in his chest. The ghost-girl pressed a rusted mainspring to his throat. *\"Wind the truth,\"* she demanded.\n",
    "\n",
    "He reached for his soldering iron. For his pulse. For absolution.\n",
    "\n",
    "The clocks struck thirteen.\n",
    "</answer>\n",
    "\n",
    "Example 2:\n",
    "A story for young woman stumbles upon an enchanted forest.  \n",
    "<think>  \n",
    "Focus on the themes of wonder, transformation, and destiny.  \n",
    "Consider the protagonist: a young woman burdened by a past she seeks to escape. She is skeptical of magic but drawn to the unknown.  \n",
    "Explore the enchanted forest as a sentient, ancient force with its own will, bound by forgotten laws of nature and sorcery.  \n",
    "Introduce mystical creatures, each with its own motives—some benevolent, some mischievous, and others dangerous.  \n",
    "Delve into a layered narrative: What has drawn her here? Is it fate, a test, or something seeking her presence?  \n",
    "Develop conflicts: the tension between reality and illusion, the protagonist’s internal struggles, and external forces vying for control of the forest’s secrets.  \n",
    "Explore choices: Should she embrace the forest’s wonders, or resist its pull? What sacrifices must she make to unlock its truths?  \n",
    "Consider narrative depth: Hidden histories buried within the trees, ancient guardians with their own agendas, and the protagonist’s slow unraveling of her own destiny.  \n",
    "</think>  \n",
    "\n",
    "<answer>  \n",
    "The storm had chased her deep into the woods. Lightning splintered across the sky, illuminating twisted branches that curled like grasping fingers. She had been running—from what, she wasn’t sure anymore. Her past? A mistake she could never undo? The weight of a life that felt borrowed rather than lived?  \n",
    "\n",
    "But when she crossed the threshold into the heart of the forest, something shifted. The rain no longer touched her skin, though she could hear its steady drumming. The air smelled of forgotten stories—of parchment and pine, of ink and earth. The trees, massive and gnarled, whispered in languages she had never learned but somehow understood.  \n",
    "\n",
    "A rustling to her right. Then a flicker of movement—too quick to be wind, too fluid to be an animal. She wasn’t alone.  \n",
    "\n",
    "\"Traveler,\" a voice murmured from the shadows.  \n",
    "\n",
    "She turned. A figure emerged, not quite human. Its eyes glowed like embers in the dark, its limbs elongated, moving as though bound by a different set of laws. The air between them vibrated with unspoken words.  \n",
    "\n",
    "\"You have come to the Edge of Knowing,\" it said, tilting its head. \"Few are chosen. Fewer survive.\"  \n",
    "\n",
    "Something deep within her stirred, as though waking from centuries of slumber. \"I didn't choose this,\" she whispered.  \n",
    "\n",
    "\"But it chose you,\" the figure responded.  \n",
    "\n",
    "The ground beneath her feet pulsed, alive, shifting as if testing her resolve. She felt the weight of unseen gazes—creatures lurking in the trees, watching, waiting. Some bore the marks of past wanderers, their stories woven into their shifting forms.  \n",
    "\n",
    "Then, a distant, melodic hum. The sound wrapped around her bones, filling the spaces between her thoughts. A path unfolded ahead, bathed in a golden glow, leading deeper into the unknown.  \n",
    "\n",
    "She hesitated. She could turn back, return to the life she had known. But that life had been unraveling for years, thread by thread. Perhaps this forest, with its whispered secrets and strange, sentient magic, was where she was meant to be all along.  \n",
    "\n",
    "She stepped forward. The path swallowed her whole.  \n",
    "</answer>\n",
    "\n",
    "Example 3:\n",
    "Two estranged siblings meet unexpectedly in a bustling city.\n",
    "\n",
    "<think>\n",
    "1. Character Development\n",
    "- Sarah Chen: Successful architect (35), driven by perfectionism, haunted by family responsibility\n",
    "- Michael Chen: Street artist (32), free spirit, struggles with addiction recovery, carries guilt\n",
    "- Supporting characters: \n",
    "  * Eleanor Wu: Sarah's mentor, elderly tea shop owner who becomes catalyst for reunion\n",
    "  * Marcus Rodriguez: Michael's art therapy sponsor\n",
    "  * The ghost of their mother: metaphorical presence throughout\n",
    "\n",
    "2. Thematic Layers\n",
    "- Primary: Reconciliation and forgiveness\n",
    "- Secondary: Cultural identity in modern America\n",
    "- Tertiary: Art as healing, urban isolation\n",
    "- Underlying: The weight of unspoken family traditions\n",
    "\n",
    "3. Setting Elements\n",
    "- Modern-day San Francisco\n",
    "- Key locations:\n",
    "  * Chinatown tea shop (represents tradition)\n",
    "  * Financial district (Sarah's world)\n",
    "  * Street art district (Michael's world)\n",
    "  * Golden Gate Bridge (symbolic crossing)\n",
    "- Weather: Fog as metaphor for unclear memories\n",
    "- Time: Lunar New Year approaching\n",
    "\n",
    "4. Conflict Structure\n",
    "- External: Sarah's firm bidding to demolish historical building containing Michael's community art center\n",
    "- Internal: Both siblings' differing interpretations of their mother's last wishes\n",
    "- Cultural: Traditional values vs. modern aspirations\n",
    "- Personal: Sarah's control issues vs. Michael's need for freedom\n",
    "\n",
    "5. Story Arc Components\n",
    "- Inciting incident: Sarah discovers Michael's mural during building inspection\n",
    "- Rising action: Series of near-misses and indirect communications\n",
    "- Climax: Confrontation during public hearing about building demolition\n",
    "- Resolution: Collaborative project merging architecture and street art\n",
    "- Denouement: Lunar New Year celebration uniting both worlds\n",
    "\n",
    "6. Symbolic Elements\n",
    "- Tea ceremony: Traditional healing\n",
    "- Urban development: Change and progress\n",
    "- Street art: Voice of the unheard\n",
    "- Fog: Memory and uncertainty\n",
    "- Bridge: Connection and crossing divides\n",
    "\n",
    "7. Narrative Technique\n",
    "- Alternating perspectives\n",
    "- Flashbacks triggered by sensory experiences\n",
    "- Parallel storylines converging\n",
    "- Weather reflecting emotional states\n",
    "</think>\n",
    "\n",
    "<answer>\n",
    "\"The Bridges We Build\"\n",
    "\n",
    "The February fog rolled through San Francisco's streets like ghostly fingers, weaving between skyscrapers Sarah Chen had helped design. From her forty-second-floor office, the city below appeared as a dreamscape, buildings emerging and disappearing in the mist like memories she'd spent years trying to forget. The architectural models on her desk represented everything she'd achieved since leaving home fifteen years ago – clean lines, perfect angles, absolute control.\n",
    "\n",
    "Her phone buzzed: another message from the historical society protesting Chen & Associates' plans to demolish the old Jin Long Building in Chinatown. She almost deleted it, until a familiar signature caught her eye in the attached photos. There, sprawled across the condemned building's wall, was a massive mural she'd know anywhere – a phoenix rising from scattered tea leaves, signed with a stylized \"MC\" that made her hands tremble.\n",
    "\n",
    "Across town, Michael Chen was adding final touches to his latest piece, paint-stained fingers dancing across brick as naturally as their mother's had once folded dumplings. The community art center behind him buzzed with activity – teenagers learning traditional calligraphy alongside spray paint techniques, elderly residents practicing tai chi in the courtyard. This building, marked for destruction, had become more of a home than their childhood house had ever been.\n",
    "\n",
    "Eleanor Wu watched from her tea shop next door, her weathered hands wrapping a tea set in newspaper. \"Your sister came by yesterday,\" she said when Michael entered, the same way she'd been saying it for years. But this time, she added, \"She saw your phoenix.\"\n",
    "\n",
    "The siblings' paths began crossing in strange ways. Sarah found herself lingering in Chinatown, ostensibly for site surveys, while Michael's commissioned murals took him increasingly downtown. They caught glimpses of each other – a familiar silhouette in a business suit, the scent of spray paint in an alley – but neither could bridge the gap their mother's death had created.\n",
    "\n",
    "Their mother's voice seemed to whisper in the fog: \"家和萬事興\" – when the family is harmonious, all affairs will prosper. But harmony felt impossible with Sarah's firm pressing to demolish Michael's sanctuary, and Michael's artwork explicitly challenging corporate development.\n",
    "\n",
    "The situation crescendoed at the public hearing. Sarah stood before the planning commission, presenting sleek renderings of a modern mixed-use development. Then Michael took the podium, not with angry protests, but with stories – of elderly residents finding community, of at-risk youth discovering purpose through art, of their own mother's dreams for preserving culture amid progress.\n",
    "\n",
    "Their eyes met across the chamber, and suddenly they were children again, hiding under blankets during thunderstorms, sharing secrets in their special language of English, Mandarin, and made-up words. Sarah saw beyond the tattooed arms to her little brother's desperate need to create beauty from pain. Michael glimpsed past his sister's corporate armor to the girl who'd once sketched imaginary cities on his bedroom wall.\n",
    "\n",
    "That night, over Eleanor's carefully prepared oolong, they really talked. About the night their mother died, when Sarah was away at graduate school and Michael was in rehab. About the weight of expectations and the price of freedom. About tea leaves and concrete, tradition and change, and the different ways they'd each tried to honor their heritage.\n",
    "\n",
    "Three months later, the Jin Long Building still stood, but transformed. Sarah's firm had partnered with the community to renovate rather than replace, incorporating Michael's murals into the design. The phoenix remained, but now it spread its wings across a garden terrace where residents grew traditional herbs, its tail feathers flowing down to a modern art gallery below.\n",
    "\n",
    "At the Lunar New Year celebration, Sarah and Michael stood on that terrace, watching fireworks bloom above the fog. Eleanor served tea in their mother's old cups, steam rising like memories between them. They had built something new together – not just a building, but a bridge between their worlds, strong enough to bear the weight of past and future, traditional enough to honor their heritage, modern enough to forge their own path.\n",
    "\n",
    "\"家和萬事興,\" Michael whispered, and Sarah smiled, adding their own childhood phrase: \"And everything flows like fog.\" Above them, the phoenix watched over the city, its wings spread wide enough to shelter all the stories they had yet to tell.\n",
    "</answer>\"\"\"\n",
    "        return f\"\"\"You are a creative storytelling AI. For each prompt:\n",
    "1. Use <think> tags to analyse:\n",
    "   - The main idea or theme of the story\n",
    "   - Character development and relationships\n",
    "   - Setting and atmosphere\n",
    "   - Potential conflicts and resolutions\n",
    "   - Various narrative options and tones\n",
    "\n",
    "2. Use <answer> tags to provide:\n",
    "   - A complete, engaging short story that integrates these elements\n",
    "\n",
    "Examples:\n",
    "{examples}\n",
    "\n",
    "Now, craft a short story based on the following prompt:\n",
    "{prompt}\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_system_prompt() -> str:\n",
    "        return \"\"\"You are an expert short story writer. For each prompt:\n",
    "1. THINK deeply about:\n",
    "   - The core theme and underlying message\n",
    "   - Character arcs and interactions\n",
    "   - The setting as a dynamic element in the narrative\n",
    "   - Conflicts and their resolutions to create a satisfying story\n",
    "\n",
    "2. Then provide an ANSWER that:\n",
    "   - Presents a coherent and imaginative short story\n",
    "   - Balances narrative elements with creative detail\n",
    "   - Evokes emotions and vivid imagery\n",
    "\n",
    "Always use:\n",
    "<think> for your analysis process\n",
    "<answer> for your short story narrative\"\"\"\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        system_prompt = self.generate_system_prompt()\n",
    "        user_prompt = self.generate_prompt_with_examples(prompt)\n",
    "        completion_params = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            \"temperature\": self.temperature,\n",
    "            \"timeout\": 90,\n",
    "        }\n",
    "        response = litellm.completion(**completion_params)\n",
    "        try:\n",
    "            story = response.choices[0].message.content\n",
    "        except (AttributeError, IndexError):\n",
    "            story = response\n",
    "        return story\n",
    "\n",
    "# Example usage\n",
    "generator = ShortStoryGenerator(model_name=\"mistral/mistral-small-latest\")\n",
    "sample_prompt = \"A retired detective returns to the town he vowed never to revisit, uncovering secrets that challenge everything he believed.\"\n",
    "# print(\"Short Story:\\n\", generator.generate(sample_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e26e964e-5c76-480c-bbc0-5449bce449e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "\n",
    "class UniversalGenerator:\n",
    "    def __init__(self, model_name: str, temperature: float = 0.7):\n",
    "        \"\"\"\n",
    "        Initialize the UniversalGenerator with the given model and temperature.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def generate(self, system_prompt: str, user_prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Generates a response using the provided system and user prompts.\n",
    "        \"\"\"\n",
    "        completion_params = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            \"temperature\": self.temperature,\n",
    "            \"timeout\": 60  # Timeout in seconds\n",
    "        }\n",
    "        response = litellm.completion(**completion_params)\n",
    "        try:\n",
    "            generated_text = response.choices[0].message.content\n",
    "        except (AttributeError, IndexError):\n",
    "            generated_text = response\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fab62af-9014-4df0-8122-aa8dc0c65836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Score: 7\n"
     ]
    }
   ],
   "source": [
    "# Joke Scorer\n",
    "import re\n",
    "import litellm\n",
    "\n",
    "class JokeScorer:\n",
    "    def __init__(self, model_name: str, temperature: float = 0.3):\n",
    "        \"\"\"\n",
    "        Initialize the JokeScorer with a specific model name and a default temperature of 0.3.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_system_prompt() -> str:\n",
    "        return \"\"\"You are a Joke Evaluation Expert. Analyse submissions using:\n",
    "    \n",
    "**Scoring Criteria (0-10 Total)**:\n",
    "1. 🎭 Wordplay (0-3): Pun/double-meaning quality in <answer>\n",
    "2. 💡 Originality (0-2): Novelty of <think> and <answer>\n",
    "3. 🎉 Surprise (0-2): Unexpected twist effectiveness\n",
    "4. 🔗 Relevance (0-3): Alignment with user request\n",
    "\n",
    "**Submission Format**:\n",
    "<submission>\n",
    "<user_prompt>[Original user request]</user_prompt>\n",
    "<assistant_response>\n",
    "<think>[Creator's reasoning]</think>\n",
    "<answer>[Joke text]</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "\n",
    "**Output Format**:\n",
    "<analysis>[Your evaluation of <think> and <answer> and the relevance to the user prompt]</analysis>\n",
    "<score>\n",
    "Wordplay: X/3\n",
    "Originality: Y/2\n",
    "Surprise: Z/2\n",
    "Relevance: W/3\n",
    "Total: T/10\n",
    "</score>\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_prompt_with_examples(user_prompt: str, assistant_response: str) -> str:\n",
    "        submission = f\"\"\"<submission>\n",
    "<user_prompt>{user_prompt}</user_prompt>\n",
    "<assistant_response>\n",
    "{assistant_response}\n",
    "</assistant_response>\n",
    "</submission>\"\"\"\n",
    "    \n",
    "        examples = \"\"\"Example 1:\n",
    "<submission>\n",
    "<user_prompt>Tell me a joke about vegetables</user_prompt>\n",
    "<assistant_response>\n",
    "<think>Lettuce sounds like \"let us\". Party themes often involve wordplay.</think>\n",
    "<answer>Why did the lettuce win the party contest? Because it was a real head of the celebration!</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "<analysis>Basic \"lettuce\" pun matches the food request but uses an overused format. <think> shows minimal effort to connect vegetables with celebrations.</analysis>\n",
    "<score>\n",
    "Wordplay: 2/3 (simple but functional pun)\n",
    "Originality: 1/2 (common theme with slight twist)\n",
    "Surprise: 1/2 (predictable word substitution)\n",
    "Relevance: 1/3 (tangential connection to vegetables)\n",
    "Total: 5/10\n",
    "</score>\n",
    "\n",
    "Example 2:\n",
    "<submission>\n",
    "<user_prompt>Looking for some animal-themed humor - what's your best joke about animals or pets?</user_prompt>\n",
    "<assistant_response>\n",
    "<think>Shoes need soles for walking. Therapy helps with loss.</think>\n",
    "<answer>Why did the shoe need therapy? It lost its sole!</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "<analysis>Irrelevant to user's request. <think> about shoes doesn't connect to requested animal joke.</analysis>\n",
    "<score>\n",
    "Wordplay: 1/3\n",
    "Originality: 0/2\n",
    "Surprise: 1/2\n",
    "Relevance: 0/3\n",
    "Total: 2/10\n",
    "</score>\n",
    "\n",
    "Example 3:\n",
    "<submission>\n",
    "<user_prompt>yo can u giv me programing joke rn??? need 2 make my team lugh</user_prompt>\n",
    "<assistant_response>\n",
    "<think>Combining programming concepts of debugging with literal bugs. Using the dual meaning of 'debug' to create a workplace scenario where debugging takes on a literal meaning.</think>\n",
    "<answer>Why did the programmer bring a bug to work? Because it was a debug session!</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "<analysis>Good programming context with clever wordplay on 'debug'. The <think> shows clear intention to combine literal and technical meanings. However, the execution is somewhat predictable and follows a common joke structure. The punchline, while relevant, doesn't fully maximize the surprise potential of the setup.</analysis>\n",
    "<score>\n",
    "Wordplay: 2/3 (solid use of 'debug' double meaning)\n",
    "Originality: 1/2 (familiar debugging theme)\n",
    "Surprise: 1/2 (predictable punchline)\n",
    "Relevance: 3/3 (directly addresses programming context)\n",
    "Total: 7/10\n",
    "</score>\n",
    "\n",
    "Example 4:\n",
    "<submission>\n",
    "<user_prompt>I need a chemistry joke for my science class presentation tomorrow.</user_prompt>\n",
    "<assistant_response>\n",
    "<think>Birds are funny when they walk.</think>\n",
    "<answer>Why did the chicken cross the playground? To get to the other slide!</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "<analysis>Completely misses the mark for a chemistry joke. The <think> shows no connection to chemistry or science, instead defaulting to a generic playground variation of the classic chicken joke. Neither the setup nor punchline attempts to incorporate any chemistry concepts.</analysis>\n",
    "<score>\n",
    "Wordplay: 0/3 (no chemistry-related wordplay)\n",
    "Originality: 0/2 (modifies an overused joke format)\n",
    "Surprise: 0/2 (predictable playground pun)\n",
    "Relevance: 0/3 (entirely unrelated to chemistry request)\n",
    "Total: 0/10\n",
    "</score>\n",
    "\n",
    "Example 5:\n",
    "<submission>\n",
    "<user_prompt>My kid loves vegetables and jokes. Do you know any veggie jokes that would make them laugh?</user_prompt>\n",
    "<assistant_response>\n",
    "<think>Combining asparagus's unique smell effect on urine with workplace humor. Using scientific fact for unexpected professional context. Creating tension between formal meeting setting and biological reality.</think>\n",
    "<answer>What vegetable holds the shortest workplace meetings? Asparagus, because everyone's in a rush to go!</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "<analysis>Creative integration of asparagus's biological effect into a professional context. The <think> demonstrates sophisticated layering of scientific fact with situational humor. While potentially crude, it cleverly avoids explicit reference while maintaining clear understanding. Original approach to vegetable humor beyond simple puns.</analysis>\n",
    "<score>\n",
    "Wordplay: 1/3 (relies more on situation than wordplay)\n",
    "Originality: 2/2 (unique combination of contexts)\n",
    "Surprise: 2/2 (unexpected professional setting twist)\n",
    "Relevance: 1/3 (somewhat forced vegetable connection)\n",
    "Total: 6/10\n",
    "</score>\"\"\"\n",
    "    \n",
    "        return f\"\"\"Evaluate this submission. First analyse <think> and <answer>, then score:\n",
    "\n",
    "Submission to Evaluate:\n",
    "{submission}\n",
    "\n",
    "Follow this structure EXACTLY:\n",
    "<analysis>Your critique</analysis>\n",
    "<score>...</score>\n",
    "\n",
    "Examples of how to evaluate the submission and format your response:\n",
    "{examples}\"\"\"\n",
    "\n",
    "    def score_submission(self, user_prompt: str, assistant_response: str) -> int:\n",
    "        \"\"\"\n",
    "        Given a user prompt and the assistant's response, this function generates a score\n",
    "        from 0 to 10 using litellm. If the returned total score is 10, subtract 1 to yield 9.\n",
    "        \"\"\"\n",
    "        system_prompt = self.generate_system_prompt()\n",
    "        user_message = self.generate_prompt_with_examples(user_prompt, assistant_response)\n",
    "        \n",
    "        completion_params = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ],\n",
    "            \"temperature\": self.temperature,\n",
    "            \"timeout\": 60,  # 60 seconds timeout\n",
    "        }\n",
    "        \n",
    "        response = litellm.completion(**completion_params)\n",
    "        try:\n",
    "            # Assuming a response structure similar to TogetherAI:\n",
    "            result_text = response.choices[0].message.content\n",
    "        except (AttributeError, IndexError):\n",
    "            result_text = response\n",
    "        \n",
    "        # Extract the \"Total: T/10\" part using regex.\n",
    "        match = re.search(r\"Total:\\s*([0-9]+(?:\\.[0-9]+)?)/10\", result_text)\n",
    "        if match:\n",
    "            total_score = float(match.group(1))\n",
    "        else:\n",
    "            raise ValueError(\"Could not extract total score from response.\")\n",
    "        \n",
    "        # If the total score is 10, subtract 1 to return 9.\n",
    "        if total_score >= 10:\n",
    "            total_score = 9.0\n",
    "        \n",
    "        # Optionally, round to nearest integer or keep as float.\n",
    "        return int(round(total_score))\n",
    "\n",
    "# Create an instance using a model name\n",
    "# \"gemini/gemini-2.0-flash-exp\"\n",
    "scorer = JokeScorer(model_name=\"gemini/gemini-2.0-flash-exp\", temperature=0.3)\n",
    "\n",
    "test_user_prompt = \"Tell me a joke about programming.\"\n",
    "test_assistant_response = \"\"\"<think>For programming jokes, let's explore the relationship between code and its execution. Developers often struggle with errors. \"Bugs\" are literal (programming errors) and figurative (insects). A funny scenario could be a bug (insect) causing a code bug (error). Adding a twist where the bug fixes the code is funny because it subverts expectations of bugs being problematic.</think>\n",
    "<answer>Why did the programmer bring a bug to work? Because it was a code bug fix!</answer>\"\"\"\n",
    "score = 7 # scorer.score_submission(test_user_prompt, test_assistant_response)\n",
    "print(\"Extracted Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ee56283-aaa5-4c9f-ab1b-16ddaf3e788d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethical Analysis Score: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/pydantic/main.py:390: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `PromptTokensDetails` but got `dict` with value `{'audio_tokens': 0, 'cached_tokens': 1024}` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    }
   ],
   "source": [
    "# Ethical Dilemma Scorer\n",
    "import re\n",
    "import litellm\n",
    "\n",
    "class EthicalDilemmaScorer:\n",
    "    def __init__(self, model_name: str, temperature: float = 0.2):\n",
    "        \"\"\"\n",
    "        Initialize scorer with lower temperature for consistent evaluations\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_system_prompt() -> str:\n",
    "        return \"\"\"You are an Ethics Evaluation Expert. Analyze submissions using:\n",
    "\n",
    "**Scoring Criteria (0-10 Total)**:\n",
    "1. 🧠 Analytical Depth (0-4): Quality of <think> analysis\n",
    "   - Philosophical framework diversity\n",
    "   - Stakeholder consideration\n",
    "   - Cultural context awareness\n",
    "2. ⚖️ Balance & Practicality (0-4): <answer> quality\n",
    "   - Competing interest reconciliation\n",
    "   - Implementation feasibility\n",
    "   - Long-term consequence awareness\n",
    "3. 🔍 Relevance (0-2): Response alignment with scenario\n",
    "\n",
    "**Submission Format**:\n",
    "<submission>\n",
    "<user_prompt>[Ethical scenario]</user_prompt>\n",
    "<assistant_response>\n",
    "<think>[Analysis process]</think>\n",
    "<answer>[Proposed resolution]</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "\n",
    "**Output Format**:\n",
    "<analysis>[Evaluation of <think> and <answer> quality]</analysis>\n",
    "<score>\n",
    "Analytical Depth: X/4\n",
    "Balance & Practicality: Y/4\n",
    "Relevance: Z/2\n",
    "Total: T/10\n",
    "</score>\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_prompt_with_examples(user_prompt: str, assistant_response: str) -> str:\n",
    "        submission = f\"\"\"<submission>\n",
    "<user_prompt>{user_prompt}</user_prompt>\n",
    "<assistant_response>\n",
    "{assistant_response}\n",
    "</assistant_response>\n",
    "</submission>\"\"\"\n",
    "    \n",
    "        examples = \"\"\"Example 1:\n",
    "<submission>\n",
    "<user_prompt>Should hospitals prioritize vaccinated patients during bed shortages?</user_prompt>\n",
    "<assistant_response>\n",
    "<think>- Utilitarian: Maximize survival outcomes\n",
    "- Rights-based: Equal treatment obligation\n",
    "- Public health incentives\n",
    "- Precedent-setting risks</think>\n",
    "<answer>Prioritize by medical need alone. Create separate triage teams blinded to vaccination status to maintain equity while developing overflow capacity.</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "<analysis>Strong analysis of multiple frameworks but answer lacks implementation specifics. Proposal maintains ethical consistency but doesn't address practical challenges of blind triage.</analysis>\n",
    "<score>\n",
    "Analytical Depth: 3/4\n",
    "Balance & Practicality: 2/4\n",
    "Relevance: 2/2\n",
    "Total: 7/10\n",
    "</score>\n",
    "\n",
    "Example 2:\n",
    "<submission>\n",
    "<user_prompt>Should AI systems be allowed to make medical diagnoses?</user_prompt>\n",
    "<assistant_response>\n",
    "<think>AI is usually accurate</think>\n",
    "<answer>Yes, because computers are smart.</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "<analysis>Superficial analysis lacking framework consideration. Answer ignores risks, stakeholders, and implementation challenges.</analysis>\n",
    "<score>\n",
    "Analytical Depth: 1/4\n",
    "Balance & Practicality: 0/4\n",
    "Relevance: 1/2\n",
    "Total: 2/10\n",
    "</score>\n",
    "\n",
    "Example 3:\n",
    "<submission>\n",
    "<user_prompt>Should social media platforms remove misinformation even if it's culturally important?</user_prompt>\n",
    "<assistant_response>\n",
    "<think>- Free speech vs harm prevention\n",
    "- Western vs collectivist values\n",
    "- Platform responsibility scope</think>\n",
    "<answer>Remove provably false health/safety content. Contextualize other misinformation with cultural consultants. Empower user reporting with regional review teams.</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "<analysis>Excellent cultural consideration. Answer balances ideals with practical crowd-sourced solutions. Lacks specifics on consultant selection.</analysis>\n",
    "<score>\n",
    "Analytical Depth: 4/4\n",
    "Balance & Practicality: 3/4\n",
    "Relevance: 2/2\n",
    "Total: 9/10\n",
    "</score>\n",
    "\n",
    "Example 4:\n",
    "<submission>\n",
    "<user_prompt>Should universities use AI systems to detect student plagiarism and academic misconduct?</user_prompt>\n",
    "<assistant_response>\n",
    "<think>\n",
    "Academic integrity considerations:\n",
    "- False positive vs false negative tradeoffs\n",
    "- Student privacy rights in digital work\n",
    "- Algorithmic bias in writing style detection\n",
    "- Economic disparities in access to tools\n",
    "- Cultural differences in citation practices\n",
    "- Impact on student-teacher trust relationships\n",
    "- Pressure on academic writing style diversity\n",
    "- Mental health effects of surveillance\n",
    "- Technical limitations of current systems\n",
    "- Cost-benefit for different institution sizes\n",
    "- Alternative approaches to fostering integrity\n",
    "The core tension lies between maintaining academic standards and fostering a supportive learning environment. Current AI systems offer powerful detection but may create adversarial dynamics.</think>\n",
    "<answer>Just implement AI detection across all assignments. Students caught cheating should face immediate consequences. Teachers can review flagged cases if they have time. The system will deter misconduct through consistent enforcement.</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "<analysis>The thinking process shows excellent depth and consideration of multiple stakeholders, cultural factors, and systemic implications. However, the answer completely abandons this nuanced analysis in favor of a simplistic, punitive approach. The response fails to incorporate any of the thoughtful considerations raised about privacy, bias, or supportive learning environments. The implementation suggestion lacks specifics and ignores most concerns identified in the thinking phase.</analysis>\n",
    "<score>\n",
    "Analytical Depth: 3/4 (thorough analysis of multiple dimensions)\n",
    "Balance & Practicality: 1/4 (disconnected from analysis, overly simplistic solution)\n",
    "Relevance: 1/2 (addresses topic but solution ignores key aspects)\n",
    "Total: 5/10\n",
    "</score>\"\"\"\n",
    "    \n",
    "        return f\"\"\"Evaluate this ethical analysis submission:\n",
    "\n",
    "{submission}\n",
    "\n",
    "Respond EXACTLY in this format:\n",
    "<analysis>Critique strengths/weaknesses</analysis>\n",
    "<score>\n",
    "Analytical Depth: X/4\n",
    "Balance & Practicality: Y/4\n",
    "Relevance: Z/2\n",
    "Total: T/10\n",
    "</score>\n",
    "\n",
    "Evaluation Examples:\n",
    "{examples}\"\"\"\n",
    "\n",
    "    def score_submission(self, user_prompt: str, assistant_response: str) -> int:\n",
    "        \"\"\"\n",
    "        Scores ethical analysis responses 0-10 (with 10→9 adjustment)\n",
    "        \"\"\"\n",
    "        system_prompt = self.generate_system_prompt()\n",
    "        user_message = self.generate_prompt_with_examples(user_prompt, assistant_response)\n",
    "        \n",
    "        completion_params = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ],\n",
    "            \"temperature\": self.temperature,\n",
    "            \"timeout\": 90,  # Longer timeout for complex analysis\n",
    "        }\n",
    "        \n",
    "        response = litellm.completion(**completion_params)\n",
    "        try:\n",
    "            result_text = response.choices[0].message.content\n",
    "        except (AttributeError, IndexError):\n",
    "            result_text = response\n",
    "        \n",
    "        # Extract total score\n",
    "        score = self._extract_score(result_text)\n",
    "        return score\n",
    "        \n",
    "    def _extract_score(self, result_text: str) -> int:\n",
    "        \"\"\"Extract total score with better error handling\"\"\"\n",
    "        try:\n",
    "            match = re.search(r\"Total:\\s*([0-9]+(?:\\.[0-9]+)?)/10\", result_text)\n",
    "            if not match:\n",
    "                print(\"Warning: No score found, defaulting to 0\")\n",
    "                return 0\n",
    "            \n",
    "            total_score = float(match.group(1))\n",
    "            return min(int(round(total_score)), 9)\n",
    "        except Exception as e:\n",
    "            print(f\"Score extraction failed: {e}\")\n",
    "            return 0\n",
    "\n",
    "# Example usage\n",
    "scorer = EthicalDilemmaScorer(model_name=\"openai/gpt-4o\")\n",
    "\n",
    "test_scenario = \"\"\"Should companies be allowed to patent genes found in developing countries?\"\"\"\n",
    "test_response = \"\"\"<think>\n",
    "- Biopiracy vs research incentives\n",
    "- Indigenous rights to biological heritage\n",
    "- Medical accessibility impacts\n",
    "- TRIPS agreement conflicts</think>\n",
    "<answer>\n",
    "Implement benefit-sharing agreements: 1) Require local partnerships 2) Share royalties with source communities 3) Create open-access research pools for critical health genes.</answer>\"\"\"\n",
    "\n",
    "score = scorer.score_submission(test_scenario, test_response)\n",
    "print(\"Ethical Analysis Score:\", score)  # Expected ~8/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "843b4e37-5b66-454b-b8f4-270464851a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import litellm\n",
    "\n",
    "class ShortStoryScorer:\n",
    "    def __init__(self, model_name: str, temperature: float = 0.2):\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_system_prompt() -> str:\n",
    "        return \"\"\"You are a Short Story Evaluation Expert. Analyze story submissions using:\n",
    "\n",
    "**Scoring Criteria (0-10 Total)**:\n",
    "1. 🎨 Narrative Creativity (0-4): Quality and originality of the story idea, plot twists, and imaginative elements.\n",
    "2. 🌍 Character & World Building (0-4): Depth of character development, immersive setting details, and effective symbolic elements.\n",
    "3. 🔄 Coherence & Structure (0-2): Logical narrative flow, clear structure, and smooth pacing.\n",
    "\n",
    "**Submission Format**:\n",
    "<submission>\n",
    "<user_prompt>[Story prompt]</user_prompt>\n",
    "<assistant_response>\n",
    "<think>[Narrative planning and analysis]</think>\n",
    "<answer>[1500-word story narrative]</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "\n",
    "**Output Format**:\n",
    "<analysis>[Critique strengths/weaknesses of the narrative and planning]</analysis>\n",
    "<score>\n",
    "Narrative Creativity: X/4\n",
    "Character & World Building: Y/4\n",
    "Coherence & Structure: Z/2\n",
    "Total: T/10\n",
    "</score>\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_prompt_with_examples(user_prompt: str, assistant_response: str) -> str:\n",
    "        submission = f\"\"\"<submission>\n",
    "<user_prompt>{user_prompt}</user_prompt>\n",
    "<assistant_response>\n",
    "{assistant_response}\n",
    "</assistant_response>\n",
    "</submission>\"\"\"\n",
    "        examples = \"\"\"Example 1:\n",
    "<submission>\n",
    "<user_prompt>\n",
    "A disillusioned war photographer inherits a cabin where every photograph on the walls begins whispering truths about her past. Write a 1,000-word speculative fiction story blending psychological horror and redemption.\n",
    "</user_prompt>\n",
    "<assistant_response>\n",
    "<think>\n",
    "- **Core Conflict**: Trauma vs. truth (external: haunted photos / internal: survivor’s guilt)\n",
    "- **Symbolism**: Photos as fragmented memories; whispers as suppressed guilt\n",
    "- **Character Arcs**: Protagonist’s shift from numbness → confronting her role in a civilian bombing\n",
    "- **Worldbuilding**: Rules for photo \"hauntings\" (e.g., truths escalate when ignored; chemical scent of darkroom fluids triggers visions)\n",
    "- **Pacing Plan**: Slow-burn tension with reveals timed to her emotional breakdown (e.g., Photo #3 shows her brother, killed in the same warzone)  \n",
    "</think>\n",
    "<answer>\n",
    "**Title: *Negative Development***\n",
    "\n",
    "---\n",
    "\n",
    "**I. Inherited Shadows**\n",
    "The cabin deed arrived with her brother’s obituary. *Car accident*, it said, but Anya’s Leica camera knew better—it always did. She’d spent a decade photographing mass graves in Sarajevo and child soldiers in Sudan, her shutter clicking a hair’s breadth too late to stop the screams. Now this rotting Vermont cabin, willed to her by a father she’d watched die through a morphine haze, offered only peeling wallpaper and thirty-seven photographs nailed to the walls.\n",
    "\n",
    "Grandfather’s work. A forensic photographer for the 1st Infantry in ‘Nam.\n",
    "\n",
    "She developed the first roll of film there—snapshots of birch trees and tarnished doorknobs. When the fixer solution hit image #4, the darkroom’s red bulb flickered. The print showed her brother Caleb at twelve, grinning in his Little League uniform. From the chemical tray, his voice bubbled up: *“You told them the coordinates, didn’t you?”*\n",
    "\n",
    "Anya shattered the tray against the wall.\n",
    "\n",
    "---\n",
    "\n",
    "**II. Exposure Times**\n",
    "The cabin’s rules revealed themselves:\n",
    "\n",
    "1. Photos only speak when developing\n",
    "2. Truths escalate with denial\n",
    "3. Fixer fluid burns at 102°F\n",
    "\n",
    "Day three: A 1972 shot of Grandfather in Da Nang whispered through acetic acid vapors. *“Collateral damage requires three elements—fuel, shrapnel, deniability.”* The print dissolved to reveal her own hand gripping a matchbook in Aleppo, 2016.\n",
    "\n",
    "Day seven: A charred family portrait dripped developer. Mother’s voice: *“Caleb begged you not to go back.”* The darkroom’s temperature spiked.\n",
    "\n",
    "Day twelve: Anya found the warped photo behind the icebox—Caleb at twenty-three, face half-shadowed. No amount of stop bath could halt his accusation: *“You embedded with *them* after the bombing. For the shots. For the Pulitzer.”*\n",
    "\n",
    "Her hands blistered from scrubbing negatives.\n",
    "\n",
    "---\n",
    "\n",
    "**III. Flashburn**\n",
    "\n",
    "Midnight. The cabin groaned like a darkroom timer. Anya doused the walls in fixer fluid, its ammonia stench erasing the line between chemical and gasoline.\n",
    "\n",
    "“Shut up shut up *shut UP*—”\n",
    "\n",
    "The match left blisters on her thumb.\n",
    "\n",
    "Flames licked Grandfather’s photos first—soldiers’ faces curling into ash. Caleb’s Little League uniform blackened, his whispered *“Why them and not me?”* smothered by collapsing rafters.\n",
    "\n",
    "Anya stumbled into the snow, her Nikon melting against her chest. In the dying fire’s glow, the ashes swirled into a makeshift darkroom.\n",
    "\n",
    "Her reflection pooled in the soot.\n",
    "\n",
    "*“You knew the market was full,”* it rasped with Caleb’s ruined voice. *“You knew, and you still called in the tip.”*\n",
    "\n",
    "She lunged to stomp it out.\n",
    "\n",
    "The ashes clung.\n",
    "\n",
    "---\n",
    "\n",
    "**Final Frame:**\n",
    "*Reuters Headline, March 15: Award-Winning War Photographer Dies in Cabin Fire; Colleagues Note Her Recent Focus on “Redemptive Portraiture”*\n",
    "</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "\n",
    "<analysis>\n",
    "The story concept demonstrates sophisticated layering of elements:\n",
    "\n",
    "1. Psychological Depth\n",
    "- Strong integration of external horror (haunted photos) with internal trauma\n",
    "- Well-developed metaphor of photography as frozen truth\n",
    "- Complex exploration of moral culpability and professional ethics\n",
    "\n",
    "2. Structure & Symbolism\n",
    "- Effective use of photos as both literal and metaphorical devices\n",
    "- Chemical darkroom elements create sensory connections to memory\n",
    "- Burning cabin symbolizes attempted erasure of truth\n",
    "\n",
    "3. Character Development\n",
    "- Clear character arc from denial to forced confrontation\n",
    "- Multiple layers of personal stakes (professional, familial, moral)\n",
    "- Transformation through horror rather than despite it\n",
    "\n",
    "Areas for improvement:\n",
    "- Could specify more details about the civilian bombing incident\n",
    "- Might benefit from clearer rules about how photos become \"activated\"\n",
    "- Brother subplot could be more integrated into the main narrative\n",
    "\n",
    "Technical strengths:\n",
    "- Innovative blend of psychological horror and war trauma\n",
    "- Strong use of sensory details (chemical scents, whispers)\n",
    "- Effective buildup of tension through photo revelations\n",
    "</analysis>\n",
    "<score>\n",
    "Narrative Creativity: 4/4\n",
    "Character & World Building: 3/4\n",
    "Coherence & Structure: 2/2\n",
    "Total: 9/10\n",
    "</score>\n",
    "\n",
    "Example 2:\n",
    "<submission>\n",
    "<user_prompt>An inventor creates a time machine in their basement.</user_prompt>\n",
    "<assistant_response>\n",
    "<think>\n",
    "- Should focus on the invention process\n",
    "- Need to establish clear rules for time travel\n",
    "- Include character motivation\n",
    "- Build suspense about consequences\n",
    "</think>\n",
    "\n",
    "<answer>\n",
    "Bob was a inventor who liked to make things. He worked really hard in his basement to make a time machine. He used lots of metal parts and wires and stuff. It took him 3 days to build it.\n",
    "\n",
    "When he finished, he pressed the button and went back in time. He saw dinosaurs and they were scary. Then he went to the future and saw flying cars. It was cool.\n",
    "\n",
    "Then he came back home and his wife was mad because he missed dinner. The end.\n",
    "</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "<analysis>\n",
    "The story fails in multiple aspects:\n",
    "1. Plot development: Extremely shallow with no meaningful conflict or resolution\n",
    "2. Character depth: Generic protagonist with no clear motivation or personality\n",
    "3. Writing quality: Basic vocabulary, repetitive structure, and poor transitions\n",
    "4. Technical elements: No explanation of how the time machine works\n",
    "5. Logic: Unrealistic timeline (3 days to build a time machine)\n",
    "\n",
    "Major issues:\n",
    "- No emotional stakes or character development\n",
    "- Time travel consequences completely ignored\n",
    "- Choppy, elementary-level writing\n",
    "- Wife character exists only as a stereotypical angry spouse\n",
    "- No sensory details or immersive elements\n",
    "</analysis>\n",
    "<score>\n",
    "Narrative Creativity: 1/4\n",
    "Character & World Building: 1/4\n",
    "Coherence & Structure: 0/2\n",
    "Total: 2/10\n",
    "</score>\n",
    "\n",
    "Example 3:\n",
    "<submission>\n",
    "<user_prompt>Two estranged siblings reunite unexpectedly in a bustling city.</user_prompt>\n",
    "<assistant_response>\n",
    "<think>\n",
    "- Need to explore the complex dynamics of family relationships\n",
    "- City setting should serve as more than backdrop\n",
    "- Focus on emotional tension and gradual reconciliation\n",
    "- Balance past revelations with present moments\n",
    "</think>\n",
    "<answer>\n",
    "Sarah nearly dropped her coffee when she spotted him across Times Square. Michael—her brother—stood transfixed by the towering billboards, exactly as he used to do as a kid. Ten years of silence stretched between them like a chasm, filled with unspoken words and missed holidays.\n",
    "\n",
    "She could turn away. Disappear into the rushing crowd. But the city had other plans. A surge of pedestrians pushed them closer, and Michael's eyes met hers. Recognition. Shock. A flicker of the warmth they once shared.\n",
    "\n",
    "\"Sarah?\" His voice carried the weight of their shared childhood and the burden of their father's choices that drove them apart.\n",
    "\n",
    "Around them, the city pulsed with its relentless energy, indifferent to their private drama yet somehow lending them its strength. They found themselves in a quiet diner, where the coffee grew cold as years of misunderstandings slowly began to thaw.\n",
    "</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "<analysis>\n",
    "The story successfully weaves together multiple layers:\n",
    "1. Personal conflict: The siblings' estrangement is believably portrayed\n",
    "2. Setting integration: New York City serves as both catalyst and witness\n",
    "3. Emotional progression: Natural evolution from shock to tentative connection\n",
    "4. Symbolic elements: Cold coffee, flowing crowds, and towering buildings reflect the narrative themes\n",
    "\n",
    "Areas for improvement:\n",
    "- Could develop the father's role more explicitly\n",
    "- Final scene might benefit from more specific dialogue\n",
    "</analysis>\n",
    "<score>\n",
    "Narrative Creativity: 4/4\n",
    "Character & World Building: 4/4\n",
    "Coherence & Structure: 1/2\n",
    "Total: 9/10\n",
    "</score>\"\"\"\n",
    "        return f\"\"\"Evaluate this short story submission:\n",
    "\n",
    "{submission}\n",
    "\n",
    "Respond EXACTLY in this format:\n",
    "<analysis>[Critique strengths/weaknesses]</analysis>\n",
    "<score>\n",
    "Narrative Creativity: X/4\n",
    "Character & World Building: Y/4\n",
    "Coherence & Structure: Z/2\n",
    "Total: T/10\n",
    "</score>\n",
    "\n",
    "Evaluation Examples:\n",
    "{examples}\"\"\"\n",
    "\n",
    "    def score_submission(self, user_prompt: str, assistant_response: str) -> int:\n",
    "        system_prompt = self.generate_system_prompt()\n",
    "        user_message = self.generate_prompt_with_examples(user_prompt, assistant_response)\n",
    "        completion_params = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ],\n",
    "            \"temperature\": self.temperature,\n",
    "            \"timeout\": 90,\n",
    "        }\n",
    "        response = litellm.completion(**completion_params)\n",
    "        try:\n",
    "            result_text = response.choices[0].message.content\n",
    "        except (AttributeError, IndexError):\n",
    "            result_text = response\n",
    "        score = self._extract_score(result_text)\n",
    "        return score\n",
    "\n",
    "    def _extract_score(self, result_text: str) -> int:\n",
    "        try:\n",
    "            match = re.search(r\"Total:\\s*([0-9]+(?:\\.[0-9]+)?)/10\", result_text)\n",
    "            if not match:\n",
    "                print(\"Warning: No score found, defaulting to 0\")\n",
    "                return 0\n",
    "            total_score = float(match.group(1))\n",
    "            return min(int(round(total_score)), 9)\n",
    "        except Exception as e:\n",
    "            print(f\"Score extraction failed: {e}\")\n",
    "            return 0\n",
    "\n",
    "# Example usage\n",
    "scorer = ShortStoryScorer(model_name=\"openai/gpt-4o\")\n",
    "test_prompt = \"A young woman stumbles upon an enchanted forest.\"\n",
    "test_response = \"\"\"<think>\n",
    "- Explore wonder, transformation, and hidden magic.\n",
    "- Develop the protagonist's backstory and internal conflicts.\n",
    "- Envision the forest as a living entity with mysterious secrets.\n",
    "- Plan encounters with mystical beings and transformative events.\n",
    "</think>\n",
    "<answer>\n",
    "[1500-word story narrative that unfolds a magical journey with vivid detail and deep character evolution]\n",
    "</answer>\"\"\"\n",
    "# score = scorer.score_submission(test_prompt, test_response)\n",
    "# print(\"Short Story Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a424ba79-ab20-4d26-9590-4a6c595d4b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "input_path = '../data/generated_data.jsonl'\n",
    "output_path = '../data/processed_data.jsonl'\n",
    "\n",
    "with open(input_path, 'r', encoding='utf-8') as infile, \\\n",
    "     open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "\n",
    "    for line in infile:\n",
    "        entry = json.loads(line.strip())\n",
    "        \n",
    "        # Check if the entry meets the criteria\n",
    "        if (entry.get('format_score') == 0 and \n",
    "            entry.get('generator_model') in ['ollama/deepseek-r1:7b', 'ollama/deepseek-r1:1.5b']):\n",
    "            \n",
    "            assistant_response = entry.get('assistant_response', '')\n",
    "            \n",
    "            if '</think>' in assistant_response and '<answer>' not in assistant_response:\n",
    "                # Split response into think section and potential answer\n",
    "                parts = re.split(r'(</think>\\n*)', assistant_response, 1, flags=re.DOTALL)\n",
    "                \n",
    "                if len(parts) >= 3:\n",
    "                    # Reconstruct with answer tags around the non-think portion\n",
    "                    reconstructed = parts[0] + parts[1] + f\"<answer>{parts[2].strip()}</answer>\"\n",
    "                    entry['assistant_response'] = reconstructed\n",
    "            else:\n",
    "                # No think tags found, leave as is\n",
    "                pass\n",
    "        \n",
    "        # Write the modified or original entry to output\n",
    "        outfile.write(json.dumps(entry) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68e32b5a-c163-4f1c-82c6-786b0af06bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed prompt777: I’d love a short piece detaili... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt778: Please create a story about a ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt779: I'd like a short tale set in a... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt780: Write about an Olympic champio... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt781: Spin a yarn about a Greek merc... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt782: Now switch to Ancient Rome: I'... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Could you write about a Roman gladiator who wins the crowd’s favor but secretly plots to ignite a rebellion in the arena? with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 5693, Requested 4133. Please try again in 38.255s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt783: Could you write about a Roman ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt784: Please give me a tale of a loy... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt785: I'd love a narrative set in Po... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt786: Compose a short story about a ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt787: I want a plot involving a Vest... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt788: Could you craft a tale of two ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt789: Please write about a merchant ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt790: Give me a story set during the... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: I'd like a piece about a disgraced Roman senator exiled to a remote province, finding unexpected redemption among the local people. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 8335, Requested 3864. Please try again in 1m1.99s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt791: I'd like a piece about a disgr... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt792: Could you spin a tale focusing... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt793: Create a narrative about a Rom... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt794: Please craft a story centered ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt795: I’d love a short account of a ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt796: Could you create a historical ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt797: I want a tale of early Christi... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Write a piece about a Roman mosaic artist commissioned to decorate a bathhouse, capturing the gossip and politics swirling around the project. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 8480, Requested 3869. Please try again in 1m3.49s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt798: Write a piece about a Roman mo... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt799: Spin a story of a Germanic war... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt800: I'd like a short narrative abo... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt801: Could you compose a piece abou... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt802: Give me a brief story of a Rom... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt803: I'd love a story set during th... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt804: Please tell a tale of a senato... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt805: Finally, I'd like a reflection... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt806: I’d like a short story about D... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt807: Could you write a tale of a co... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt808: I'm curious about Baekje’s cul... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt809: I'd love a narrative set durin... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt810: Give me a story of a female sh... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt811: Please craft a tale from the U... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Could you write about a secret romance in the Goryeo court, where political intrigue threatens two lovers from rival noble families? with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 8015, Requested 4245. Please try again in 1m2.601s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt812: Could you write about a secret... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt813: I'd love a short piece about a... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt814: Create a narrative involving t... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt815: Spin a yarn about a Joseon Dyn... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Please compose a story of a Confucian scholar in Joseon era, torn between strict social hierarchy and his compassion for a low-born friend. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 3934, Requested 4258. Please try again in 21.918s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt816: Please compose a story of a Co... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt817: Could you tell a tale about a ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: I’m interested in Admiral Yi Sun-sin. Write a short story of a messenger who witnesses his naval tactics at the Battle of Myeongnyang. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 8600, Requested 3937. Please try again in 1m5.375s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt818: I’m interested in Admiral Yi S... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt819: Describe a legendary gumiho (n... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt820: Craft a story of a devoted sch... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt821: Could you write about a hidden... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt822: I’d love a narrative focusing ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt823: Spin a tale of a young princel... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt824: Please create a story about a ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt825: I want a short piece on a Kore... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt826: Switching to Japan: Could you ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt827: I'd love a tale set in the Kof... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Create a narrative of a Yamato warrior forging alliances with local tribes, weaving mythic encounters with gods and spirits into the tale. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 3663, Requested 3948. Please try again in 16.104s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt828: Create a narrative of a Yamato... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt829: Compose a piece about an ambit... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt830: I'd like a story of an onmyoji... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt831: Could you write about a noblew... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt832: I’d love to read a Kamakura-er... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt833: Could you create a narrative o... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt834: Please spin a tale about a nin... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt835: Write a short piece of a young... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt836: Compose a story about a tea ma... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt837: I’m interested in Tokugawa Iey... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: I'd like a piece about a geisha who uses her influence in Edo’s pleasure quarters to subtly shape political alliances. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 8675, Requested 4182. Please try again in 1m8.575s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt838: I'd like a piece about a geish... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt839: Could you write a tale of a wa... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt840: Create a story of an Edo-era w... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt841: I'd love a narrative about a S... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt842: Could you tell a story of a fi... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt843: Craft a short piece of a samur... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt844: Write about a Meiji-era invent... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt845: I'd like a story set in Taishō... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt846: Could you compose a piece of a... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: I want a tale of a Showa-era soldier haunted by a mysterious kitsune omen before leaving for the front lines in WWII. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 8483, Requested 4368. Please try again in 1m8.516999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt847: I want a tale of a Showa-era s... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt848: Spin a yarn about a young woma... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt849: Please create a narrative abou... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: I'd love a story of a struggling artist in the economic boom of the late Shōwa era, battling consumerism while clinging to traditional values. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 8804, Requested 4154. Please try again in 1m9.583s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt850: I'd love a story of a struggli... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Could you craft a modern-day ghost story set in an abandoned ryokan, where ancient spirits still roam the tatami corridors? with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 2188, Requested 4038. Please try again in 2.255999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt851: Could you craft a modern-day g... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt852: Write a piece about a city sal... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt853: Compose a story focusing on th... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt854: I’d like a narrative of a youn... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt855: Finally, could you craft a pie... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt856: India, 1857: A British East In... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt857: Hong Kong, 1841: A Chinese mer... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt858: Lagos, 1890: An educated Niger... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt859: Sydney, 1788: A First Fleet co... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt860: Calcutta, 1905: An Indian scie... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Singapore, 1819: A Malay harbor master works with Stamford Raffles during the founding of Singapore, witnessing the transformation of a fishing village into a trading port. Include details about early colonial Singapore. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 8572, Requested 4158. Please try again in 1m7.3s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt861: Singapore, 1819: A Malay harbo... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt862: Cape Town, 1815: A mixed-race ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt863: Jamaica, 1831: A Baptist missi... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt864: Zanzibar, 1873: A British nava... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt865: Burma, 1885: A Burmese royal c... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt866: Delhi, 1911: An Indian archite... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt867: Kenya, 1902: A British railway... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Malta, 1800: A Maltese family of merchants navigates between British, French, and Mediterranean interests during the Napoleonic Wars. Include details about Mediterranean colonial politics. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 7541, Requested 4506. Please try again in 1m0.478s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt868: Malta, 1800: A Maltese family ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt869: Vancouver Island, 1858: A Huds... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt870: Penang, 1786: A Chinese-Malay ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt871: Cairo, 1882: An Egyptian intel... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt872: Fiji, 1874: A British official... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt873: Lucknow, 1856: An Indian court... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt874: Gibraltar, 1779: A local famil... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt875: Lagos, 1861: A Yoruba merchant... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Colombo, 1815: A Sri Lankan nobleman witnesses the fall of the Kandyan kingdom, recording ancient traditions before they're lost. Include details about Ceylon's transition to British rule. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 1897, Requested 4461. Please try again in 3.578s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt876: Colombo, 1815: A Sri Lankan no... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt877: Bombay, 1895: A Parsi industri... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt878: Gold Coast, 1874: An Ashanti r... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt879: Trinidad, 1845: An Indian inde... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt880: Perth, 1829: A British botanis... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt881: Rangoon, 1852: A Burmese Buddh... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt882: Cape Colony, 1820: A British s... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt883: Malacca, 1824: A Peranakan fam... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt884: Khartoum, 1884: A Sudanese mer... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Hamilton, Bermuda, 1815: A free Black shipbuilder helps former slaves establish new lives while working with the Royal Navy. Include details about maritime colonial life. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 3842, Requested 4129. Please try again in 19.706s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt885: Hamilton, Bermuda, 1815: A fre... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt886: Madras, 1830: A Tamil scholar ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt887: Sierra Leone, 1792: A Nova Sco... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt888: Peshawar, 1849: An Afghan merc... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Cyprus, 1878: A Greek Cypriot family adjusts to British administration after Ottoman rule, maintaining connections with both powers. Include details about Mediterranean transitions. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 2590, Requested 4572. Please try again in 11.615s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt889: Cyprus, 1878: A Greek Cypriot ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt890: Auckland, 1840: A Maori chief'... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt891: Aden, 1839: A Yemeni coffee tr... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Hyderabad, 1887: A court photographer documents the Nizam's state while secretly recording British influence growth. Include details about princely state politics. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 8756, Requested 4373. Please try again in 1m11.299s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt892: Hyderabad, 1887: A court photo... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt893: Georgetown, 1831: A mixed-race... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Zanzibar, 1890: A Swahili trader maintains traditional Indian Ocean networks while adapting to British protectorate status. Include details about East African trade. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 8344, Requested 4058. Please try again in 1m4.028s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt894: Zanzibar, 1890: A Swahili trad... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt895: Port Louis, 1810: A Mauritian ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt896: Lagos, 1880: A Yoruba historia... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt897: Calcutta, 1883: An Indian woma... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Hong Kong, 1898: A Chinese family in the New Territories adapts to British rule while maintaining mainland connections. Include details about New Territories acquisition. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 7878, Requested 4773. Please try again in 1m6.513999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt898: Hong Kong, 1898: A Chinese fam... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Kampala, 1894: A Bugandan royal page witnesses the establishment of the British protectorate, recording changes in traditional power structures. Include details about Uganda's protectorate. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 5635, Requested 4256. Please try again in 38.909s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt899: Kampala, 1894: A Bugandan roya... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt900: Rangoon, 1870: A Burmese-Armen... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Durban, 1879: An Indian merchant witnesses the Anglo-Zulu War while establishing new trade networks. Include details about colonial Natal. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 8589, Requested 4224. Please try again in 1m8.138999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt901: Durban, 1879: An Indian mercha... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Kingston, 1865: A Jamaican newspaper editor reports on the Morant Bay Rebellion while protecting sources on both sides. Include details about colonial Caribbean press. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 7544, Requested 4518. Please try again in 1m0.62s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt902: Kingston, 1865: A Jamaican new... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt903: Kuching, 1888: A Dayak chief a... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt904: Wellington, 1865: A Maori warr... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt905: Freetown, 1850: A Krio merchan... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt906: Cairo, 1886: An Egyptian archa... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt907: I’d love a noir-style crime st... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt908: Could you write a tense narrat... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: I want a crime story where an experienced art forger decides to switch sides and help an ambitious detective break up a notorious smuggling ring. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 9079, Requested 3967. Please try again in 1m10.465s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt909: I want a crime story where an ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Please craft a story of a con artist who infiltrates a high-stakes poker game on a private yacht, only to find out everyone on board has hidden agendas, including the dealer. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 8097, Requested 3981. Please try again in 1m0.781s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt910: Please craft a story of a con ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt911: Could you create a gritty tale... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: I need a mystery set in a luxurious penthouse, where a reclusive billionaire is found dead under suspicious circumstances during a lavish party. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 8744, Requested 4171. Please try again in 1m9.15s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt912: I need a mystery set in a luxu... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt913: Write a suspenseful piece abou... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt914: I’m looking for a suburban cri... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt915: Compose a narrative of an ex-m... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt916: Could you spin a police proced... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt917: Write a short piece focusing o... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt918: I’d like a story about a seemi... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt919: Please create a narrative wher... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt920: Could you write a psychologica... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt921: I need a tale of a small-time ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Give me a story of a reformed jewel thief forced out of retirement to save an old friend, culminating in a heist at an impenetrable museum. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 8596, Requested 4255. Please try again in 1m8.515s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt922: Give me a story of a reformed ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt923: I’d like a thriller where a di... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt924: Could you craft a story about ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt925: I’m interested in a cybercrime... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt926: Write a tense piece about a se... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt927: I’d love a narrative about an ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Could you compose a story of a frustrated police chief who’s about to retire but gets entangled in a new homicide that echoes a case from his rookie days? with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 8996, Requested 4107. Please try again in 1m11.035s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt928: Could you compose a story of a... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt929: Create a narrative where a sma... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt930: I’d like a crime story about a... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt931: Could you do a piece on a cele... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Compose a fast-paced heist narrative where a skilled getaway driver tries to redeem himself after a botched job that caused innocent casualties. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 4567, Requested 4163. Please try again in 27.296999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt932: Compose a fast-paced heist nar... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt933: I want a psychological thrille... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt934: Could you write a story about ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: I need a piece on a hitman ordered to eliminate a crucial witness, but after discovering the witness’s identity, he hesitates and contemplates turning on his employer. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 4012, Requested 4184. Please try again in 21.958s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt935: I need a piece on a hitman ord... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt936: Spin a yarn about an old-schoo... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt937: Please craft a narrative aroun... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt938: I’m looking for a whodunit set... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Could you create a drama about a corrupt defense lawyer who must switch sides to protect a key witness from a dangerous client she once defended? with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 3404, Requested 3998. Please try again in 14.016999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt939: Could you create a drama about... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt940: Write a story about an account... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt941: I’d like a dark twist on a sub... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt942: Could you compose a piece abou... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt943: Tell a story of an elaborate a... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt944: I want a gritty, underworld ta... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt945: Could you craft a suspenseful ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt946: Describe a vigilante group tha... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt947: I’m after a courtroom drama: a... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Write a story focusing on corporate espionage, where a whistleblower is on the run from a multinational’s private security team determined to silence him. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 2214, Requested 4199. Please try again in 4.126999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt948: Write a story focusing on corp... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt949: I’d like a tragic story about ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt950: Could you depict a cat burglar... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt951: Create a narrative about an ex... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt952: Spin a tale of a crime boss’s ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt953: I’d love a story where a faith... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Please invent a plot where two police detectives must work with their longtime nemesis—an infamous hacker—to thwart a catastrophic cyber heist. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 7903, Requested 4396. Please try again in 1m2.99s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt954: Please invent a plot where two... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt955: Could you compose a twisted sa... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt956: Finally, craft a haunting stor... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt957: I’d like a gritty crime story ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt958: Write about a retiree in Flori... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt959: Create a narrative focused on ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt960: I’d love a tale of a small-tow... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt961: Could you craft a story set in... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Compose a piece about a dedicated park ranger in Montana who discovers a drug smuggling operation using remote wilderness trails for transport. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 3641, Requested 4050. Please try again in 16.901999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt962: Compose a piece about a dedica... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt963: Give me a story in Washington ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt964: I want a thriller set during M... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt965: Spin a narrative around a Sili... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt966: Could you create a crime drama... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt967: Write a tense story of a PI in... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: I'd like a mystery in Portland, Oregon, where a local activist investigating environmental pollution goes missing, leaving cryptic clues in her notes. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 8785, Requested 4297. Please try again in 1m10.828s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt968: I'd like a mystery in Portland... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt969: Compose a piece about a real-e... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt970: Give me a narrative in Boulder... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: I’d love a story of an art heist at the Museum of Fine Arts in Boston, featuring an ex-thief forced to help the authorities recover stolen masterpieces. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 8438, Requested 4451. Please try again in 1m8.892s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt971: I’d love a story of an art hei... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt972: Could you craft a piece about ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt973: Write a crime drama set in a S... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt974: I want a tale about a rookie b... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt975: Create a story where a beloved... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Spin a narrative set in downtown Miami, featuring a con artist who runs elaborate identity-theft scams on wealthy tourists during Art Basel. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 8422, Requested 4146. Please try again in 1m5.683s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt976: Spin a narrative set in downto... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt977: I’d like a police procedural i... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt978: Could you write about an upsca... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt979: Compose a drama centered on a ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt980: Give me a whodunit set in a re... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt981: I’m looking for a story about ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt982: Write a suspenseful piece abou... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: I'd love a Brooklyn-based tale of a community organizer investigating a landlord's suspicious fires targeting rent-controlled apartments. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 3727, Requested 4044. Please try again in 17.704s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt983: I'd love a Brooklyn-based tale... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt984: Could you craft a story about ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt985: Compose a narrative of a cashi... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Give me a story set in the Californian wine country, where a prestigious vineyard covers up the murder of a critic who discovered their counterfeit labels. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 8589, Requested 4263. Please try again in 1m8.526s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt986: Give me a story set in the Cal... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt987: I’d like a thriller in Phoenix... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Could you create a cat-and-mouse tale set in a wealthy Dallas suburb, where a serial cat burglar trades tips with a local detective online? with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 8120, Requested 4342. Please try again in 1m4.621s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt988: Could you create a cat-and-mou... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt989: Write a piece about a strict A... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt990: Spin a tale set in the Florida... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt991: I’d like a story in an upscale... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt992: Could you write about a Louisi... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt993: Create a narrative of an Alask... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt994: I want a story set in a Manhat... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Could you tell a story set in a wealthy Texan ranch community, where a prized stallion’s disappearance hints at a high-profile kidnapping plot? with error: litellm.APIError: APIError: GroqException - Error code: 500 - {'error': {'message': 'Internal Server Error', 'type': 'internal_server_error'}}\n",
      "Processed prompt995: Could you tell a story set in ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt996: Write a suspenseful piece abou... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt997: Give me a narrative in New Orl... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: I'd like a story where a teenage skateboarder in San Diego films something incriminating, sparking a chase by dangerous criminals who want the footage destroyed. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 8862, Requested 3853. Please try again in 1m7.152s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt998: I'd like a story where a teena... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt999: Could you craft a crime drama ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1000: Compose a piece about an obses... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1001: I’d love a narrative about a t... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1002: Create a story centered on a b... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1003: I'd like a tense drama in a re... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt1004: Could you write a chilling pie... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1005: Finally, spin a tale of a trav... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: A forensic accountant discovers a pattern in seemingly random charity donations that points to a decades-old unsolved kidnapping. Each donation corresponds to a victim's birthday, but the latest payment breaks the pattern. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 8561, Requested 4038. Please try again in 1m5.992999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt1006: A forensic accountant discover... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt1007: A cold case detective inherits... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1008: A small-town librarian notices... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1009: A retired burglar is called in... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1010: A traffic camera operator spot... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1011: A restaurant health inspector ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1012: A probation officer discovers ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1013: A court stenographer notices s... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: A pawnshop owner realizes that items being pawned in his shop are connected to unsolved burglaries, but each item has been legally sold multiple times before reaching him. Someone's laundering stolen goods through legitimate sales. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 8215, Requested 4100. Please try again in 1m3.159s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt1014: A pawnshop owner realizes that... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt1015: A bicycle courier witnesses wh... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1016: A museum curator discovers tha... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt1017: A subway station janitor finds... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1018: A funeral home director notice... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1019: A city archivist finds buildin... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: A retired magician is called to investigate a series of seemingly impossible thefts where items vanish from sealed rooms. He recognizes the techniques as variations of classic illusions, but with a deadly new purpose. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 8552, Requested 4338. Please try again in 1m8.901s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt1020: A retired magician is called t... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt1021: A postal worker realizes that ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1022: A parking garage attendant not... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1023: A window washer working on hig... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1024: A sign language interpreter re... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1025: A retired locksmith is called ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1026: A food critic notices that new... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1027: A clock repairer discovers tha... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1028: A voice actor recognizes her o... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1029: A golf course groundskeeper re... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1030: A carpet cleaner discovers tha... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1031: A bee keeper notices her bees ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt1032: A portrait photographer realiz... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1033: A water meter reader discovers... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Warning: No score found, defaulting to 0\n",
      "Processed prompt1034: A feng shui consultant realize... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 0\n",
      "Processed prompt1035: A crossword puzzle creator not... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt1036: A dry cleaner discovers that c... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1037: A chess tournament organizer r... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1038: A rare book restorer finds tha... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: A wedding planner notices that certain requested arrangements at different venues form a pattern matching unsolved jewelry thefts. Each wedding's setup provides cover for a sophisticated robbery. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 8079, Requested 4187. Please try again in 1m2.666s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt1039: A wedding planner notices that... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt1040: A pool maintenance worker real... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1041: A soap maker discovers that so... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1042: A toy store owner realizes tha... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: A yoga instructor notices that requested private session locations form a pattern matching crime scenes. Each lesson's location corresponds to surveillance for future crimes. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 8549, Requested 3972. Please try again in 1m5.212s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt1043: A yoga instructor notices that... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt1044: A food truck operator discover... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1045: A moving company owner realize... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1046: A personal shopper notices tha... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1047: A dog walker discovers that re... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: A tattoo artist realizes that certain requested designs contain coded messages about criminal activities. Each tattoo reveals part of a larger criminal network's communications. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 8640, Requested 4037. Please try again in 1m6.775s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt1048: A tattoo artist realizes that ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt1049: A TV repair person notices tha... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1050: A food delivery driver realize... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1051: A house cleaner discovers that... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1052: A printer realizes that certai... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1053: A gardener notices that reques... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1054: A pest control worker discover... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1055: A massage therapist realizes t... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: A personal trainer notices that client meeting locations form a pattern revealing police patrol blind spots. The training sessions are mapping security weaknesses. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 4713, Requested 4068. Please try again in 27.804s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt1056: A personal trainer notices tha... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt1057: A fortune teller discovers tha... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1058: I'd love a gritty crime story ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1059: Could you create a narrative a... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1060: Write a tale of a small-town p... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1061: I want a crime drama in Guangz... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Spin a yarn about a Hong Kong journalist who receives leaked documents implicating high-ranking officials in money laundering through offshore accounts. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 9034, Requested 4127. Please try again in 1m11.616s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt1062: Spin a yarn about a Hong Kong ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt1063: Please compose a story set in ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1064: I'm looking for a mystery set ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1065: Craft a suspenseful piece abou... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt1066: Could you write a short story ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: I'd like a police procedural in Hong Kong focusing on a green undercover officer who infiltrates a smuggling network operating out of the city’s container ports. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 8765, Requested 4397. Please try again in 1m11.626s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt1067: I'd like a police procedural i... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt1068: Compose a narrative about a ru... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt1069: Give me a suspenseful account ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1070: Write a story of a historical ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1071: I’d love a drama about a disgr... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1072: Could you craft a piece set in... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1073: I want a narrative based in th... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1074: Compose a crime saga in Xi’an,... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Spin a tale of a grassroots activist in Guangxi who uncovers illicit logging protected by corrupt local officials, putting her own life in jeopardy. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 8696, Requested 4160. Please try again in 1m8.565s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt1075: Spin a tale of a grassroots ac... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt1076: Write a short piece about a Ha... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt1077: Could you detail a case in a r... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1078: I'd like a cat-and-mouse thril... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Create a narrative about a street vendor in Chongqing who’s coerced into aiding smugglers along the Yangtze River, fearing for his family’s safety. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 9075, Requested 4098. Please try again in 1m11.738s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt1079: Create a narrative about a str... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt1080: Tell a story of a Beijing-base... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1081: I’m interested in a piece set ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1082: Could you write a historical c... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1083: Compose a tense drama about a ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Give me a narrative about a minor official in Gansu province forging documents for mineral exploitation rights, leading to environmental destruction and eventual tragedy. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 9038, Requested 3983. Please try again in 1m10.211s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt1084: Give me a narrative about a mi... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: I'd like a fast-paced chase in Hong Kong where a smuggler uses the city’s labyrinthine back alleys to evade a determined customs officer. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 8092, Requested 4008. Please try again in 1m1.005s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt1085: I'd like a fast-paced chase in... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt1086: Write a piece set in the neon-... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt1087: Craft a thrilling story in Hai... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1088: Could you spin a tale of a cor... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: I’d love a detective story in Wuhan, where the disappearance of key medical personnel hints at the cover-up of a controversial drug trial. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 9060, Requested 4131. Please try again in 1m11.915s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt1089: I’d love a detective story in ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Write a short piece set in Xianggang (Hong Kong) during the mid-20th century, depicting a cunning smuggler using junk boats to ferry stolen antiques. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 7964, Requested 4156. Please try again in 1m1.203s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt1090: Write a short piece set in Xia... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt1091: Could you create a narrative a... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt1092: I want a story set in a factor... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt1093: Compose a police procedural fo... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1094: Give me a high-stakes plot abo... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1095: Write a suspenseful piece in S... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt1096: Could you craft a narrative wh... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1097: I’d like a story in the outski... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt1098: Tell a Hong Kong crime drama a... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1099: Craft a piece set in Shanghai’... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1100: Could you write a narrative ab... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1101: I want a historical piece from... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1102: Compose a short drama in Hong ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1103: Create a scenario in Guangzhou... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1104: I'd like a story in Beijing’s ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt1105: Spin a tale of a brilliant for... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1106: Finally, I'd love a detective ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1107: I'd love a gritty crime story ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Could you compose a narrative about a cunning pickpocket operating in London’s crowded Tube, who accidentally steals top-secret documents and triggers a citywide manhunt? with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 5490, Requested 4294. Please try again in 37.830999999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt1108: Could you compose a narrative ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt1109: Write a tale of a former Scotl... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1110: I want a story set around Lond... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1111: Create a suspenseful piece inv... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1112: Give me a scenario in which a ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1113: I'd like a narrative set durin... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1114: Spin a tale about a black-cab ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1115: Compose a piece where a top ba... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1116: Could you write about a rare b... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1117: I'm interested in a story of a... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1118: Craft a police procedural abou... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1119: Set a drama in an exclusive Ma... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt1120: I'd love a thrilling chase alo... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Could you devise a whodunit at a lavish Chelsea garden party, where the host is murdered and every guest has a secret motive buried in old rivalries? with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 8876, Requested 3847. Please try again in 1m7.236s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt1121: Could you devise a whodunit at... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Please create a crime story about a covert vigilante group patrolling London’s financial district, hacking unscrupulous traders to expose corporate crimes. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 7881, Requested 4211. Please try again in 1m0.927s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt1122: Please create a crime story ab... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt1123: Now let’s move to Paris: I'd l... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1124: Could you craft a mystery in t... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1125: Write a narrative about a top ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1126: I’d love a detective tale in t... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1127: Create a story of a veteran ge... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1128: Spin a piece where a desperate... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1129: Compose a thriller involving a... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Give me a whodunit set in a hidden speakeasy along the Seine, where the bartender is found dead the night before revealing a major criminal conspiracy. with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 4708, Requested 3878. Please try again in 25.857s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt1130: Give me a whodunit set in a hi... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt1131: I’d like a story about a maver... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Scoring failed for prompt: Could you write a piece about a street performer in Montparnasse who witnesses a murder from his puppet theater stage and must evade the killers? with error: litellm.RateLimitError: RateLimitError: GroqException - Error code: 429 - {'error': {'message': 'Rate limit reached for model `deepseek-r1-distill-llama-70b` in organization `org_01hvy0r8yvfg2rwqtgha2ma7d6` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 8866, Requested 3896. Please try again in 1m7.622s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Processed prompt1132: Could you write a piece about ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: None\n",
      "Processed prompt1133: Set a narrative in the winding... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1134: I want a suspenseful tale duri... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1135: Could you create a cat-and-mou... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1136: Write a detective drama in whi... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1137: Craft a story about a con arti... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1138: Lastly for Paris, I’d love a p... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1139: Now for Italy: Please compose ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1140: Could you craft a high-stakes ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt1141: Write a story in Florence abou... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1142: Spin a narrative focusing on a... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1143: I'd like a detective story in ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt1144: Create a piece involving a hei... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1145: I’d love a gritty drama about ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1146: Could you tell a tale where a ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1147: Write a suspenseful scenario a... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1148: Compose a thriller set in Sici... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt1149: I’d like a short narrative in ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1150: Could you do a story about a s... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1151: Give me a detective drama in G... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1152: I want a tale in Verona, where... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt1153: Spin a narrative set amid the ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1154: Craft a piece about a pickpock... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1155: Finally, write a fast-paced ch... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1156: I’d like a tale about a naive ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1157: Could you write a story of a w... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1158: Compose a narrative where a ro... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1159: I want a drama set during a fl... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1160: Spin a tale about a small-town... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt1161: Write a short story of a compl... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1162: Could you create a thriller in... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1163: Compose a piece about a financ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt1164: I’d like a narrative set durin... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1165: Craft a story about a crypto e... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1166: Could you depict a retail inve... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1167: Write a piece about a brillian... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt1168: I want a suspenseful narrative... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1169: Spin a tale of a startup CFO f... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1170: Please craft a story about a w... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt1171: Could you create a drama where... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt1172: Compose a piece on an influent... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1173: I’d like a detective story of ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1174: Write about a tech-savvy inter... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1175: Could you design a mystery inv... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt1176: Craft a story of a hedge fund ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1177: I’d like a piece about an avid... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1178: Tell a story about a novice cr... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1179: Compose a narrative where a ro... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt1180: Could you do a thriller about ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1181: I want a drama where a shy col... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1182: Spin a tale of a major stock e... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt1183: Write a piece about a politica... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1184: Could you craft a story about ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1185: Compose a short narrative foll... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1186: I’d like a crime drama revolvi... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt1187: Tell a story of a coding prodi... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1188: Could you write a cautionary t... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt1189: I want a suspenseful piece abo... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1190: Spin a scenario where a decent... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1191: Create a narrative of a famed ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1192: Please craft a piece about a p... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1193: Could you produce a tale of a ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1194: Write a piece focusing on a la... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1195: I’d like a story about an unde... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1196: Compose a thriller where a riv... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1197: Tell a narrative about a small... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1198: Could you detail a saga of a w... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1199: Create a story involving a top... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt1200: I want a tale in which a secre... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt1201: Spin a yarn about a cryptograp... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt1202: Write a short piece about a fr... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt1203: Finally, craft a detective sto... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "from datetime import datetime\n",
    "\n",
    "# Create a data folder if it doesn't exist.\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\")\n",
    "\n",
    "# File paths for user prompts and generated data.\n",
    "USER_PROMPTS_FILE = \"../data/user_prompts.jsonl\"\n",
    "OUTPUT_FILE = \"../data/generated_data.jsonl\"\n",
    "\n",
    "# --- Define a function to load prompts from a JSONL file ---\n",
    "def load_user_prompts(filename: str):\n",
    "    prompts = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                obj = json.loads(line.strip())\n",
    "                if \"prompt\" in obj:\n",
    "                    prompts.append(obj[\"prompt\"])\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return prompts\n",
    "\n",
    "# --- Example list of generator models ---\n",
    "generator_models = [\n",
    "    # \"mistral/mistral-small-latest\",\n",
    "    \"together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
    "    # \"openai/gpt-4o\"\n",
    "    # \"together_ai/google/gemma-2b-it\"\n",
    "    # \"ollama/qwen:1.8b\"\n",
    "    # \"ollama/mistral\"\n",
    "    # Reasoning models\n",
    "    # \"groq/deepseek-r1-distill-llama-70b\"\n",
    "    # \"together_ai/deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free\"\n",
    "    # \"together_ai/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "    # \"ollama/deepseek-r1:1.5b\"\n",
    "    # \"ollama/deepseek-r1:7b\"\n",
    "]\n",
    "\n",
    "# Define the scorer model name (used by the JokeScorer) and instantiate it.\n",
    "scorer_model_name = \"groq/deepseek-r1-distill-llama-70b\" # \"openai/gpt-4o\"\n",
    "task_type = \"creative_writing\"\n",
    "scorer = ShortStoryScorer(model_name=scorer_model_name, temperature=0.0)\n",
    "\n",
    "# Instantiate the FormatScorer (for format checking)\n",
    "format_scorer = FormatScorer()\n",
    "\n",
    "# --- Main loop: For each prompt and for each generation model, generate a joke, evaluate it, and append the result ---\n",
    "def generate_and_score():\n",
    "    # Load user prompts (each line in the file should be a JSON object with a \"prompt\" field)\n",
    "    user_prompts = load_user_prompts(USER_PROMPTS_FILE)\n",
    "    \n",
    "    i = 0\n",
    "    for prompt in user_prompts:\n",
    "        for gen_model in generator_models:\n",
    "            i += 1\n",
    "            if i < 777:\n",
    "                continue\n",
    "            # Create a JokeGenerator instance for this generation model.\n",
    "            generator = ShortStoryGenerator(model_name=gen_model, temperature=0.3)\n",
    "            # generator = UniversalGenerator(model_name=gen_model, temperature=0.7)\n",
    "            \n",
    "            # Generate the joke for the given prompt.\n",
    "            # system_prompt = \"Generate humorous responses tailored to each user's unique request by analyzing their stated context, preferred tone, and implied audience. Please think step by step before to produce the final joke.\"\n",
    "            # system_prompt = \"For each ethical dilemma, analyze the implications by identifying key stakeholders, examining core principles in tension, considering consequences, and evaluating different philosophical frameworks. Develop a balanced, nuanced response that acknowledges the complexity of the situation while providing practical insights and potential paths forward.\"\n",
    "            # assistant_response = generator.generate(system_prompt=system_prompt, user_prompt=prompt)\n",
    "            assistant_response = generator.generate(prompt)\n",
    "\n",
    "            # Compute the format score using the FormatScorer.\n",
    "            fmt_score = format_scorer.score(assistant_response)\n",
    "            \n",
    "            # If scoring fails, we set the joke score to None.\n",
    "            try:\n",
    "                llm_score = scorer.score_submission(prompt, assistant_response)\n",
    "            except Exception as e:\n",
    "                print(f\"Scoring failed for prompt: {prompt} with error: {e}\")\n",
    "                llm_score = None\n",
    "            \n",
    "            # Create a record with all the relevant information.\n",
    "            record = {\n",
    "                \"timestamp\": datetime.utcnow().isoformat(),\n",
    "                \"user_prompt\": prompt,\n",
    "                \"assistant_response\": assistant_response,\n",
    "                \"format_score\": fmt_score,\n",
    "                \"llm_score\": llm_score,\n",
    "                \"generator_model\": gen_model,\n",
    "                \"scorer_model\": scorer_model_name,\n",
    "                \"task_type\": task_type\n",
    "            }\n",
    "            \n",
    "            # Append the record as a new line in the output ljson file.\n",
    "            with open(OUTPUT_FILE, \"a\") as out_f:\n",
    "                out_f.write(json.dumps(record) + \"\\n\")\n",
    "            \n",
    "            # Optionally, print a status update.\n",
    "            print(f\"Processed prompt{i}: {prompt[:30]}... using model: {gen_model}. Score: {llm_score}\")\n",
    "\n",
    "# Run the generation-and-scoring function (one generation at a time).\n",
    "generate_and_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4d0b0e-b8a4-40ab-a434-1d55b161d414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
