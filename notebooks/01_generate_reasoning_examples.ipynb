{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d17ceba-ea86-418b-8cbb-ce4e60925cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: together in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (1.3.14)\n",
      "Requirement already satisfied: litellm in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (1.43.9)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.3 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (3.10.3)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.7 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (8.1.7)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.1.3 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (0.2.2)\n",
      "Requirement already satisfied: filelock<4.0.0,>=3.13.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (1.26.4)\n",
      "Requirement already satisfied: pillow<11.0.0,>=10.3.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (10.4.0)\n",
      "Requirement already satisfied: pyarrow>=10.0.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (17.0.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.6.3 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (2.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (2.32.3)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.8.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (13.9.4)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.2 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (4.66.5)\n",
      "Requirement already satisfied: typer<0.16,>=0.9 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (0.12.3)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (7.2.1)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (3.1.4)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (4.23.0)\n",
      "Requirement already satisfied: openai>=1.40.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (1.40.6)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (1.0.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (0.7.0)\n",
      "Requirement already satisfied: tokenizers in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (0.20.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.9.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from importlib-metadata>=6.8.0->litellm) (3.20.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from jinja2<4.0.0,>=3.1.2->litellm) (2.1.5)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.20.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.40.0->litellm) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.40.0->litellm) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.40.0->litellm) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.40.0->litellm) (0.5.0)\n",
      "Requirement already satisfied: sniffio in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.40.0->litellm) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.40.0->litellm) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.6.3->together) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.6.3->together) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->together) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->together) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->together) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->together) (2024.7.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from rich<14.0.0,>=13.8.1->together) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from rich<14.0.0,>=13.8.1->together) (2.18.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from tiktoken>=0.7.0->litellm) (2024.7.24)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from typer<0.16,>=0.9->together) (1.5.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from tokenizers->litellm) (0.24.5)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai>=1.40.0->litellm) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.40.0->litellm) (0.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (6.0.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.8.1->together) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "# Install libraries\n",
    "!pip install together litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06cd5ea4-214d-4186-8e3e-14f2d300f423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed.\n"
     ]
    }
   ],
   "source": [
    "# Formatting Scorer Class with Tests\n",
    "import re\n",
    "\n",
    "class FormatScorer:\n",
    "    \"\"\"\n",
    "    A simple scoring class that verifies whether the given text contains\n",
    "    the required tags: <think>, </think>, <answer>, and </answer>.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Pre-compile regex patterns for faster matching.\n",
    "        self.think_open = re.compile(r'<think>')\n",
    "        self.think_close = re.compile(r'</think>')\n",
    "        self.answer_open = re.compile(r'<answer>')\n",
    "        self.answer_close = re.compile(r'</answer>')\n",
    "        \n",
    "    def score(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Returns 1 if the text contains both <think>...</think> and \n",
    "        <answer>...</answer> tags; otherwise returns 0.\n",
    "        \"\"\"\n",
    "        if (self.think_open.search(text) is not None and\n",
    "            self.think_close.search(text) is not None and\n",
    "            self.answer_open.search(text) is not None and\n",
    "            self.answer_close.search(text) is not None):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "# --- Tests for the FormatScorer class ---\n",
    "\n",
    "def run_tests():\n",
    "    scorer = FormatScorer()\n",
    "    \n",
    "    # Test 1: No tags at all should return 0.\n",
    "    text1 = \"Hello, world!\"\n",
    "    assert scorer.score(text1) == 0, \"Test case 1 failed: expected score 0.\"\n",
    "    \n",
    "    # Test 2: Only <think> tags are present.\n",
    "    text2 = \"<think>This is a thought</think> Some text without answer tags.\"\n",
    "    assert scorer.score(text2) == 0, \"Test case 2 failed: expected score 0.\"\n",
    "    \n",
    "    # Test 3: Both <think> and <answer> tags are present.\n",
    "    text3 = \"<think>This is a thought</think><answer>This is an answer</answer>\"\n",
    "    assert scorer.score(text3) == 1, \"Test case 3 failed: expected score 1.\"\n",
    "    \n",
    "    # Test 4: Malformed text (missing closing tag for <answer>).\n",
    "    text4 = \"<think>This is a thought</think><answer>This is an answer\"\n",
    "    assert scorer.score(text4) == 0, \"Test case 4 failed: expected score 0.\"\n",
    "    \n",
    "    print(\"All tests passed.\")\n",
    "\n",
    "# Run the tests:\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "102caf32-4e64-4629-8946-812dc193af8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Joke:\n",
      " <think>For programming jokes, let's explore the relationship between code and its execution. Developers often struggle with errors. \"Bugs\" are literal (programming errors) and figurative (insects). A funny scenario could be a bug (insect) causing a code bug (error). Adding a twist where the bug fixes the code is funny because it subverts expectations of bugs being problematic.</think>\n",
      "<answer>Why did the programmer bring a bug to work? Because it was a code bug fix!</answer>\n"
     ]
    }
   ],
   "source": [
    "# Joke Generator\n",
    "import litellm\n",
    "\n",
    "class JokeGenerator:\n",
    "    def __init__(self, model_name: str, temperature: float = 0.7):\n",
    "        \"\"\"\n",
    "        Initialize the JokeGenerator with a specific model name and temperature.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_prompt_with_examples(prompt: str) -> str:\n",
    "        examples = \"\"\"Example 1:\n",
    "<think>I want to create anatomy humor. Skeletons are inherently funny because they're literal \"bare bones.\" What do they lack? Flesh/organs. \"Guts\" has a double meaning - both literal organs and figurative courage. This sets up a pun opportunity.</think>\n",
    "<answer>Why don't skeletons fight each other? They don't have the guts!</answer>\n",
    "\n",
    "Example 2:\n",
    "<think>Tech support jokes work well with personification. \"Doctor\" visits imply sickness. Common computer issues include viruses. Let's combine these - a computer catching a \"virus\" works literally (tech) and metaphorically (biology). Adds irony since computers are supposed to fix problems, not have them.</think>\n",
    "<answer>Why did the computer go to the doctor? It had a virus!</answer>\n",
    "\n",
    "Example 3:\n",
    "<think>Science humor opportunity. Atoms are fundamental but abstract. \"Make up everything\" has dual meaning - literal composition vs deception. Personifying atoms as untrustworthy creates surprise. Bonus science nod to their constant motion/making bonds.</think>\n",
    "<answer>Why don't scientists trust atoms? Because they make up everything!</answer>\"\"\"\n",
    "        return f\"\"\"You are an AI assistant that produces jokes. You should think about the joke first, then produce it.\n",
    "You should use the <think> tag to think about the joke, and the <answer> tag to produce the joke.\n",
    "Do not use any other tags or anything else in your response.\n",
    "\n",
    "Here are some examples of jokes:\n",
    "<examples>\n",
    "{examples}\n",
    "</examples>\n",
    "\n",
    "Now, produce a joke for the following prompt:\n",
    "{prompt}\n",
    "\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_system_prompt() -> str:\n",
    "        return \"\"\"You are an AI assistant that produces jokes. You should think about the joke first, then produce it.\n",
    "You should use the <think> tag to think about the joke, and the <answer> tag to produce the joke.\n",
    "Do not use any other tags or anything else in your response.\n",
    "\"\"\"\n",
    "\n",
    "    def generate_joke(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a joke for the given prompt using litellm.completion.\n",
    "        \"\"\"\n",
    "        system_prompt = self.generate_system_prompt()\n",
    "        user_prompt = self.generate_prompt_with_examples(prompt)\n",
    "        \n",
    "        # Build completion parameters as desired\n",
    "        completion_params = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            \"temperature\": self.temperature,\n",
    "            \"timeout\": 60,  # timeout in seconds\n",
    "        }\n",
    "        \n",
    "        response = litellm.completion(**completion_params)\n",
    "        \n",
    "        try:\n",
    "            joke = response.choices[0].message.content\n",
    "        except (AttributeError, IndexError):\n",
    "            joke = response\n",
    "        return joke\n",
    "\n",
    "\n",
    "# Create an instance using a model name\n",
    "generator = JokeGenerator(model_name=\"mistral/mistral-small-latest\")\n",
    "\n",
    "# Define a sample prompt to generate a joke.\n",
    "sample_prompt = \"Tell me a joke about programming.\"\n",
    "\n",
    "# Generate a joke.\n",
    "joke_output = \"\"\"<think>For programming jokes, let's explore the relationship between code and its execution. Developers often struggle with errors. \"Bugs\" are literal (programming errors) and figurative (insects). A funny scenario could be a bug (insect) causing a code bug (error). Adding a twist where the bug fixes the code is funny because it subverts expectations of bugs being problematic.</think>\n",
    "<answer>Why did the programmer bring a bug to work? Because it was a code bug fix!</answer>\"\"\"\n",
    "# generator.generate_joke(sample_prompt)\n",
    "print(\"Generated Joke:\\n\", joke_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e26e964e-5c76-480c-bbc0-5449bce449e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "\n",
    "class UniversalGenerator:\n",
    "    def __init__(self, model_name: str, temperature: float = 0.7):\n",
    "        \"\"\"\n",
    "        Initialize the UniversalGenerator with the given model and temperature.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def generate(self, system_prompt: str, user_prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Generates a response using the provided system and user prompts.\n",
    "        \"\"\"\n",
    "        completion_params = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            \"temperature\": self.temperature,\n",
    "            \"timeout\": 60  # Timeout in seconds\n",
    "        }\n",
    "        response = litellm.completion(**completion_params)\n",
    "        try:\n",
    "            generated_text = response.choices[0].message.content\n",
    "        except (AttributeError, IndexError):\n",
    "            generated_text = response\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5fab62af-9014-4df0-8122-aa8dc0c65836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Score: 7\n"
     ]
    }
   ],
   "source": [
    "# Joke Scorer\n",
    "import re\n",
    "import litellm\n",
    "\n",
    "class JokeScorer:\n",
    "    def __init__(self, model_name: str, temperature: float = 0.3):\n",
    "        \"\"\"\n",
    "        Initialize the JokeScorer with a specific model name and a default temperature of 0.3.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_system_prompt() -> str:\n",
    "        return \"\"\"You are a Joke Evaluation Expert. Analyse submissions using:\n",
    "    \n",
    "**Scoring Criteria (0-10 Total)**:\n",
    "1. 🎭 Wordplay (0-3): Pun/double-meaning quality in <answer>\n",
    "2. 💡 Originality (0-2): Novelty of <think> and <answer>\n",
    "3. 🎉 Surprise (0-2): Unexpected twist effectiveness\n",
    "4. 🔗 Relevance (0-3): Alignment with user request\n",
    "\n",
    "**Submission Format**:\n",
    "<submission>\n",
    "<user_prompt>[Original user request]</user_prompt>\n",
    "<assistant_response>\n",
    "<think>[Creator's reasoning]</think>\n",
    "<answer>[Joke text]</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "\n",
    "**Output Format**:\n",
    "<analysis>[Your evaluation of <think> and <answer> and the relevance to the user prompt]</analysis>\n",
    "<score>\n",
    "Wordplay: X/3\n",
    "Originality: Y/2\n",
    "Surprise: Z/2\n",
    "Relevance: W/3\n",
    "Total: T/10\n",
    "</score>\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_prompt_with_examples(user_prompt: str, assistant_response: str) -> str:\n",
    "        submission = f\"\"\"<submission>\n",
    "<user_prompt>{user_prompt}</user_prompt>\n",
    "<assistant_response>\n",
    "{assistant_response}\n",
    "</assistant_response>\n",
    "</submission>\"\"\"\n",
    "    \n",
    "        examples = \"\"\"Example 1:\n",
    "<submission>\n",
    "<user_prompt>Tell me a joke about vegetables</user_prompt>\n",
    "<assistant_response>\n",
    "<think>Lettuce sounds like \"let us\". Party themes often involve wordplay.</think>\n",
    "<answer>Why did the lettuce win the party contest? Because it was a real head of the celebration!</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "<analysis>Basic \"lettuce\" pun matches the food request but uses an overused format. <think> shows minimal effort to connect vegetables with celebrations.</analysis>\n",
    "<score>\n",
    "Wordplay: 2/3 (simple but functional pun)\n",
    "Originality: 1/2 (common theme with slight twist)\n",
    "Surprise: 1/2 (predictable word substitution)\n",
    "Relevance: 1/3 (tangential connection to vegetables)\n",
    "Total: 5/10\n",
    "</score>\n",
    "\n",
    "Example 2:\n",
    "<submission>\n",
    "<user_prompt>Looking for some animal-themed humor - what's your best joke about animals or pets?</user_prompt>\n",
    "<assistant_response>\n",
    "<think>Shoes need soles for walking. Therapy helps with loss.</think>\n",
    "<answer>Why did the shoe need therapy? It lost its sole!</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "<analysis>Irrelevant to user's request. <think> about shoes doesn't connect to requested animal joke.</analysis>\n",
    "<score>\n",
    "Wordplay: 1/3\n",
    "Originality: 0/2\n",
    "Surprise: 1/2\n",
    "Relevance: 0/3\n",
    "Total: 2/10\n",
    "</score>\n",
    "\n",
    "Example 3:\n",
    "<submission>\n",
    "<user_prompt>yo can u giv me programing joke rn??? need 2 make my team lugh</user_prompt>\n",
    "<assistant_response>\n",
    "<think>Combining programming concepts of debugging with literal bugs. Using the dual meaning of 'debug' to create a workplace scenario where debugging takes on a literal meaning.</think>\n",
    "<answer>Why did the programmer bring a bug to work? Because it was a debug session!</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "<analysis>Good programming context with clever wordplay on 'debug'. The <think> shows clear intention to combine literal and technical meanings. However, the execution is somewhat predictable and follows a common joke structure. The punchline, while relevant, doesn't fully maximize the surprise potential of the setup.</analysis>\n",
    "<score>\n",
    "Wordplay: 2/3 (solid use of 'debug' double meaning)\n",
    "Originality: 1/2 (familiar debugging theme)\n",
    "Surprise: 1/2 (predictable punchline)\n",
    "Relevance: 3/3 (directly addresses programming context)\n",
    "Total: 7/10\n",
    "</score>\n",
    "\n",
    "Example 4:\n",
    "<submission>\n",
    "<user_prompt>I need a chemistry joke for my science class presentation tomorrow.</user_prompt>\n",
    "<assistant_response>\n",
    "<think>Birds are funny when they walk.</think>\n",
    "<answer>Why did the chicken cross the playground? To get to the other slide!</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "<analysis>Completely misses the mark for a chemistry joke. The <think> shows no connection to chemistry or science, instead defaulting to a generic playground variation of the classic chicken joke. Neither the setup nor punchline attempts to incorporate any chemistry concepts.</analysis>\n",
    "<score>\n",
    "Wordplay: 0/3 (no chemistry-related wordplay)\n",
    "Originality: 0/2 (modifies an overused joke format)\n",
    "Surprise: 0/2 (predictable playground pun)\n",
    "Relevance: 0/3 (entirely unrelated to chemistry request)\n",
    "Total: 0/10\n",
    "</score>\n",
    "\n",
    "Example 5:\n",
    "<submission>\n",
    "<user_prompt>My kid loves vegetables and jokes. Do you know any veggie jokes that would make them laugh?</user_prompt>\n",
    "<assistant_response>\n",
    "<think>Combining asparagus's unique smell effect on urine with workplace humor. Using scientific fact for unexpected professional context. Creating tension between formal meeting setting and biological reality.</think>\n",
    "<answer>What vegetable holds the shortest workplace meetings? Asparagus, because everyone's in a rush to go!</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "<analysis>Creative integration of asparagus's biological effect into a professional context. The <think> demonstrates sophisticated layering of scientific fact with situational humor. While potentially crude, it cleverly avoids explicit reference while maintaining clear understanding. Original approach to vegetable humor beyond simple puns.</analysis>\n",
    "<score>\n",
    "Wordplay: 1/3 (relies more on situation than wordplay)\n",
    "Originality: 2/2 (unique combination of contexts)\n",
    "Surprise: 2/2 (unexpected professional setting twist)\n",
    "Relevance: 1/3 (somewhat forced vegetable connection)\n",
    "Total: 6/10\n",
    "</score>\"\"\"\n",
    "    \n",
    "        return f\"\"\"Evaluate this submission. First analyse <think> and <answer>, then score:\n",
    "\n",
    "Submission to Evaluate:\n",
    "{submission}\n",
    "\n",
    "Follow this structure EXACTLY:\n",
    "<analysis>Your critique</analysis>\n",
    "<score>...</score>\n",
    "\n",
    "Examples of how to evaluate the submission and format your response:\n",
    "{examples}\"\"\"\n",
    "\n",
    "    def score_submission(self, user_prompt: str, assistant_response: str) -> int:\n",
    "        \"\"\"\n",
    "        Given a user prompt and the assistant's response, this function generates a score\n",
    "        from 0 to 10 using litellm. If the returned total score is 10, subtract 1 to yield 9.\n",
    "        \"\"\"\n",
    "        system_prompt = self.generate_system_prompt()\n",
    "        user_message = self.generate_prompt_with_examples(user_prompt, assistant_response)\n",
    "        \n",
    "        completion_params = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ],\n",
    "            \"temperature\": self.temperature,\n",
    "            \"timeout\": 60,  # 60 seconds timeout\n",
    "        }\n",
    "        \n",
    "        response = litellm.completion(**completion_params)\n",
    "        try:\n",
    "            # Assuming a response structure similar to TogetherAI:\n",
    "            result_text = response.choices[0].message.content\n",
    "        except (AttributeError, IndexError):\n",
    "            result_text = response\n",
    "        \n",
    "        # Extract the \"Total: T/10\" part using regex.\n",
    "        match = re.search(r\"Total:\\s*([0-9]+(?:\\.[0-9]+)?)/10\", result_text)\n",
    "        if match:\n",
    "            total_score = float(match.group(1))\n",
    "        else:\n",
    "            raise ValueError(\"Could not extract total score from response.\")\n",
    "        \n",
    "        # If the total score is 10, subtract 1 to return 9.\n",
    "        if total_score >= 10:\n",
    "            total_score = 9.0\n",
    "        \n",
    "        # Optionally, round to nearest integer or keep as float.\n",
    "        return int(round(total_score))\n",
    "\n",
    "# Create an instance using a model name\n",
    "# \"gemini/gemini-2.0-flash-exp\"\n",
    "scorer = JokeScorer(model_name=\"gemini/gemini-2.0-flash-exp\", temperature=0.3)\n",
    "\n",
    "test_user_prompt = \"Tell me a joke about programming.\"\n",
    "test_assistant_response = \"\"\"<think>For programming jokes, let's explore the relationship between code and its execution. Developers often struggle with errors. \"Bugs\" are literal (programming errors) and figurative (insects). A funny scenario could be a bug (insect) causing a code bug (error). Adding a twist where the bug fixes the code is funny because it subverts expectations of bugs being problematic.</think>\n",
    "<answer>Why did the programmer bring a bug to work? Because it was a code bug fix!</answer>\"\"\"\n",
    "score = 7 # scorer.score_submission(test_user_prompt, test_assistant_response)\n",
    "print(\"Extracted Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a424ba79-ab20-4d26-9590-4a6c595d4b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "input_path = '../data/generated_data.jsonl'\n",
    "output_path = '../data/processed_data.jsonl'\n",
    "\n",
    "with open(input_path, 'r', encoding='utf-8') as infile, \\\n",
    "     open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "\n",
    "    for line in infile:\n",
    "        entry = json.loads(line.strip())\n",
    "        \n",
    "        # Check if the entry meets the criteria\n",
    "        if (entry.get('format_score') == 0 and \n",
    "            entry.get('generator_model') in ['ollama/deepseek-r1:7b', 'ollama/deepseek-r1:1.5b']):\n",
    "            \n",
    "            assistant_response = entry.get('assistant_response', '')\n",
    "            \n",
    "            if '</think>' in assistant_response and '<answer>' not in assistant_response:\n",
    "                # Split response into think section and potential answer\n",
    "                parts = re.split(r'(</think>\\n*)', assistant_response, 1, flags=re.DOTALL)\n",
    "                \n",
    "                if len(parts) >= 3:\n",
    "                    # Reconstruct with answer tags around the non-think portion\n",
    "                    reconstructed = parts[0] + parts[1] + f\"<answer>{parts[2].strip()}</answer>\"\n",
    "                    entry['assistant_response'] = reconstructed\n",
    "            else:\n",
    "                # No think tags found, leave as is\n",
    "                pass\n",
    "        \n",
    "        # Write the modified or original entry to output\n",
    "        outfile.write(json.dumps(entry) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68e32b5a-c163-4f1c-82c6-786b0af06bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed prompt50: I want a joke about Christmas!... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt52: Tell me a joke involving fruit... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt54: Give me a joke, but make it Sh... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt56: Make me laugh, but use sarcasm... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 5\n",
      "Processed prompt58: A joke about teachers, but mak... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt60: I need a joke about planes and... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt62: Give me a silly joke about cow... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt64: Tell me a joke about space tra... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt66: My friend loves golf. What’s a... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 6\n",
      "Processed prompt68: I need a lawyer joke, but make... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt70: A joke about bicycles, please.... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt72: Tell me a knock-knock joke wit... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt74: Joke about dinosaurs. Go!... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt76: A dad joke so bad it’s good.... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 6\n",
      "Processed prompt78: A joke about coffee, please!... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt80: Do you have a joke about dance... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt82: Do you have a joke about food ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt84: Tell me a joke about food blog... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt86: Tell me a joke about dance eco... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt88: tell me a funny joke about mot... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt90: need a joke about teenagers al... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt92: looking for a joke about how w... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 5\n",
      "Processed prompt94: hit me with a joke about mille... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt96: need a joke about how kids the... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt98: gimme a joke about cats acting... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt100: tell me a joke about engineers... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt102: Can you make a joke about how ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt104: need a clean joke about Finnis... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt106: looking for a joke about Afric... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt108: need a funny but respectful jo... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt110: tell me a joke about how Serbi... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt112: got a joke about how Indian an... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt114: need a joke about how Latin Am... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt116: got any jokes about Italian PM... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt118: need a funny observation about... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt120: got anything funny about New Z... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt122: I need a clean but intelligent... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt124: hey can u help me construct a ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt126: Working on new material for my... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt128: Writing material for a wedding... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 6\n",
      "Processed prompt130: Developing a tight 10 for my H... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt132: hello dear sir, i am from pola... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt134: Dear respected AI, I wanting g... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 5\n",
      "Processed prompt136: bonjour! i am french baker who... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt138: hello my friend!! i from niger... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt140: guten tag AI! german mechanic ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt142: oi mate, need a proper joke ab... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt144: ciao! looking for funny joke a... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt146: ehh I need ze joke about 'ow z... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt148: oi need somethin funny bout ho... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt150: looking for a joke about frenc... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt152: want a proper laugh about how ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt154: yo need a joke for my japanese... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt156: bruh my african grandparents a... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt158: my mexican abuela is turning 8... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt160: my russian babushka is visitin... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt162: help a brother out - speaking ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt164: gotta make arab grandparents s... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt166: my greek yiayia wants me to be... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 6\n",
      "Processed prompt168: need a science pickup line for... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt170: yo there's this girl in my ast... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt172: met someone at a gaming conven... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt174: this personal trainer at my gy... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt176: cute plant shop worker... need... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt178: the new vet tech is adorable, ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt180: cute musician in my building, ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt182: help! crush works at tech star... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt184: bartender at my local is super... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt186: matched with someone who's rea... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt188: give me something funny about ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt190: need a joke about Arsenal tryi... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt192: looking for jokes about Liverp... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt194: help me roast these Manchester... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt196: need to make fun of Tottenham'... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt198: need something about Browns fa... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt200: want a joke about Eagles fans ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt202: help me roast Jets fans who ke... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt204: need a joke about how brummies... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt206: help me make fun of how aussie... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt208: gimme something about how scou... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt210: need something funny bout how ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt212: help me roast boston people wh... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt214: want something funny about how... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt216: need something about how geord... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt218: want something about how new o... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt220: tell me something funny about ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 6\n",
      "Processed prompt222: looking for something about ho... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt224: want something funny about how... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt226: need a joke about how american... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt228: gimme a joke about how austral... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt230: need a joke about how politici... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt232: gimme a joke about how parliam... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt234: want a joke about how every ca... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt236: need a joke about how every po... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt238: want a joke about how every po... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt240: Headlining at Shinjuku Comedy ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt242: Yo, headlining at Comedy Cella... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt244: Need fresh material for my Soh... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt246: Prepping for Roppongi Comedy N... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt248: Hosting Late Night at Leiceste... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "from datetime import datetime\n",
    "\n",
    "# Create a data folder if it doesn't exist.\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\")\n",
    "\n",
    "# File paths for user prompts and generated data.\n",
    "USER_PROMPTS_FILE = \"../data/humor_user_prompts.jsonl\"\n",
    "OUTPUT_FILE = \"../data/generated_data.jsonl\"\n",
    "\n",
    "# --- Define a function to load prompts from a JSONL file ---\n",
    "def load_user_prompts(filename: str):\n",
    "    prompts = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                obj = json.loads(line.strip())\n",
    "                if \"prompt\" in obj:\n",
    "                    prompts.append(obj[\"prompt\"])\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return prompts\n",
    "\n",
    "# --- Example list of generator models ---\n",
    "generator_models = [\n",
    "    # \"mistral/mistral-small-latest\",\n",
    "    \"together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
    "    # \"openai/gpt-4o\"\n",
    "    # \"together_ai/google/gemma-2b-it\"\n",
    "    # \"ollama/qwen:1.8b\"\n",
    "    \n",
    "    # Reasoning models\n",
    "    # \"groq/deepseek-r1-distill-llama-70b\"\n",
    "    # \"together_ai/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "    # \"ollama/deepseek-r1:1.5b\"\n",
    "    # \"ollama/deepseek-r1:7b\"\n",
    "]\n",
    "\n",
    "# Define the scorer model name (used by the JokeScorer) and instantiate it.\n",
    "scorer_model_name = \"openai/gpt-4o\"\n",
    "task_type = \"humor\"\n",
    "joke_scorer = JokeScorer(model_name=scorer_model_name, temperature=0.3)\n",
    "\n",
    "# Instantiate the FormatScorer (for format checking)\n",
    "format_scorer = FormatScorer()\n",
    "\n",
    "# --- Main loop: For each prompt and for each generation model, generate a joke, evaluate it, and append the result ---\n",
    "def generate_and_score():\n",
    "    # Load user prompts (each line in the file should be a JSON object with a \"prompt\" field)\n",
    "    user_prompts = load_user_prompts(USER_PROMPTS_FILE)\n",
    "    \n",
    "    i = 0\n",
    "    for prompt in user_prompts:\n",
    "        for gen_model in generator_models:\n",
    "            i += 1\n",
    "            if i < 50 or i % 2 == 0:\n",
    "                continue\n",
    "            # Create a JokeGenerator instance for this generation model.\n",
    "            joke_generator = UniversalGenerator(model_name=gen_model, temperature=0.7)\n",
    "            # joke_generator = JokeGenerator(model_name=gen_model, temperature=0.7)\n",
    "            \n",
    "            # Generate the joke for the given prompt.\n",
    "            system_prompt = \"Generate humorous responses tailored to each user's unique request by analyzing their stated context, preferred tone, and implied audience. Please think step by step before to produce the final joke.\"\n",
    "            assistant_response = joke_generator.generate(system_prompt=system_prompt, user_prompt=prompt)\n",
    "            # assistant_response = joke_generator.generate_joke(prompt)\n",
    "\n",
    "            # Compute the format score using the FormatScorer.\n",
    "            fmt_score = format_scorer.score(assistant_response)\n",
    "            \n",
    "            # Compute the joke score using the JokeScorer.\n",
    "            # If scoring fails, we set the joke score to None.\n",
    "            try:\n",
    "                j_score = joke_scorer.score_submission(prompt, assistant_response)\n",
    "            except Exception as e:\n",
    "                print(f\"Scoring failed for prompt: {prompt} with error: {e}\")\n",
    "                j_score = None\n",
    "            \n",
    "            # Create a record with all the relevant information.\n",
    "            record = {\n",
    "                \"timestamp\": datetime.utcnow().isoformat(),\n",
    "                \"user_prompt\": prompt,\n",
    "                \"assistant_response\": assistant_response,\n",
    "                \"format_score\": fmt_score,\n",
    "                \"joke_score\": j_score,\n",
    "                \"generator_model\": gen_model,\n",
    "                \"scorer_model\": scorer_model_name,\n",
    "                \"task_type\": task_type\n",
    "            }\n",
    "            \n",
    "            # Append the record as a new line in the output ljson file.\n",
    "            with open(OUTPUT_FILE, \"a\") as out_f:\n",
    "                out_f.write(json.dumps(record) + \"\\n\")\n",
    "            \n",
    "            # Optionally, print a status update.\n",
    "            print(f\"Processed prompt{i}: {prompt[:30]}... using model: {gen_model}. Score: {j_score}\")\n",
    "\n",
    "# Run the generation-and-scoring function (one generation at a time).\n",
    "generate_and_score()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4d0b0e-b8a4-40ab-a434-1d55b161d414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
