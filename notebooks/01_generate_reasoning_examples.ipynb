{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d17ceba-ea86-418b-8cbb-ce4e60925cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: together in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (1.3.14)\n",
      "Requirement already satisfied: litellm in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (1.43.9)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.3 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (3.10.3)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.7 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (8.1.7)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.1.3 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (0.2.2)\n",
      "Requirement already satisfied: filelock<4.0.0,>=3.13.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (1.26.4)\n",
      "Requirement already satisfied: pillow<11.0.0,>=10.3.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (10.4.0)\n",
      "Requirement already satisfied: pyarrow>=10.0.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (17.0.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.6.3 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (2.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (2.32.3)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.8.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (13.9.4)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.2 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (4.66.5)\n",
      "Requirement already satisfied: typer<0.16,>=0.9 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from together) (0.12.3)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (7.2.1)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (3.1.4)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (4.23.0)\n",
      "Requirement already satisfied: openai>=1.40.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (1.40.6)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (1.0.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (0.7.0)\n",
      "Requirement already satisfied: tokenizers in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from litellm) (0.20.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.9.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from importlib-metadata>=6.8.0->litellm) (3.20.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from jinja2<4.0.0,>=3.1.2->litellm) (2.1.5)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.20.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.40.0->litellm) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.40.0->litellm) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.40.0->litellm) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.40.0->litellm) (0.5.0)\n",
      "Requirement already satisfied: sniffio in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.40.0->litellm) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from openai>=1.40.0->litellm) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.6.3->together) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.6.3->together) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->together) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->together) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->together) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->together) (2024.7.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from rich<14.0.0,>=13.8.1->together) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from rich<14.0.0,>=13.8.1->together) (2.18.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from tiktoken>=0.7.0->litellm) (2024.7.24)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from typer<0.16,>=0.9->together) (1.5.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from tokenizers->litellm) (0.24.5)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai>=1.40.0->litellm) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.40.0->litellm) (0.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (6.0.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/isavita/.pyenv/versions/3.11.7/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.8.1->together) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "# Install libraries\n",
    "!pip install together litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06cd5ea4-214d-4186-8e3e-14f2d300f423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed.\n"
     ]
    }
   ],
   "source": [
    "# Formatting Scorer Class with Tests\n",
    "import re\n",
    "\n",
    "class FormatScorer:\n",
    "    \"\"\"\n",
    "    A simple scoring class that verifies whether the given text contains\n",
    "    the required tags: <think>, </think>, <answer>, and </answer>.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Pre-compile regex patterns for faster matching.\n",
    "        self.think_open = re.compile(r'<think>')\n",
    "        self.think_close = re.compile(r'</think>')\n",
    "        self.answer_open = re.compile(r'<answer>')\n",
    "        self.answer_close = re.compile(r'</answer>')\n",
    "        \n",
    "    def score(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Returns 1 if the text contains both <think>...</think> and \n",
    "        <answer>...</answer> tags; otherwise returns 0.\n",
    "        \"\"\"\n",
    "        if (self.think_open.search(text) is not None and\n",
    "            self.think_close.search(text) is not None and\n",
    "            self.answer_open.search(text) is not None and\n",
    "            self.answer_close.search(text) is not None):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "# --- Tests for the FormatScorer class ---\n",
    "\n",
    "def run_tests():\n",
    "    scorer = FormatScorer()\n",
    "    \n",
    "    # Test 1: No tags at all should return 0.\n",
    "    text1 = \"Hello, world!\"\n",
    "    assert scorer.score(text1) == 0, \"Test case 1 failed: expected score 0.\"\n",
    "    \n",
    "    # Test 2: Only <think> tags are present.\n",
    "    text2 = \"<think>This is a thought</think> Some text without answer tags.\"\n",
    "    assert scorer.score(text2) == 0, \"Test case 2 failed: expected score 0.\"\n",
    "    \n",
    "    # Test 3: Both <think> and <answer> tags are present.\n",
    "    text3 = \"<think>This is a thought</think><answer>This is an answer</answer>\"\n",
    "    assert scorer.score(text3) == 1, \"Test case 3 failed: expected score 1.\"\n",
    "    \n",
    "    # Test 4: Malformed text (missing closing tag for <answer>).\n",
    "    text4 = \"<think>This is a thought</think><answer>This is an answer\"\n",
    "    assert scorer.score(text4) == 0, \"Test case 4 failed: expected score 0.\"\n",
    "    \n",
    "    print(\"All tests passed.\")\n",
    "\n",
    "# Run the tests:\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "102caf32-4e64-4629-8946-812dc193af8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Joke:\n",
      " <think>For programming jokes, let's explore the relationship between code and its execution. Developers often struggle with errors. \"Bugs\" are literal (programming errors) and figurative (insects). A funny scenario could be a bug (insect) causing a code bug (error). Adding a twist where the bug fixes the code is funny because it subverts expectations of bugs being problematic.</think>\n",
      "<answer>Why did the programmer bring a bug to work? Because it was a code bug fix!</answer>\n"
     ]
    }
   ],
   "source": [
    "# Joke Generator\n",
    "import litellm\n",
    "\n",
    "class JokeGenerator:\n",
    "    def __init__(self, model_name: str, temperature: float = 0.7):\n",
    "        \"\"\"\n",
    "        Initialize the JokeGenerator with a specific model name and temperature.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_prompt_with_examples(prompt: str) -> str:\n",
    "        examples = \"\"\"Example 1:\n",
    "<think>I want to create anatomy humor. Skeletons are inherently funny because they're literal \"bare bones.\" What do they lack? Flesh/organs. \"Guts\" has a double meaning - both literal organs and figurative courage. This sets up a pun opportunity.</think>\n",
    "<answer>Why don't skeletons fight each other? They don't have the guts!</answer>\n",
    "\n",
    "Example 2:\n",
    "<think>Tech support jokes work well with personification. \"Doctor\" visits imply sickness. Common computer issues include viruses. Let's combine these - a computer catching a \"virus\" works literally (tech) and metaphorically (biology). Adds irony since computers are supposed to fix problems, not have them.</think>\n",
    "<answer>Why did the computer go to the doctor? It had a virus!</answer>\n",
    "\n",
    "Example 3:\n",
    "<think>Science humor opportunity. Atoms are fundamental but abstract. \"Make up everything\" has dual meaning - literal composition vs deception. Personifying atoms as untrustworthy creates surprise. Bonus science nod to their constant motion/making bonds.</think>\n",
    "<answer>Why don't scientists trust atoms? Because they make up everything!</answer>\"\"\"\n",
    "        return f\"\"\"You are an AI assistant that produces jokes. You should think about the joke first, then produce it.\n",
    "You should use the <think> tag to think about the joke, and the <answer> tag to produce the joke.\n",
    "Do not use any other tags or anything else in your response.\n",
    "\n",
    "Here are some examples of jokes:\n",
    "<examples>\n",
    "{examples}\n",
    "</examples>\n",
    "\n",
    "Now, produce a joke for the following prompt:\n",
    "{prompt}\n",
    "\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_system_prompt() -> str:\n",
    "        return \"\"\"You are an AI assistant that produces jokes. You should think about the joke first, then produce it.\n",
    "You should use the <think> tag to think about the joke, and the <answer> tag to produce the joke.\n",
    "Do not use any other tags or anything else in your response.\n",
    "\"\"\"\n",
    "\n",
    "    def generate_joke(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a joke for the given prompt using litellm.completion.\n",
    "        \"\"\"\n",
    "        system_prompt = self.generate_system_prompt()\n",
    "        user_prompt = self.generate_prompt_with_examples(prompt)\n",
    "        \n",
    "        # Build completion parameters as desired\n",
    "        completion_params = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            \"temperature\": self.temperature,\n",
    "            \"timeout\": 60,  # timeout in seconds\n",
    "        }\n",
    "        \n",
    "        response = litellm.completion(**completion_params)\n",
    "        \n",
    "        try:\n",
    "            joke = response.choices[0].message.content\n",
    "        except (AttributeError, IndexError):\n",
    "            joke = response\n",
    "        return joke\n",
    "\n",
    "\n",
    "# Create an instance using a model name\n",
    "generator = JokeGenerator(model_name=\"mistral/mistral-small-latest\")\n",
    "\n",
    "# Define a sample prompt to generate a joke.\n",
    "sample_prompt = \"Tell me a joke about programming.\"\n",
    "\n",
    "# Generate a joke.\n",
    "joke_output = \"\"\"<think>For programming jokes, let's explore the relationship between code and its execution. Developers often struggle with errors. \"Bugs\" are literal (programming errors) and figurative (insects). A funny scenario could be a bug (insect) causing a code bug (error). Adding a twist where the bug fixes the code is funny because it subverts expectations of bugs being problematic.</think>\n",
    "<answer>Why did the programmer bring a bug to work? Because it was a code bug fix!</answer>\"\"\"\n",
    "# generator.generate_joke(sample_prompt)\n",
    "print(\"Generated Joke:\\n\", joke_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fab62af-9014-4df0-8122-aa8dc0c65836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Score: 7\n"
     ]
    }
   ],
   "source": [
    "# Joke Scorer\n",
    "import re\n",
    "import litellm\n",
    "\n",
    "class JokeScorer:\n",
    "    def __init__(self, model_name: str, temperature: float = 0.3):\n",
    "        \"\"\"\n",
    "        Initialize the JokeScorer with a specific model name and a default temperature of 0.3.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_system_prompt() -> str:\n",
    "        return \"\"\"You are a Joke Evaluation Expert. Analyse submissions using:\n",
    "    \n",
    "**Scoring Criteria (0-10 Total)**:\n",
    "1. 🎭 Wordplay (0-3): Pun/double-meaning quality in <answer>\n",
    "2. 💡 Originality (0-2): Novelty of <think> and <answer>\n",
    "3. 🎉 Surprise (0-2): Unexpected twist effectiveness\n",
    "4. 🔗 Relevance (0-3): Alignment with user request\n",
    "\n",
    "**Submission Format**:\n",
    "<submission>\n",
    "<user_prompt>[Original user request]</user_prompt>\n",
    "<assistant_response>\n",
    "<think>[Creator's reasoning]</think>\n",
    "<answer>[Joke text]</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "\n",
    "**Output Format**:\n",
    "<analysis>[Your evaluation of <think> and <answer> and the relevance to the user prompt]</analysis>\n",
    "<score>\n",
    "Wordplay: X/3\n",
    "Originality: Y/2\n",
    "Surprise: Z/2\n",
    "Relevance: W/3\n",
    "Total: T/10\n",
    "</score>\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_prompt_with_examples(user_prompt: str, assistant_response: str) -> str:\n",
    "        submission = f\"\"\"<submission>\n",
    "<user_prompt>{user_prompt}</user_prompt>\n",
    "<assistant_response>\n",
    "{assistant_response}\n",
    "</assistant_response>\n",
    "</submission>\"\"\"\n",
    "    \n",
    "        examples = \"\"\"Example 1:\n",
    "<submission>\n",
    "<user_prompt>Tell me a joke about vegetables</user_prompt>\n",
    "<assistant_response>\n",
    "<think>Lettuce sounds like \"let us\". Party themes often involve wordplay.</think>\n",
    "<answer>Why did the lettuce win the party contest? Because it was a real head of the celebration!</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "<analysis>Basic \"lettuce\" pun matches the food request but uses an overused format. <think> shows minimal effort to connect vegetables with celebrations.</analysis>\n",
    "<score>\n",
    "Wordplay: 2/3 (simple but functional pun)\n",
    "Originality: 1/2 (common theme with slight twist)\n",
    "Surprise: 1/2 (predictable word substitution)\n",
    "Relevance: 1/3 (tangential connection to vegetables)\n",
    "Total: 5/10\n",
    "</score>\n",
    "\n",
    "Example 2:\n",
    "<submission>\n",
    "<user_prompt>Looking for some animal-themed humor - what's your best joke about animals or pets?</user_prompt>\n",
    "<assistant_response>\n",
    "<think>Shoes need soles for walking. Therapy helps with loss.</think>\n",
    "<answer>Why did the shoe need therapy? It lost its sole!</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "<analysis>Irrelevant to user's request. <think> about shoes doesn't connect to requested animal joke.</analysis>\n",
    "<score>\n",
    "Wordplay: 1/3\n",
    "Originality: 0/2\n",
    "Surprise: 1/2\n",
    "Relevance: 0/3\n",
    "Total: 2/10\n",
    "</score>\n",
    "\n",
    "Example 3:\n",
    "<submission>\n",
    "<user_prompt>yo can u giv me programing joke rn??? need 2 make my team lugh</user_prompt>\n",
    "<assistant_response>\n",
    "<think>Combining programming concepts of debugging with literal bugs. Using the dual meaning of 'debug' to create a workplace scenario where debugging takes on a literal meaning.</think>\n",
    "<answer>Why did the programmer bring a bug to work? Because it was a debug session!</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "<analysis>Good programming context with clever wordplay on 'debug'. The <think> shows clear intention to combine literal and technical meanings. However, the execution is somewhat predictable and follows a common joke structure. The punchline, while relevant, doesn't fully maximize the surprise potential of the setup.</analysis>\n",
    "<score>\n",
    "Wordplay: 2/3 (solid use of 'debug' double meaning)\n",
    "Originality: 1/2 (familiar debugging theme)\n",
    "Surprise: 1/2 (predictable punchline)\n",
    "Relevance: 3/3 (directly addresses programming context)\n",
    "Total: 7/10\n",
    "</score>\n",
    "\n",
    "Example 4:\n",
    "<submission>\n",
    "<user_prompt>I need a chemistry joke for my science class presentation tomorrow.</user_prompt>\n",
    "<assistant_response>\n",
    "<think>Birds are funny when they walk.</think>\n",
    "<answer>Why did the chicken cross the playground? To get to the other slide!</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "<analysis>Completely misses the mark for a chemistry joke. The <think> shows no connection to chemistry or science, instead defaulting to a generic playground variation of the classic chicken joke. Neither the setup nor punchline attempts to incorporate any chemistry concepts.</analysis>\n",
    "<score>\n",
    "Wordplay: 0/3 (no chemistry-related wordplay)\n",
    "Originality: 0/2 (modifies an overused joke format)\n",
    "Surprise: 0/2 (predictable playground pun)\n",
    "Relevance: 0/3 (entirely unrelated to chemistry request)\n",
    "Total: 0/10\n",
    "</score>\n",
    "\n",
    "Example 5:\n",
    "<submission>\n",
    "<user_prompt>My kid loves vegetables and jokes. Do you know any veggie jokes that would make them laugh?</user_prompt>\n",
    "<assistant_response>\n",
    "<think>Combining asparagus's unique smell effect on urine with workplace humor. Using scientific fact for unexpected professional context. Creating tension between formal meeting setting and biological reality.</think>\n",
    "<answer>What vegetable holds the shortest workplace meetings? Asparagus, because everyone's in a rush to go!</answer>\n",
    "</assistant_response>\n",
    "</submission>\n",
    "<analysis>Creative integration of asparagus's biological effect into a professional context. The <think> demonstrates sophisticated layering of scientific fact with situational humor. While potentially crude, it cleverly avoids explicit reference while maintaining clear understanding. Original approach to vegetable humor beyond simple puns.</analysis>\n",
    "<score>\n",
    "Wordplay: 1/3 (relies more on situation than wordplay)\n",
    "Originality: 2/2 (unique combination of contexts)\n",
    "Surprise: 2/2 (unexpected professional setting twist)\n",
    "Relevance: 1/3 (somewhat forced vegetable connection)\n",
    "Total: 6/10\n",
    "</score>\"\"\"\n",
    "    \n",
    "        return f\"\"\"Evaluate this submission. First analyse <think> and <answer>, then score:\n",
    "\n",
    "Submission to Evaluate:\n",
    "{submission}\n",
    "\n",
    "Follow this structure EXACTLY:\n",
    "<analysis>Your critique</analysis>\n",
    "<score>...</score>\n",
    "\n",
    "Examples of how to evaluate the submission and format your response:\n",
    "{examples}\"\"\"\n",
    "\n",
    "    def score_submission(self, user_prompt: str, assistant_response: str) -> int:\n",
    "        \"\"\"\n",
    "        Given a user prompt and the assistant's response, this function generates a score\n",
    "        from 0 to 10 using litellm. If the returned total score is 10, subtract 1 to yield 9.\n",
    "        \"\"\"\n",
    "        system_prompt = self.generate_system_prompt()\n",
    "        user_message = self.generate_prompt_with_examples(user_prompt, assistant_response)\n",
    "        \n",
    "        completion_params = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ],\n",
    "            \"temperature\": self.temperature,\n",
    "            \"timeout\": 60,  # 60 seconds timeout\n",
    "        }\n",
    "        \n",
    "        response = litellm.completion(**completion_params)\n",
    "        try:\n",
    "            # Assuming a response structure similar to TogetherAI:\n",
    "            result_text = response.choices[0].message.content\n",
    "        except (AttributeError, IndexError):\n",
    "            result_text = response\n",
    "        \n",
    "        # Extract the \"Total: T/10\" part using regex.\n",
    "        match = re.search(r\"Total:\\s*([0-9]+(?:\\.[0-9]+)?)/10\", result_text)\n",
    "        if match:\n",
    "            total_score = float(match.group(1))\n",
    "        else:\n",
    "            raise ValueError(\"Could not extract total score from response.\")\n",
    "        \n",
    "        # If the total score is 10, subtract 1 to return 9.\n",
    "        if total_score >= 10:\n",
    "            total_score = 9.0\n",
    "        \n",
    "        # Optionally, round to nearest integer or keep as float.\n",
    "        return int(round(total_score))\n",
    "\n",
    "# Create an instance using a model name\n",
    "# \"gemini/gemini-2.0-flash-exp\"\n",
    "scorer = JokeScorer(model_name=\"gemini/gemini-2.0-flash-exp\", temperature=0.3)\n",
    "\n",
    "test_user_prompt = \"Tell me a joke about programming.\"\n",
    "test_assistant_response = \"\"\"<think>For programming jokes, let's explore the relationship between code and its execution. Developers often struggle with errors. \"Bugs\" are literal (programming errors) and figurative (insects). A funny scenario could be a bug (insect) causing a code bug (error). Adding a twist where the bug fixes the code is funny because it subverts expectations of bugs being problematic.</think>\n",
    "<answer>Why did the programmer bring a bug to work? Because it was a code bug fix!</answer>\"\"\"\n",
    "score = 7 # scorer.score_submission(test_user_prompt, test_assistant_response)\n",
    "print(\"Extracted Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68e32b5a-c163-4f1c-82c6-786b0af06bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed prompt: Need jokez abot penguins pls!... using model: mistral/mistral-small-latest. Score: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/pydantic/main.py:390: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `PromptTokensDetails` but got `dict` with value `{'audio_tokens': 0, 'cached_tokens': 0}` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed prompt: Need jokez abot penguins pls!... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: Need jokez abot penguins pls!... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: Could you kindly compose a hum... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: Could you kindly compose a hum... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: Could you kindly compose a hum... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: Funny cat joke needed ASAP!... using model: mistral/mistral-small-latest. Score: 6\n",
      "Processed prompt: Funny cat joke needed ASAP!... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: Funny cat joke needed ASAP!... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: Joke about dentists that isn't... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: Joke about dentists that isn't... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: Joke about dentists that isn't... using model: openai/gpt-4o. Score: 6\n",
      "Processed prompt: yo hit me with a dad joke abou... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: yo hit me with a dad joke abou... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: yo hit me with a dad joke abou... using model: openai/gpt-4o. Score: 4\n",
      "Processed prompt: Requesting a meteorological-th... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: Requesting a meteorological-th... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: Requesting a meteorological-th... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: Grandma wants clean kitchen jo... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: Grandma wants clean kitchen jo... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: Grandma wants clean kitchen jo... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: Humorous analogy between block... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: Humorous analogy between block... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: Humorous analogy between block... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: jooke abt space pls (5yr old b... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: jooke abt space pls (5yr old b... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: jooke abt space pls (5yr old b... using model: openai/gpt-4o. Score: 5\n",
      "Processed prompt: As a senior librarian, I requi... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: As a senior librarian, I requi... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 6\n",
      "Processed prompt: As a senior librarian, I requi... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: Funny farm animal pun for kind... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: Funny farm animal pun for kind... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: Funny farm animal pun for kind... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: JOKE NEEDED: Lawyer jokes but ... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: JOKE NEEDED: Lawyer jokes but ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: JOKE NEEDED: Lawyer jokes but ... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: Hilarious comparison between T... using model: mistral/mistral-small-latest. Score: 4\n",
      "Processed prompt: Hilarious comparison between T... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: Hilarious comparison between T... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: blblblbl joke about ducks?... using model: mistral/mistral-small-latest. Score: 4\n",
      "Processed prompt: blblblbl joke about ducks?... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: blblblbl joke about ducks?... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: Philosophy humor for Nietzsche... using model: mistral/mistral-small-latest. Score: 9\n",
      "Processed prompt: Philosophy humor for Nietzsche... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: Philosophy humor for Nietzsche... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: Construction worker needs clea... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: Construction worker needs clea... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: Construction worker needs clea... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: Jest regarding machine learnin... using model: mistral/mistral-small-latest. Score: 9\n",
      "Processed prompt: Jest regarding machine learnin... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: Jest regarding machine learnin... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: Funny haiku about procrastinat... using model: mistral/mistral-small-latest. Score: 9\n",
      "Processed prompt: Funny haiku about procrastinat... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: Funny haiku about procrastinat... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: Emergency! Need joke about WiF... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: Emergency! Need joke about WiF... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: Emergency! Need joke about WiF... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: Tell me a joke about programmi... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: Tell me a joke about programmi... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: Tell me a joke about programmi... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: hi can u giv me a funny joke 4... using model: mistral/mistral-small-latest. Score: 6\n",
      "Processed prompt: hi can u giv me a funny joke 4... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: hi can u giv me a funny joke 4... using model: openai/gpt-4o. Score: 4\n",
      "Processed prompt: As a physics professor, I'm lo... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: As a physics professor, I'm lo... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: As a physics professor, I'm lo... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: joke plz... using model: mistral/mistral-small-latest. Score: 4\n",
      "Processed prompt: joke plz... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: joke plz... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: I'm organizing a retirement pa... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: I'm organizing a retirement pa... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: I'm organizing a retirement pa... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: My 6-year-old is obsessed with... using model: mistral/mistral-small-latest. Score: 9\n",
      "Processed prompt: My 6-year-old is obsessed with... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: My 6-year-old is obsessed with... using model: openai/gpt-4o. Score: 5\n",
      "Processed prompt: Need a clean, work-appropriate... using model: mistral/mistral-small-latest. Score: 2\n",
      "Processed prompt: Need a clean, work-appropriate... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: Need a clean, work-appropriate... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: gimme ur best dad joke lol... using model: mistral/mistral-small-latest. Score: 5\n",
      "Processed prompt: gimme ur best dad joke lol... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: gimme ur best dad joke lol... using model: openai/gpt-4o. Score: 6\n",
      "Processed prompt: Looking for a clever mathemati... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: Looking for a clever mathemati... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: Looking for a clever mathemati... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: can u help me with a joke? my ... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: can u help me with a joke? my ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: can u help me with a joke? my ... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: Seeking a sophisticated culina... using model: mistral/mistral-small-latest. Score: 4\n",
      "Processed prompt: Seeking a sophisticated culina... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: Seeking a sophisticated culina... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: tell joke now... using model: mistral/mistral-small-latest. Score: 6\n",
      "Processed prompt: tell joke now... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: tell joke now... using model: openai/gpt-4o. Score: 5\n",
      "Processed prompt: I require assistance in crafti... using model: mistral/mistral-small-latest. Score: 9\n",
      "Processed prompt: I require assistance in crafti... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: I require assistance in crafti... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: bruh i need a joke for instagr... using model: mistral/mistral-small-latest. Score: 5\n",
      "Processed prompt: bruh i need a joke for instagr... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 5\n",
      "Processed prompt: bruh i need a joke for instagr... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: My grandmother is turning 90 a... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: My grandmother is turning 90 a... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: My grandmother is turning 90 a... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: hey AI! hit me with ur best ca... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: hey AI! hit me with ur best ca... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: hey AI! hit me with ur best ca... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: As the best man at my brother'... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: As the best man at my brother'... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: As the best man at my brother'... using model: openai/gpt-4o. Score: 6\n",
      "Processed prompt: got any jokes bout coffee? wor... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: got any jokes bout coffee? wor... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: got any jokes bout coffee? wor... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: In preparation for my doctoral... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: In preparation for my doctoral... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: In preparation for my doctoral... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: yo make me laugh... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: yo make me laugh... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: yo make me laugh... using model: openai/gpt-4o. Score: 4\n",
      "Processed prompt: I am organizing an internation... using model: mistral/mistral-small-latest. Score: 3\n",
      "Processed prompt: I am organizing an internation... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: I am organizing an internation... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: help! need funny joke 4 first ... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: help! need funny joke 4 first ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: help! need funny joke 4 first ... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: As a middle school science tea... using model: mistral/mistral-small-latest. Score: 6\n",
      "Processed prompt: As a middle school science tea... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: As a middle school science tea... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: make a joke about books - libr... using model: mistral/mistral-small-latest. Score: 6\n",
      "Processed prompt: make a joke about books - libr... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: make a joke about books - libr... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: Dear AI Assistant, I would gre... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: Dear AI Assistant, I would gre... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: Dear AI Assistant, I would gre... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: What’s the funniest pun you kn... using model: mistral/mistral-small-latest. Score: 4\n",
      "Processed prompt: What’s the funniest pun you kn... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: What’s the funniest pun you kn... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: Give me a joke about farmers.... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: Give me a joke about farmers.... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: Give me a joke about farmers.... using model: openai/gpt-4o. Score: 6\n",
      "Processed prompt: I want a joke about AI and Cha... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: I want a joke about AI and Cha... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: I want a joke about AI and Cha... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: Tell me an old-school joke fro... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: Tell me an old-school joke fro... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: Tell me an old-school joke fro... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: Got any gaming jokes? Somethin... using model: mistral/mistral-small-latest. Score: 9\n",
      "Processed prompt: Got any gaming jokes? Somethin... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: Got any gaming jokes? Somethin... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: I want a joke about Christmas!... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: I want a joke about Christmas!... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: I want a joke about Christmas!... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: What’s a good Thanksgiving jok... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: What’s a good Thanksgiving jok... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 5\n",
      "Processed prompt: What’s a good Thanksgiving jok... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: Tell me a joke involving fruit... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: Tell me a joke involving fruit... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: Tell me a joke involving fruit... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: A joke about dogs, please!... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: A joke about dogs, please!... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: A joke about dogs, please!... using model: openai/gpt-4o. Score: 5\n",
      "Processed prompt: Give me a joke, but make it Sh... using model: mistral/mistral-small-latest. Score: 9\n",
      "Processed prompt: Give me a joke, but make it Sh... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: Give me a joke, but make it Sh... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: What's a funny way to explain ... using model: mistral/mistral-small-latest. Score: 6\n",
      "Processed prompt: What's a funny way to explain ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: What's a funny way to explain ... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: Make me laugh, but use sarcasm... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: Make me laugh, but use sarcasm... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: Make me laugh, but use sarcasm... using model: openai/gpt-4o. Score: 5\n",
      "Processed prompt: Tell me a joke about dentists.... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: Tell me a joke about dentists.... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: Tell me a joke about dentists.... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: A joke about teachers, but mak... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: A joke about teachers, but mak... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: A joke about teachers, but mak... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: Give me a joke about gym worko... using model: mistral/mistral-small-latest. Score: 9\n",
      "Processed prompt: Give me a joke about gym worko... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: Give me a joke about gym worko... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: I need a joke about planes and... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: I need a joke about planes and... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 4\n",
      "Processed prompt: I need a joke about planes and... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: Tell me a terrible joke on pur... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: Tell me a terrible joke on pur... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: Tell me a terrible joke on pur... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: Give me a silly joke about cow... using model: mistral/mistral-small-latest. Score: 3\n",
      "Processed prompt: Give me a silly joke about cow... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: Give me a silly joke about cow... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: I want a joke with a surprise ... using model: mistral/mistral-small-latest. Score: 9\n",
      "Processed prompt: I want a joke with a surprise ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 6\n",
      "Processed prompt: I want a joke with a surprise ... using model: openai/gpt-4o. Score: 4\n",
      "Processed prompt: Tell me a joke about space tra... using model: mistral/mistral-small-latest. Score: 4\n",
      "Processed prompt: Tell me a joke about space tra... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: Tell me a joke about space tra... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: A joke about fishing, please!... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: A joke about fishing, please!... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: A joke about fishing, please!... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: My friend loves golf. What’s a... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: My friend loves golf. What’s a... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: My friend loves golf. What’s a... using model: openai/gpt-4o. Score: 6\n",
      "Processed prompt: A joke that’s funny and involv... using model: mistral/mistral-small-latest. Score: 4\n",
      "Processed prompt: A joke that’s funny and involv... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: A joke that’s funny and involv... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: I need a lawyer joke, but make... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: I need a lawyer joke, but make... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: I need a lawyer joke, but make... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: Give me an office-related joke... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: Give me an office-related joke... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: Give me an office-related joke... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: A joke about bicycles, please.... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: A joke about bicycles, please.... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: A joke about bicycles, please.... using model: openai/gpt-4o. Score: 6\n",
      "Processed prompt: Make me laugh with a joke abou... using model: mistral/mistral-small-latest. Score: 4\n",
      "Processed prompt: Make me laugh with a joke abou... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: Make me laugh with a joke abou... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: Tell me a knock-knock joke wit... using model: mistral/mistral-small-latest. Score: 3\n",
      "Processed prompt: Tell me a knock-knock joke wit... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: Tell me a knock-knock joke wit... using model: openai/gpt-4o. Score: 6\n",
      "Processed prompt: I need a pirate joke, but make... using model: mistral/mistral-small-latest. Score: 4\n",
      "Processed prompt: I need a pirate joke, but make... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: I need a pirate joke, but make... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: Joke about dinosaurs. Go!... using model: mistral/mistral-small-latest. Score: 9\n",
      "Processed prompt: Joke about dinosaurs. Go!... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: Joke about dinosaurs. Go!... using model: openai/gpt-4o. Score: 4\n",
      "Processed prompt: I love astronomy. Got a space ... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: I love astronomy. Got a space ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: I love astronomy. Got a space ... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: A dad joke so bad it’s good.... using model: mistral/mistral-small-latest. Score: 5\n",
      "Processed prompt: A dad joke so bad it’s good.... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: A dad joke so bad it’s good.... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: Make me laugh with a programmi... using model: mistral/mistral-small-latest. Score: 9\n",
      "Processed prompt: Make me laugh with a programmi... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: Make me laugh with a programmi... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: A joke about coffee, please!... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: A joke about coffee, please!... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: A joke about coffee, please!... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: Tell me a joke to impress my d... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: Tell me a joke to impress my d... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: Tell me a joke to impress my d... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: Do you have a joke about dance... using model: mistral/mistral-small-latest. Score: 4\n",
      "Processed prompt: Do you have a joke about dance... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: Do you have a joke about dance... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: Share a joke for a dance patro... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: Share a joke for a dance patro... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: Share a joke for a dance patro... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: Do you have a joke about food ... using model: mistral/mistral-small-latest. Score: 4\n",
      "Processed prompt: Do you have a joke about food ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: Do you have a joke about food ... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: Share a joke for a food review... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: Share a joke for a food review... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: Share a joke for a food review... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: Tell me a joke about food blog... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: Tell me a joke about food blog... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: Tell me a joke about food blog... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: I need a joke for a food blogg... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: I need a joke for a food blogg... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: I need a joke for a food blogg... using model: openai/gpt-4o. Score: 6\n",
      "Processed prompt: Tell me a joke about dance eco... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: Tell me a joke about dance eco... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: Tell me a joke about dance eco... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: bro need a joke about why my w... using model: mistral/mistral-small-latest. Score: 5\n",
      "Processed prompt: bro need a joke about why my w... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 4\n",
      "Processed prompt: bro need a joke about why my w... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: tell me a funny joke about mot... using model: mistral/mistral-small-latest. Score: 6\n",
      "Processed prompt: tell me a funny joke about mot... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: tell me a funny joke about mot... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: Got any jokes about how men ne... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: Got any jokes about how men ne... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: Got any jokes about how men ne... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: need a joke about teenagers al... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: need a joke about teenagers al... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 6\n",
      "Processed prompt: need a joke about teenagers al... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: gimme ur best 'why did the blo... using model: mistral/mistral-small-latest. Score: 4\n",
      "Processed prompt: gimme ur best 'why did the blo... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: gimme ur best 'why did the blo... using model: openai/gpt-4o. Score: 6\n",
      "Processed prompt: looking for a joke about how w... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: looking for a joke about how w... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 6\n",
      "Processed prompt: looking for a joke about how w... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: need a funny joke about dads a... using model: mistral/mistral-small-latest. Score: 5\n",
      "Processed prompt: need a funny joke about dads a... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: need a funny joke about dads a... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: hit me with a joke about mille... using model: mistral/mistral-small-latest. Score: 5\n",
      "Processed prompt: hit me with a joke about mille... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: hit me with a joke about mille... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: got any jokes about IT guys te... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: got any jokes about IT guys te... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: got any jokes about IT guys te... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: need a joke about how kids the... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: need a joke about how kids the... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: need a joke about how kids the... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: tell me smthing funny bout veg... using model: mistral/mistral-small-latest. Score: 5\n",
      "Processed prompt: tell me smthing funny bout veg... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: tell me smthing funny bout veg... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: gimme a joke about cats acting... using model: mistral/mistral-small-latest. Score: 6\n",
      "Processed prompt: gimme a joke about cats acting... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: gimme a joke about cats acting... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: need a lawyer joke - you know,... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: need a lawyer joke - you know,... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: need a lawyer joke - you know,... using model: openai/gpt-4o. Score: 5\n",
      "Processed prompt: tell me a joke about engineers... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: tell me a joke about engineers... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: tell me a joke about engineers... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: got any jokes about how grandp... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: got any jokes about how grandp... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: got any jokes about how grandp... using model: openai/gpt-4o. Score: 6\n",
      "Processed prompt: Can you make a joke about how ... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: Can you make a joke about how ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: Can you make a joke about how ... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: gimme a funny joke about AMLO'... using model: mistral/mistral-small-latest. Score: 6\n",
      "Processed prompt: gimme a funny joke about AMLO'... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: gimme a funny joke about AMLO'... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: need a clean joke about Finnis... using model: mistral/mistral-small-latest. Score: 4\n",
      "Processed prompt: need a clean joke about Finnis... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: need a clean joke about Finnis... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: tell me something funny about ... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: tell me something funny about ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: tell me something funny about ... using model: openai/gpt-4o. Score: 5\n",
      "Processed prompt: looking for a joke about Afric... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: looking for a joke about Afric... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: looking for a joke about Afric... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: got any jokes about Japanese p... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: got any jokes about Japanese p... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: got any jokes about Japanese p... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: need a funny but respectful jo... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: need a funny but respectful jo... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: need a funny but respectful jo... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: gimme ur best joke about Austr... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: gimme ur best joke about Austr... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: gimme ur best joke about Austr... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: tell me a joke about how Serbi... using model: mistral/mistral-small-latest. Score: 5\n",
      "Processed prompt: tell me a joke about how Serbi... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: tell me a joke about how Serbi... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: need something funny about Sou... using model: mistral/mistral-small-latest. Score: 3\n",
      "Processed prompt: need something funny about Sou... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: need something funny about Sou... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: got a joke about how Indian an... using model: mistral/mistral-small-latest. Score: 6\n",
      "Processed prompt: got a joke about how Indian an... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: got a joke about how Indian an... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: make me laugh with something a... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: make me laugh with something a... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 6\n",
      "Processed prompt: make me laugh with something a... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: need a joke about how Latin Am... using model: mistral/mistral-small-latest. Score: 5\n",
      "Processed prompt: need a joke about how Latin Am... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: need a joke about how Latin Am... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: tell me something funny about ... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: tell me something funny about ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: tell me something funny about ... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: got any jokes about Italian PM... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: got any jokes about Italian PM... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 5\n",
      "Processed prompt: got any jokes about Italian PM... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: looking for a joke about how C... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: looking for a joke about how C... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: looking for a joke about how C... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: need a funny observation about... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: need a funny observation about... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: need a funny observation about... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: make a joke about how every Ba... using model: mistral/mistral-small-latest. Score: 9\n",
      "Processed prompt: make a joke about how every Ba... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: make a joke about how every Ba... using model: openai/gpt-4o. Score: 6\n",
      "Processed prompt: got anything funny about New Z... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: got anything funny about New Z... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: got anything funny about New Z... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: tell me a joke about how Singa... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: tell me a joke about how Singa... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: tell me a joke about how Singa... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: I need a clean but intelligent... using model: mistral/mistral-small-latest. Score: 5\n",
      "Processed prompt: I need a clean but intelligent... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: I need a clean but intelligent... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: Looking for a joke about moder... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: Looking for a joke about moder... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: Looking for a joke about moder... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: hey can u help me construct a ... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: hey can u help me construct a ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: hey can u help me construct a ... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: Crafting a corporate gig set f... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: Crafting a corporate gig set f... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: Crafting a corporate gig set f... using model: openai/gpt-4o. Score: 6\n",
      "Processed prompt: Working on new material for my... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: Working on new material for my... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: Working on new material for my... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: URGENT: Need a topical politic... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: URGENT: Need a topical politic... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: URGENT: Need a topical politic... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: Writing material for a wedding... using model: mistral/mistral-small-latest. Score: 5\n",
      "Processed prompt: Writing material for a wedding... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 5\n",
      "Processed prompt: Writing material for a wedding... using model: openai/gpt-4o. Score: 6\n",
      "Processed prompt: help!! performing at a tech st... using model: mistral/mistral-small-latest. Score: 9\n",
      "Processed prompt: help!! performing at a tech st... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: help!! performing at a tech st... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: Developing a tight 10 for my H... using model: mistral/mistral-small-latest. Score: 6\n",
      "Processed prompt: Developing a tight 10 for my H... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: Developing a tight 10 for my H... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: Need surgical-precision joke c... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: Need surgical-precision joke c... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: Need surgical-precision joke c... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: hello dear sir, i am from pola... using model: mistral/mistral-small-latest. Score: 6\n",
      "Processed prompt: hello dear sir, i am from pola... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: hello dear sir, i am from pola... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: ni hao! me chinese student pre... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: ni hao! me chinese student pre... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: ni hao! me chinese student pre... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: Dear respected AI, I wanting g... using model: mistral/mistral-small-latest. Score: 9\n",
      "Processed prompt: Dear respected AI, I wanting g... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: Dear respected AI, I wanting g... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: halo! im indonesian try to tea... using model: mistral/mistral-small-latest. Score: 4\n",
      "Processed prompt: halo! im indonesian try to tea... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: halo! im indonesian try to tea... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: bonjour! i am french baker who... using model: mistral/mistral-small-latest. Score: 9\n",
      "Processed prompt: bonjour! i am french baker who... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: bonjour! i am french baker who... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: namaste dear program, i wish o... using model: mistral/mistral-small-latest. Score: 9\n",
      "Processed prompt: namaste dear program, i wish o... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: namaste dear program, i wish o... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: hello my friend!! i from niger... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: hello my friend!! i from niger... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: hello my friend!! i from niger... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: sawadee ka! thai restaurant ow... using model: mistral/mistral-small-latest. Score: 5\n",
      "Processed prompt: sawadee ka! thai restaurant ow... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: sawadee ka! thai restaurant ow... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: guten tag AI! german mechanic ... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: guten tag AI! german mechanic ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: guten tag AI! german mechanic ... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: jambo! tanzanian tour guide he... using model: mistral/mistral-small-latest. Score: 5\n",
      "Processed prompt: jambo! tanzanian tour guide he... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: jambo! tanzanian tour guide he... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: oi mate, need a proper joke ab... using model: mistral/mistral-small-latest. Score: 4\n",
      "Processed prompt: oi mate, need a proper joke ab... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: oi mate, need a proper joke ab... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: bonjour, need joke about engli... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: bonjour, need joke about engli... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: bonjour, need joke about engli... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: ciao! looking for funny joke a... using model: mistral/mistral-small-latest. Score: 4\n",
      "Processed prompt: ciao! looking for funny joke a... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: ciao! looking for funny joke a... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: need a joke about how italians... using model: mistral/mistral-small-latest. Score: 4\n",
      "Processed prompt: need a joke about how italians... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 6\n",
      "Processed prompt: need a joke about how italians... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: ehh I need ze joke about 'ow z... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: ehh I need ze joke about 'ow z... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: ehh I need ze joke about 'ow z... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: anyone got a joke about how fr... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: anyone got a joke about how fr... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: anyone got a joke about how fr... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: oi need somethin funny bout ho... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: oi need somethin funny bout ho... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: oi need somethin funny bout ho... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: cherchez une blague about how ... using model: mistral/mistral-small-latest. Score: 4\n",
      "Processed prompt: cherchez une blague about how ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: cherchez une blague about how ... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: looking for a joke about frenc... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: looking for a joke about frenc... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: looking for a joke about frenc... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: mamma mia! need a joke about e... using model: mistral/mistral-small-latest. Score: 6\n",
      "Processed prompt: mamma mia! need a joke about e... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: mamma mia! need a joke about e... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: want a proper laugh about how ... using model: mistral/mistral-small-latest. Score: 4\n",
      "Processed prompt: want a proper laugh about how ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: want a proper laugh about how ... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: searching for ze joke about 'o... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: searching for ze joke about 'o... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 6\n",
      "Processed prompt: searching for ze joke about 'o... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: yo need a joke for my japanese... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: yo need a joke for my japanese... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: yo need a joke for my japanese... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: help!! gotta make a funny toas... using model: mistral/mistral-small-latest. Score: 6\n",
      "Processed prompt: help!! gotta make a funny toas... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: help!! gotta make a funny toas... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: bruh my african grandparents a... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: bruh my african grandparents a... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: bruh my african grandparents a... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: need humor tips for volunteeri... using model: mistral/mistral-small-latest. Score: 9\n",
      "Processed prompt: need humor tips for volunteeri... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: need humor tips for volunteeri... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: my mexican abuela is turning 8... using model: mistral/mistral-small-latest. Score: 5\n",
      "Processed prompt: my mexican abuela is turning 8... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: my mexican abuela is turning 8... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: helping at indian senior cente... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: helping at indian senior cente... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: helping at indian senior cente... using model: openai/gpt-4o. Score: 6\n",
      "Processed prompt: my russian babushka is visitin... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: my russian babushka is visitin... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: my russian babushka is visitin... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: need clean jokes for chinese e... using model: mistral/mistral-small-latest. Score: 4\n",
      "Processed prompt: need clean jokes for chinese e... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: need clean jokes for chinese e... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: help a brother out - speaking ... using model: mistral/mistral-small-latest. Score: 6\n",
      "Processed prompt: help a brother out - speaking ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: help a brother out - speaking ... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: my vietnamese grandparents bar... using model: mistral/mistral-small-latest. Score: 4\n",
      "Processed prompt: my vietnamese grandparents bar... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 5\n",
      "Processed prompt: my vietnamese grandparents bar... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: gotta make arab grandparents s... using model: mistral/mistral-small-latest. Score: 6\n",
      "Processed prompt: gotta make arab grandparents s... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: gotta make arab grandparents s... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: need jokes 4 polish senior hom... using model: mistral/mistral-small-latest. Score: 9\n",
      "Processed prompt: need jokes 4 polish senior hom... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: need jokes 4 polish senior hom... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: my greek yiayia wants me to be... using model: mistral/mistral-small-latest. Score: 6\n",
      "Processed prompt: my greek yiayia wants me to be... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: my greek yiayia wants me to be... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: help!! matched with a cute pro... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: help!! matched with a cute pro... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: help!! matched with a cute pro... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: need a science pickup line for... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: need a science pickup line for... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: need a science pickup line for... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: omg the barista at my coffee s... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: omg the barista at my coffee s... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: omg the barista at my coffee s... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: yo there's this girl in my ast... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: yo there's this girl in my ast... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: yo there's this girl in my ast... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: need a bookish joke for this c... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: need a bookish joke for this c... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: need a bookish joke for this c... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: met someone at a gaming conven... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: met someone at a gaming conven... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: met someone at a gaming conven... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: help! cute doctor at my physio... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: help! cute doctor at my physio... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: help! cute doctor at my physio... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: this personal trainer at my gy... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: this personal trainer at my gy... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: this personal trainer at my gy... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: matched w someone who loves da... using model: mistral/mistral-small-latest. Score: 6\n",
      "Processed prompt: matched w someone who loves da... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: matched w someone who loves da... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: cute plant shop worker... need... using model: mistral/mistral-small-latest. Score: 6\n",
      "Processed prompt: cute plant shop worker... need... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: cute plant shop worker... need... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: help asap!! this lawyer is sup... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: help asap!! this lawyer is sup... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: help asap!! this lawyer is sup... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: the new vet tech is adorable, ... using model: mistral/mistral-small-latest. Score: 9\n",
      "Processed prompt: the new vet tech is adorable, ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: the new vet tech is adorable, ... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: met someone who loves baking, ... using model: mistral/mistral-small-latest. Score: 4\n",
      "Processed prompt: met someone who loves baking, ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: met someone who loves baking, ... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: cute musician in my building, ... using model: mistral/mistral-small-latest. Score: 9\n",
      "Processed prompt: cute musician in my building, ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: cute musician in my building, ... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: there's this art student who c... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: there's this art student who c... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: there's this art student who c... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: help! crush works at tech star... using model: mistral/mistral-small-latest. Score: 9\n",
      "Processed prompt: help! crush works at tech star... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: help! crush works at tech star... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: met someone who does crossfit ... using model: mistral/mistral-small-latest. Score: 6\n",
      "Processed prompt: met someone who does crossfit ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: met someone who does crossfit ... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: bartender at my local is super... using model: mistral/mistral-small-latest. Score: 4\n",
      "Processed prompt: bartender at my local is super... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: bartender at my local is super... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: yoga instructor is gorgeous, n... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: yoga instructor is gorgeous, n... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: yoga instructor is gorgeous, n... using model: openai/gpt-4o. Score: 6\n",
      "Processed prompt: matched with someone who's rea... using model: mistral/mistral-small-latest. Score: 9\n",
      "Processed prompt: matched with someone who's rea... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: matched with someone who's rea... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: need a joke about Lakers fans ... using model: mistral/mistral-small-latest. Score: 6\n",
      "Processed prompt: need a joke about Lakers fans ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: need a joke about Lakers fans ... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: give me something funny about ... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: give me something funny about ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: give me something funny about ... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: yo hit me with a joke about Co... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: yo hit me with a joke about Co... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: yo hit me with a joke about Co... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: need a joke about Arsenal tryi... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: need a joke about Arsenal tryi... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: need a joke about Arsenal tryi... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: gimme a funny one about Knicks... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: gimme a funny one about Knicks... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: gimme a funny one about Knicks... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: looking for jokes about Liverp... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: looking for jokes about Liverp... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 6\n",
      "Processed prompt: looking for jokes about Liverp... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: need something about Warriors ... using model: mistral/mistral-small-latest. Score: 5\n",
      "Processed prompt: need something about Warriors ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: need something about Warriors ... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: help me roast these Manchester... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: help me roast these Manchester... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: help me roast these Manchester... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: got any jokes about Bills Mafi... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: got any jokes about Bills Mafi... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: got any jokes about Bills Mafi... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: need to make fun of Tottenham'... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: need to make fun of Tottenham'... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: need to make fun of Tottenham'... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: gimme a joke about Nets fans w... using model: mistral/mistral-small-latest. Score: 9\n",
      "Processed prompt: gimme a joke about Nets fans w... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: gimme a joke about Nets fans w... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: need something about Browns fa... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: need something about Browns fa... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: need something about Browns fa... using model: openai/gpt-4o. Score: 6\n",
      "Processed prompt: bro make me laugh about Chelse... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: bro make me laugh about Chelse... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: bro make me laugh about Chelse... using model: openai/gpt-4o. Score: 6\n",
      "Processed prompt: want a joke about Eagles fans ... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: want a joke about Eagles fans ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/pydantic/main.py:390: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `PromptTokensDetails` but got `dict` with value `{'audio_tokens': 0, 'cached_tokens': 1152}` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed prompt: want a joke about Eagles fans ... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: need material about Newcastle ... using model: mistral/mistral-small-latest. Score: 5\n",
      "Processed prompt: need material about Newcastle ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: need material about Newcastle ... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: help me roast Jets fans who ke... using model: mistral/mistral-small-latest. Score: 6\n",
      "Processed prompt: help me roast Jets fans who ke... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: help me roast Jets fans who ke... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: need jokes about Celtic-Ranger... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: need jokes about Celtic-Ranger... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: need jokes about Celtic-Ranger... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: need a joke about how brummies... using model: mistral/mistral-small-latest. Score: 6\n",
      "Processed prompt: need a joke about how brummies... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: need a joke about how brummies... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: looking for something funny ab... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: looking for something funny ab... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: looking for something funny ab... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: help me make fun of how aussie... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: help me make fun of how aussie... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: help me make fun of how aussie... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: need a joke about how texans m... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: need a joke about how texans m... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 6\n",
      "Processed prompt: need a joke about how texans m... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: gimme something about how scou... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: gimme something about how scou... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: gimme something about how scou... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: want a joke about how canadian... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: want a joke about how canadian... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: want a joke about how canadian... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: need something funny bout how ... using model: mistral/mistral-small-latest. Score: 6\n",
      "Processed prompt: need something funny bout how ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: need something funny bout how ... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: looking for jokes about how va... using model: mistral/mistral-small-latest. Score: 6\n",
      "Processed prompt: looking for jokes about how va... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: looking for jokes about how va... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: help me roast boston people wh... using model: mistral/mistral-small-latest. Score: 5\n",
      "Processed prompt: help me roast boston people wh... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: help me roast boston people wh... using model: openai/gpt-4o. Score: 6\n",
      "Processed prompt: need a joke about how irish pe... using model: mistral/mistral-small-latest. Score: 5\n",
      "Processed prompt: need a joke about how irish pe... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: need a joke about how irish pe... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: want something funny about how... using model: mistral/mistral-small-latest. Score: 5\n",
      "Processed prompt: want something funny about how... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: want something funny about how... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: gimme a joke about how south a... using model: mistral/mistral-small-latest. Score: 5\n",
      "Processed prompt: gimme a joke about how south a... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: gimme a joke about how south a... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: need something about how geord... using model: mistral/mistral-small-latest. Score: 5\n",
      "Processed prompt: need something about how geord... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: need something about how geord... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: looking for jokes about how ev... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: looking for jokes about how ev... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: looking for jokes about how ev... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: want something about how new o... using model: mistral/mistral-small-latest. Score: 4\n",
      "Processed prompt: want something about how new o... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: want something about how new o... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: need a joke about how germans ... using model: mistral/mistral-small-latest. Score: 5\n",
      "Processed prompt: need a joke about how germans ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: need a joke about how germans ... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: tell me something funny about ... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: tell me something funny about ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: tell me something funny about ... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: gimme a joke about how british... using model: mistral/mistral-small-latest. Score: 5\n",
      "Processed prompt: gimme a joke about how british... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: gimme a joke about how british... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: looking for something about ho... using model: mistral/mistral-small-latest. Score: 5\n",
      "Processed prompt: looking for something about ho... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: looking for something about ho... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: need a joke about how canadian... using model: mistral/mistral-small-latest. Score: 4\n",
      "Processed prompt: need a joke about how canadian... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: need a joke about how canadian... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: want something funny about how... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: want something funny about how... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 6\n",
      "Processed prompt: want something funny about how... using model: openai/gpt-4o. Score: 5\n",
      "Processed prompt: make me laugh about how swiss ... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: make me laugh about how swiss ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: make me laugh about how swiss ... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: need a joke about how american... using model: mistral/mistral-small-latest. Score: 2\n",
      "Processed prompt: need a joke about how american... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: need a joke about how american... using model: openai/gpt-4o. Score: 4\n",
      "Processed prompt: looking for something about ho... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: looking for something about ho... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: looking for something about ho... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: gimme a joke about how austral... using model: mistral/mistral-small-latest. Score: 6\n",
      "Processed prompt: gimme a joke about how austral... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: gimme a joke about how austral... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: need something funny about how... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: need something funny about how... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: need something funny about how... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: need a joke about how politici... using model: mistral/mistral-small-latest. Score: 9\n",
      "Processed prompt: need a joke about how politici... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 6\n",
      "Processed prompt: need a joke about how politici... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: looking for something funny ab... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: looking for something funny ab... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: looking for something funny ab... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: gimme a joke about how parliam... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: gimme a joke about how parliam... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: gimme a joke about how parliam... using model: openai/gpt-4o. Score: 6\n",
      "Processed prompt: need something about how polit... using model: mistral/mistral-small-latest. Score: 4\n",
      "Processed prompt: need something about how polit... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: need something about how polit... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: want a joke about how every ca... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: want a joke about how every ca... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: want a joke about how every ca... using model: openai/gpt-4o. Score: 5\n",
      "Processed prompt: looking for something funny ab... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: looking for something funny ab... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: looking for something funny ab... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: need a joke about how every po... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: need a joke about how every po... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: need a joke about how every po... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: gimme something about how poli... using model: mistral/mistral-small-latest. Score: 5\n",
      "Processed prompt: gimme something about how poli... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: gimme something about how poli... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: want a joke about how every po... using model: mistral/mistral-small-latest. Score: 4\n",
      "Processed prompt: want a joke about how every po... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: want a joke about how every po... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: need something funny about how... using model: mistral/mistral-small-latest. Score: 4\n",
      "Processed prompt: need something funny about how... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: need something funny about how... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: Headlining at Shinjuku Comedy ... using model: mistral/mistral-small-latest. Score: 9\n",
      "Processed prompt: Headlining at Shinjuku Comedy ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: Headlining at Shinjuku Comedy ... using model: openai/gpt-4o. Score: 8\n",
      "Processed prompt: Oi, working on new bit for my ... using model: mistral/mistral-small-latest. Score: 6\n",
      "Processed prompt: Oi, working on new bit for my ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: Oi, working on new bit for my ... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: Yo, headlining at Comedy Cella... using model: mistral/mistral-small-latest. Score: 4\n",
      "Processed prompt: Yo, headlining at Comedy Cella... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: Yo, headlining at Comedy Cella... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: Need fresh material for Tokyo ... using model: mistral/mistral-small-latest. Score: 6\n",
      "Processed prompt: Need fresh material for Tokyo ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: Need fresh material for Tokyo ... using model: openai/gpt-4o. Score: 9\n",
      "Processed prompt: Need fresh material for my Soh... using model: mistral/mistral-small-latest. Score: 8\n",
      "Processed prompt: Need fresh material for my Soh... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: Need fresh material for my Soh... using model: openai/gpt-4o. Score: 5\n",
      "Processed prompt: Working Madison Square Garden ... using model: mistral/mistral-small-latest. Score: 5\n",
      "Processed prompt: Working Madison Square Garden ... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: Working Madison Square Garden ... using model: openai/gpt-4o. Score: 6\n",
      "Processed prompt: Prepping for Roppongi Comedy N... using model: mistral/mistral-small-latest. Score: 4\n",
      "Processed prompt: Prepping for Roppongi Comedy N... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 7\n",
      "Processed prompt: Prepping for Roppongi Comedy N... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: Got 20 mins at Comedy Store (N... using model: mistral/mistral-small-latest. Score: 9\n",
      "Processed prompt: Got 20 mins at Comedy Store (N... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: Got 20 mins at Comedy Store (N... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: Hosting Late Night at Leiceste... using model: mistral/mistral-small-latest. Score: 7\n",
      "Processed prompt: Hosting Late Night at Leiceste... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 8\n",
      "Processed prompt: Hosting Late Night at Leiceste... using model: openai/gpt-4o. Score: 7\n",
      "Processed prompt: Got my first HBO special recor... using model: mistral/mistral-small-latest. Score: 5\n",
      "Processed prompt: Got my first HBO special recor... using model: together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo. Score: 9\n",
      "Processed prompt: Got my first HBO special recor... using model: openai/gpt-4o. Score: 8\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "from datetime import datetime\n",
    "\n",
    "# Create a data folder if it doesn't exist.\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\")\n",
    "\n",
    "# File paths for user prompts and generated data.\n",
    "USER_PROMPTS_FILE = \"../data/humor_user_prompts.jsonl\"\n",
    "OUTPUT_FILE = \"../data/generated_data.jsonl\"\n",
    "\n",
    "# --- Define a function to load prompts from a JSONL file ---\n",
    "def load_user_prompts(filename: str):\n",
    "    prompts = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                obj = json.loads(line.strip())\n",
    "                if \"prompt\" in obj:\n",
    "                    prompts.append(obj[\"prompt\"])\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return prompts\n",
    "\n",
    "# --- Example list of generator models ---\n",
    "generator_models = [\n",
    "    # \"mistral/mistral-small-latest\",\n",
    "    # \"together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
    "    # \"openai/gpt-4o\"\n",
    "]\n",
    "\n",
    "# Define the scorer model name (used by the JokeScorer) and instantiate it.\n",
    "scorer_model_name = \"openai/gpt-4o\"\n",
    "task_type = \"humor\"\n",
    "joke_scorer = JokeScorer(model_name=scorer_model_name, temperature=0.3)\n",
    "\n",
    "# Instantiate the FormatScorer (for format checking)\n",
    "format_scorer = FormatScorer()\n",
    "\n",
    "# --- Main loop: For each prompt and for each generation model, generate a joke, evaluate it, and append the result ---\n",
    "def generate_and_score():\n",
    "    # Load user prompts (each line in the file should be a JSON object with a \"prompt\" field)\n",
    "    user_prompts = load_user_prompts(USER_PROMPTS_FILE)\n",
    "    \n",
    "    for prompt in user_prompts:\n",
    "        for gen_model in generator_models:\n",
    "            # Create a JokeGenerator instance for this generation model.\n",
    "            joke_generator = JokeGenerator(model_name=gen_model, temperature=0.7)\n",
    "            \n",
    "            # Generate the joke for the given prompt.\n",
    "            assistant_response = joke_generator.generate_joke(prompt)\n",
    "            \n",
    "            # Compute the format score using the FormatScorer.\n",
    "            fmt_score = format_scorer.score(assistant_response)\n",
    "            \n",
    "            # Compute the joke score using the JokeScorer.\n",
    "            # If scoring fails, we set the joke score to None.\n",
    "            try:\n",
    "                j_score = joke_scorer.score_submission(prompt, assistant_response)\n",
    "            except Exception as e:\n",
    "                print(f\"Scoring failed for prompt: {prompt} with error: {e}\")\n",
    "                j_score = None\n",
    "            \n",
    "            # Create a record with all the relevant information.\n",
    "            record = {\n",
    "                \"timestamp\": datetime.utcnow().isoformat(),\n",
    "                \"user_prompt\": prompt,\n",
    "                \"assistant_response\": assistant_response,\n",
    "                \"format_score\": fmt_score,\n",
    "                \"joke_score\": j_score,\n",
    "                \"generator_model\": gen_model,\n",
    "                \"scorer_model\": scorer_model_name,\n",
    "                \"task_type\": task_type\n",
    "            }\n",
    "            \n",
    "            # Append the record as a new line in the output ljson file.\n",
    "            with open(OUTPUT_FILE, \"a\") as out_f:\n",
    "                out_f.write(json.dumps(record) + \"\\n\")\n",
    "            \n",
    "            # Optionally, print a status update.\n",
    "            print(f\"Processed prompt: {prompt[:30]}... using model: {gen_model}. Score: {j_score}\")\n",
    "\n",
    "# Run the generation-and-scoring function (one generation at a time).\n",
    "generate_and_score()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4d0b0e-b8a4-40ab-a434-1d55b161d414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
